{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 复习上课内容以及复现课程代码"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在本部分，你需要复习上课内容和课程代码后，自己复现课程代码。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Node:\n",
    "    # 该类为所有其他图节点类的父类\n",
    "    def __init__(self, inputs=[]):\n",
    "        #定义每个节点的输入和输出\n",
    "        self.inputs = inputs\n",
    "        self.outputs = []\n",
    "        \n",
    "        #每个节点都是其输入节点的输出节点\n",
    "        for n in self.inputs:\n",
    "            # 把自身加入到输入节点的输出节点列表中\n",
    "            n.outputs.append(self)\n",
    "\n",
    "        self.value = None\n",
    "        \n",
    "        self.gradients = {}\n",
    "        # 梯度是dict形式，key是input节点，value是输入节点对这个节点的偏导\n",
    "        # keys are the inputs to this node, and their\n",
    "        # values are the partials of this node with \n",
    "        # respect to that input.\n",
    "        # \\partial{node}{input_i}\n",
    "        \n",
    "\n",
    "    def forward(self):\n",
    "        #前向传播函数 继承该类的其他类会覆写该函数\n",
    "        '''\n",
    "        Forward propagation. \n",
    "        Compute the output value vased on 'inbound_nodes' and store the \n",
    "        result in self.value\n",
    "        '''\n",
    "\n",
    "        raise NotImplemented\n",
    "    \n",
    "\n",
    "    def backward(self):\n",
    "        #反向传播函数，继承该类的其他类会覆写该函数\n",
    "\n",
    "        raise NotImplemented\n",
    "        \n",
    "class Input(Node):\n",
    "    # 输入节点，包括神经网络输入节点，权重节点，和偏差节点\n",
    "    def __init__(self):\n",
    "        '''\n",
    "        An Input node has no inbound nodes.\n",
    "        So no need to pass anything to the Node instantiator.\n",
    "        '''\n",
    "        Node.__init__(self)\n",
    "\n",
    "    def forward(self, value=None):\n",
    "        '''\n",
    "        Only input node is the node where the value may be passed\n",
    "        as an argument to forward().\n",
    "        All other node implementations should get the value of the \n",
    "        previous node from self.inbound_nodes\n",
    "        \n",
    "        Example: \n",
    "        val0: self.inbound_nodes[0].value\n",
    "        '''\n",
    "        #定义节点数值\n",
    "        if value is not None:\n",
    "            self.value = value\n",
    "            ## It's is input node, when need to forward, this node initiate self's value.\n",
    "\n",
    "        # Input subclass just holds a value, such as a data feature or a model parameter(weight/bias)\n",
    "        \n",
    "    def backward(self):\n",
    "        #计算节点梯度\n",
    "        self.gradients = {self:0} # initialization \n",
    "        for n in self.outputs:\n",
    "            #以下计算该节点的输出节点对该节点的梯度\n",
    "            grad_cost = n.gradients[self]\n",
    "            self.gradients[self] = grad_cost * 1\n",
    "            \n",
    "        \n",
    "        # input N --> N1, N2\n",
    "        # \\partial L / \\partial N \n",
    "        # ==> \\partial L / \\partial N1 * \\ partial N1 / \\partial N\n",
    "\n",
    "\n",
    "class Add(Node):\n",
    "    def __init__(self, *nodes):\n",
    "        Node.__init__(self, nodes)\n",
    "\n",
    "\n",
    "    def forward(self):\n",
    "        # 把它所有input的value值累加\n",
    "        self.value = sum(map(lambda n: n.value, self.inputs))\n",
    "        ## when execute forward, this node caculate value as defined.\n",
    "\n",
    "class Linear(Node):\n",
    "    #全连接网络层的计算\n",
    "    def __init__(self, nodes, weights, bias):\n",
    "        Node.__init__(self, [nodes, weights, bias])\n",
    "\n",
    "    def forward(self):\n",
    "        #前向传播计算 y = w*x + b\n",
    "        inputs = self.inputs[0].value\n",
    "        weights = self.inputs[1].value\n",
    "        bias = self.inputs[2].value\n",
    "\n",
    "        self.value = np.dot(inputs, weights) + bias\n",
    "        \n",
    "    def backward(self):\n",
    "        #反向传播计算\n",
    "        # initial a partial for each of the inbound_nodes.\n",
    "        self.gradients = {n: np.zeros_like(n.value) for n in self.inputs}\n",
    "\n",
    "        for n in self.outputs:\n",
    "            # Get the partial of the cost w.r.t this node.\n",
    "            grad_cost = n.gradients[self]\n",
    "            #以下分别计算对inputs， weights, bias的梯度\n",
    "            self.gradients[self.inputs[0]] = np.dot(grad_cost, self.inputs[1].value.T)\n",
    "            self.gradients[self.inputs[1]] = np.dot(self.inputs[0].value.T, grad_cost)\n",
    "            self.gradients[self.inputs[2]] = np.sum(grad_cost, axis=0, keepdims=False)\n",
    "\n",
    "        # WX + B / W ==> X\n",
    "        # WX + B / X ==> W\n",
    "\n",
    "class Sigmoid(Node):\n",
    "    #定义sigmoid函数\n",
    "    def __init__(self, node):\n",
    "        Node.__init__(self, [node])\n",
    "\n",
    "\n",
    "    def _sigmoid(self, x):\n",
    "        return 1./(1 + np.exp(-1 * x))\n",
    "\n",
    "    def forward(self):\n",
    "        #前向 即为sigmoid函数计算\n",
    "        self.x = self.inputs[0].value   # [0] input is a list\n",
    "        self.value = self._sigmoid(self.x)\n",
    "\n",
    "    def backward(self):\n",
    "        #反向传播计算梯度\n",
    "        self.partial = self._sigmoid(self.x) * (1 - self._sigmoid(self.x))\n",
    "        \n",
    "        # y = 1 / (1 + e^-x)\n",
    "        # y' = 1 / (1 + e^-x) (1 - 1 / (1 + e^-x))\n",
    "        \n",
    "        self.gradients = {n: np.zeros_like(n.value) for n in self.inputs}\n",
    "\n",
    "        for n in self.outputs:\n",
    "            grad_cost = n.gradients[self]  # Get the partial of the cost with respect to this node.\n",
    "\n",
    "            self.gradients[self.inputs[0]] = grad_cost * self.partial\n",
    "            # use * to keep all the dimension same!.\n",
    "\n",
    "\n",
    "\n",
    "class MSE(Node):\n",
    "    # 定义平均平方误差\n",
    "    def __init__(self, y, a):\n",
    "        Node.__init__(self, [y, a])\n",
    "\n",
    "\n",
    "    def forward(self):\n",
    "        #前向传播计算\n",
    "        y = self.inputs[0].value.reshape(-1, 1)\n",
    "        a = self.inputs[1].value.reshape(-1, 1)\n",
    "        assert(y.shape == a.shape)\n",
    "\n",
    "        self.m = self.inputs[0].value.shape[0]\n",
    "        self.diff = y - a\n",
    "\n",
    "        self.value = np.mean(self.diff**2)\n",
    "\n",
    "\n",
    "    def backward(self):\n",
    "        #反向计算相应梯度\n",
    "        self.gradients[self.inputs[0]] = (2 / self.m) * self.diff\n",
    "        self.gradients[self.inputs[1]] = (-2 / self.m) * self.diff\n",
    "\n",
    "\n",
    "def forward_and_backward(outputnode, graph):\n",
    "    # execute all the forward method of sorted_nodes.\n",
    "\n",
    "    ## In practice, it's common to feed in mutiple data example in each forward pass rather than just 1. Because the examples can be processed in parallel. The number of examples is called batch size.\n",
    "    for n in graph:\n",
    "        n.forward()\n",
    "        ## each node execute forward, get self.value based on the topological sort result.\n",
    "\n",
    "    for n in  graph[::-1]:\n",
    "        n.backward()\n",
    "\n",
    "    #return outputnode.value\n",
    "\n",
    "###   v -->  a -->  C\n",
    "##    b --> C\n",
    "##    b --> v -- a --> C\n",
    "##    v --> v ---> a -- > C\n",
    "\n",
    "def topological_sort(feed_dict):\n",
    "    \"\"\"\n",
    "    Sort generic nodes in topological order using Kahn's Algorithm.\n",
    "    `feed_dict`: A dictionary where the key is a `Input` node and the value is the respective value feed to that node.\n",
    "    Returns a list of sorted nodes.\n",
    "    \"\"\"\n",
    "\n",
    "    input_nodes = [n for n in feed_dict.keys()]\n",
    "\n",
    "    G = {}\n",
    "    nodes = [n for n in input_nodes]\n",
    "    while len(nodes) > 0:\n",
    "        n = nodes.pop(0)\n",
    "        if n not in G:\n",
    "            G[n] = {'in': set(), 'out': set()}\n",
    "        for m in n.outputs:\n",
    "            if m not in G:\n",
    "                G[m] = {'in': set(), 'out': set()}\n",
    "            G[n]['out'].add(m)\n",
    "            G[m]['in'].add(n)\n",
    "            nodes.append(m)\n",
    "\n",
    "    L = []\n",
    "    S = set(input_nodes)\n",
    "    while len(S) > 0:\n",
    "        n = S.pop()\n",
    "\n",
    "        if isinstance(n, Input):\n",
    "            n.value = feed_dict[n]\n",
    "            ## if n is Input Node, set n'value as \n",
    "            ## feed_dict[n]\n",
    "            ## else, n's value is caculate as its\n",
    "            ## inbounds\n",
    "\n",
    "        L.append(n)\n",
    "        for m in n.outputs:\n",
    "            G[n]['out'].remove(m)\n",
    "            G[m]['in'].remove(n)\n",
    "            # if no other incoming edges add to S\n",
    "            if len(G[m]['in']) == 0:\n",
    "                S.add(m)\n",
    "    return L\n",
    "\n",
    "\n",
    "def sgd_update(trainables, learning_rate=1e-2):\n",
    "    # there are so many other update / optimization methods\n",
    "    # such as Adam, Mom, \n",
    "    for t in trainables:\n",
    "        t.value += -1 * learning_rate * t.gradients[t]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "data = load_boston()\n",
    "losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of examples = 506\n",
      "Epoch: 1, Loss: 132.308\n",
      "Epoch: 101, Loss: 6.737\n",
      "Epoch: 201, Loss: 6.032\n",
      "Epoch: 301, Loss: 5.096\n",
      "Epoch: 401, Loss: 4.349\n",
      "Epoch: 501, Loss: 4.989\n",
      "Epoch: 601, Loss: 4.041\n",
      "Epoch: 701, Loss: 3.290\n",
      "Epoch: 801, Loss: 3.317\n",
      "Epoch: 901, Loss: 3.427\n",
      "Epoch: 1001, Loss: 3.665\n",
      "Epoch: 1101, Loss: 3.305\n",
      "Epoch: 1201, Loss: 3.468\n",
      "Epoch: 1301, Loss: 3.025\n",
      "Epoch: 1401, Loss: 3.153\n",
      "Epoch: 1501, Loss: 3.058\n",
      "Epoch: 1601, Loss: 3.143\n",
      "Epoch: 1701, Loss: 3.563\n",
      "Epoch: 1801, Loss: 2.785\n",
      "Epoch: 1901, Loss: 3.490\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Check out the new network architecture and dataset!\n",
    "Notice that the weights and biases are\n",
    "generated randomly.\n",
    "No need to change anything, but feel free to tweak\n",
    "to test your network, play around with the epochs, batch size, etc!\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.utils import shuffle, resample\n",
    "#from miniflow import *\n",
    "\n",
    "# Load data\n",
    "data = load_boston()\n",
    "X_ = data['data']\n",
    "y_ = data['target']\n",
    "\n",
    "# Normalize data\n",
    "X_ = (X_ - np.mean(X_, axis=0)) / np.std(X_, axis=0)\n",
    "\n",
    "n_features = X_.shape[1]\n",
    "n_hidden = 10\n",
    "W1_ = np.random.randn(n_features, n_hidden)\n",
    "b1_ = np.zeros(n_hidden)\n",
    "W2_ = np.random.randn(n_hidden, 1)\n",
    "b2_ = np.zeros(1)\n",
    "\n",
    "# Neural network\n",
    "X, y = Input(), Input()\n",
    "W1, b1 = Input(), Input()\n",
    "W2, b2 = Input(), Input()\n",
    "\n",
    "l1 = Linear(X, W1, b1)\n",
    "s1 = Sigmoid(l1)\n",
    "l2 = Linear(s1, W2, b2)\n",
    "cost = MSE(y, l2)\n",
    "\n",
    "feed_dict = {\n",
    "    X: X_,\n",
    "    y: y_,\n",
    "    W1: W1_,\n",
    "    b1: b1_,\n",
    "    W2: W2_,\n",
    "    b2: b2_\n",
    "}\n",
    "\n",
    "epochs = 2000\n",
    "# Total number of examples\n",
    "m = X_.shape[0]\n",
    "batch_size = 16\n",
    "steps_per_epoch = m // batch_size\n",
    "\n",
    "graph = topological_sort(feed_dict)\n",
    "trainables = [W1, b1, W2, b2]\n",
    "\n",
    "print(\"Total number of examples = {}\".format(m))\n",
    "\n",
    "# Step 4\n",
    "for i in range(epochs):\n",
    "    loss = 0\n",
    "    for j in range(steps_per_epoch):\n",
    "        # Step 1\n",
    "        # Randomly sample a batch of examples\n",
    "        X_batch, y_batch = resample(X_, y_, n_samples=batch_size)\n",
    "\n",
    "        # Reset value of X and y Inputs\n",
    "        X.value = X_batch\n",
    "        y.value = y_batch\n",
    "\n",
    "        # Step 2\n",
    "        _ = None\n",
    "        forward_and_backward(_, graph) # set output node not important.\n",
    "\n",
    "        # Step 3\n",
    "        rate = 1e-2\n",
    "    \n",
    "        sgd_update(trainables, rate)\n",
    "\n",
    "        loss += graph[-1].value\n",
    "    \n",
    "    if i % 100 == 0: \n",
    "        print(\"Epoch: {}, Loss: {:.3f}\".format(i+1, loss/steps_per_epoch))\n",
    "        losses.append(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(outputNode,graph):\n",
    "    for n in graph:\n",
    "        n.forward()\n",
    "    return outputNode.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[22.84597206],\n",
       "       [23.38372329],\n",
       "       [29.83587353],\n",
       "       [21.6152016 ],\n",
       "       [15.29049743],\n",
       "       [19.10201608],\n",
       "       [23.36596846],\n",
       "       [29.19298091],\n",
       "       [29.19298091],\n",
       "       [23.01129639],\n",
       "       [31.10102438],\n",
       "       [50.76809897],\n",
       "       [18.59359116],\n",
       "       [28.096956  ],\n",
       "       [20.11631233],\n",
       "       [20.02723679]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forward(l2,graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x28e2fc06390>]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(range(len(losses)), losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4101.541552062237,\n",
       " 208.84346694378902,\n",
       " 186.98000913669335,\n",
       " 157.98924729368218,\n",
       " 134.8285091403252,\n",
       " 154.64738715158904,\n",
       " 125.2779636082419,\n",
       " 101.98124670654518,\n",
       " 102.8139058604992,\n",
       " 106.23306191722028,\n",
       " 113.62453480063839,\n",
       " 102.45364193900079,\n",
       " 107.49983936739392,\n",
       " 93.7695926423976,\n",
       " 97.74359417488394,\n",
       " 94.78876384235208,\n",
       " 97.42155143900355,\n",
       " 110.44911623609458,\n",
       " 86.32852233215338,\n",
       " 108.20339602008931]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 6.74575884],\n",
       "       [12.68728052],\n",
       "       [ 5.39541491],\n",
       "       [ 5.7213527 ],\n",
       "       [ 7.04946485],\n",
       "       [ 7.68613884],\n",
       "       [10.22213898],\n",
       "       [-9.40173312],\n",
       "       [ 5.55930856],\n",
       "       [ 9.12933038]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W2.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_ = data['data']\n",
    "# Normalize data\n",
    "X_ = (X_ - np.mean(X_, axis=0)) / np.std(X_, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(units=64, activation='relu', input_dim=13))\n",
    "model.add(Dense(units=30, activation='relu', input_dim=64))\n",
    "model.add(Dense(units=1))\n",
    "\n",
    "model.compile(loss='mse',\n",
    "              optimizer='sgd',\n",
    "              metrics=['mse'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "506/506 [==============================] - 0s 359us/step - loss: 403.4494 - mse: 403.4494\n",
      "Epoch 2/500\n",
      "506/506 [==============================] - 0s 109us/step - loss: 115.4834 - mse: 115.4835\n",
      "Epoch 3/500\n",
      "506/506 [==============================] - 0s 124us/step - loss: 50.0369 - mse: 50.0369\n",
      "Epoch 4/500\n",
      "506/506 [==============================] - 0s 117us/step - loss: 33.7663 - mse: 33.7663\n",
      "Epoch 5/500\n",
      "506/506 [==============================] - 0s 101us/step - loss: 30.0563 - mse: 30.0563\n",
      "Epoch 6/500\n",
      "506/506 [==============================] - 0s 104us/step - loss: 21.0638 - mse: 21.0638\n",
      "Epoch 7/500\n",
      "506/506 [==============================] - 0s 102us/step - loss: 26.4041 - mse: 26.4040\n",
      "Epoch 8/500\n",
      "506/506 [==============================] - 0s 108us/step - loss: 16.1903 - mse: 16.1903\n",
      "Epoch 9/500\n",
      "506/506 [==============================] - 0s 121us/step - loss: 21.0512 - mse: 21.0512\n",
      "Epoch 10/500\n",
      "506/506 [==============================] - 0s 137us/step - loss: 12.5404 - mse: 12.5404\n",
      "Epoch 11/500\n",
      "506/506 [==============================] - 0s 137us/step - loss: 14.4545 - mse: 14.4545\n",
      "Epoch 12/500\n",
      "506/506 [==============================] - 0s 102us/step - loss: 9.7091 - mse: 9.7091\n",
      "Epoch 13/500\n",
      "506/506 [==============================] - 0s 114us/step - loss: 10.3837 - mse: 10.3837\n",
      "Epoch 14/500\n",
      "506/506 [==============================] - 0s 120us/step - loss: 10.5551 - mse: 10.5551\n",
      "Epoch 15/500\n",
      "506/506 [==============================] - 0s 101us/step - loss: 21.3048 - mse: 21.3048\n",
      "Epoch 16/500\n",
      "506/506 [==============================] - 0s 99us/step - loss: 9.4193 - mse: 9.4193\n",
      "Epoch 17/500\n",
      "506/506 [==============================] - 0s 106us/step - loss: 14.6528 - mse: 14.6528\n",
      "Epoch 18/500\n",
      "506/506 [==============================] - 0s 98us/step - loss: 9.8007 - mse: 9.8007\n",
      "Epoch 19/500\n",
      "506/506 [==============================] - 0s 93us/step - loss: 7.7281 - mse: 7.7281\n",
      "Epoch 20/500\n",
      "506/506 [==============================] - 0s 95us/step - loss: 8.7774 - mse: 8.7774\n",
      "Epoch 21/500\n",
      "506/506 [==============================] - 0s 101us/step - loss: 7.4517 - mse: 7.4517\n",
      "Epoch 22/500\n",
      "506/506 [==============================] - 0s 100us/step - loss: 8.3411 - mse: 8.3411\n",
      "Epoch 23/500\n",
      "506/506 [==============================] - 0s 101us/step - loss: 15.5016 - mse: 15.5016\n",
      "Epoch 24/500\n",
      "506/506 [==============================] - 0s 87us/step - loss: 16.3171 - mse: 16.3171\n",
      "Epoch 25/500\n",
      "506/506 [==============================] - 0s 104us/step - loss: 8.7550 - mse: 8.7550\n",
      "Epoch 26/500\n",
      "506/506 [==============================] - 0s 99us/step - loss: 6.7344 - mse: 6.7344\n",
      "Epoch 27/500\n",
      "506/506 [==============================] - 0s 91us/step - loss: 7.2601 - mse: 7.2601\n",
      "Epoch 28/500\n",
      "506/506 [==============================] - 0s 101us/step - loss: 7.5685 - mse: 7.5685\n",
      "Epoch 29/500\n",
      "506/506 [==============================] - 0s 104us/step - loss: 5.9891 - mse: 5.9891\n",
      "Epoch 30/500\n",
      "506/506 [==============================] - 0s 91us/step - loss: 8.6142 - mse: 8.6142\n",
      "Epoch 31/500\n",
      "506/506 [==============================] - 0s 108us/step - loss: 11.6130 - mse: 11.6130\n",
      "Epoch 32/500\n",
      "506/506 [==============================] - 0s 132us/step - loss: 19.1453 - mse: 19.1453\n",
      "Epoch 33/500\n",
      "506/506 [==============================] - 0s 132us/step - loss: 7.4174 - mse: 7.4174\n",
      "Epoch 34/500\n",
      "506/506 [==============================] - 0s 110us/step - loss: 7.0999 - mse: 7.0999\n",
      "Epoch 35/500\n",
      "506/506 [==============================] - 0s 100us/step - loss: 6.8904 - mse: 6.8904\n",
      "Epoch 36/500\n",
      "506/506 [==============================] - 0s 95us/step - loss: 6.6716 - mse: 6.6716\n",
      "Epoch 37/500\n",
      "506/506 [==============================] - 0s 90us/step - loss: 6.5051 - mse: 6.5051\n",
      "Epoch 38/500\n",
      "506/506 [==============================] - 0s 102us/step - loss: 6.9742 - mse: 6.9742\n",
      "Epoch 39/500\n",
      "506/506 [==============================] - 0s 102us/step - loss: 8.3680 - mse: 8.3680\n",
      "Epoch 40/500\n",
      "506/506 [==============================] - 0s 99us/step - loss: 6.7761 - mse: 6.7761\n",
      "Epoch 41/500\n",
      "506/506 [==============================] - 0s 112us/step - loss: 6.0099 - mse: 6.0099\n",
      "Epoch 42/500\n",
      "506/506 [==============================] - 0s 102us/step - loss: 7.8194 - mse: 7.8194\n",
      "Epoch 43/500\n",
      "506/506 [==============================] - 0s 90us/step - loss: 5.9123 - mse: 5.9123\n",
      "Epoch 44/500\n",
      "506/506 [==============================] - 0s 101us/step - loss: 6.9717 - mse: 6.9717\n",
      "Epoch 45/500\n",
      "506/506 [==============================] - 0s 89us/step - loss: 5.5843 - mse: 5.5843\n",
      "Epoch 46/500\n",
      "506/506 [==============================] - 0s 99us/step - loss: 10.6647 - mse: 10.6647\n",
      "Epoch 47/500\n",
      "506/506 [==============================] - 0s 102us/step - loss: 6.1281 - mse: 6.1281\n",
      "Epoch 48/500\n",
      "506/506 [==============================] - 0s 119us/step - loss: 4.5266 - mse: 4.5266\n",
      "Epoch 49/500\n",
      "506/506 [==============================] - 0s 108us/step - loss: 7.0621 - mse: 7.0620\n",
      "Epoch 50/500\n",
      "506/506 [==============================] - 0s 114us/step - loss: 5.2436 - mse: 5.2436\n",
      "Epoch 51/500\n",
      "506/506 [==============================] - 0s 120us/step - loss: 7.6364 - mse: 7.6364\n",
      "Epoch 52/500\n",
      "506/506 [==============================] - 0s 97us/step - loss: 10.5438 - mse: 10.5438\n",
      "Epoch 53/500\n",
      "506/506 [==============================] - 0s 108us/step - loss: 7.0679 - mse: 7.0679\n",
      "Epoch 54/500\n",
      "506/506 [==============================] - 0s 123us/step - loss: 4.6880 - mse: 4.6880\n",
      "Epoch 55/500\n",
      "506/506 [==============================] - 0s 112us/step - loss: 7.2315 - mse: 7.2315\n",
      "Epoch 56/500\n",
      "506/506 [==============================] - 0s 135us/step - loss: 7.0428 - mse: 7.0428\n",
      "Epoch 57/500\n",
      "506/506 [==============================] - 0s 124us/step - loss: 6.2480 - mse: 6.2480\n",
      "Epoch 58/500\n",
      "506/506 [==============================] - 0s 102us/step - loss: 4.9182 - mse: 4.9182\n",
      "Epoch 59/500\n",
      "506/506 [==============================] - 0s 99us/step - loss: 3.9743 - mse: 3.9743\n",
      "Epoch 60/500\n",
      "506/506 [==============================] - 0s 91us/step - loss: 4.9595 - mse: 4.9595\n",
      "Epoch 61/500\n",
      "506/506 [==============================] - 0s 91us/step - loss: 4.2153 - mse: 4.2153\n",
      "Epoch 62/500\n",
      "506/506 [==============================] - 0s 97us/step - loss: 5.3470 - mse: 5.3470\n",
      "Epoch 63/500\n",
      "506/506 [==============================] - 0s 102us/step - loss: 5.1812 - mse: 5.1812\n",
      "Epoch 64/500\n",
      "506/506 [==============================] - 0s 122us/step - loss: 5.9748 - mse: 5.9748\n",
      "Epoch 65/500\n",
      "506/506 [==============================] - 0s 110us/step - loss: 5.6735 - mse: 5.6735\n",
      "Epoch 66/500\n",
      "506/506 [==============================] - 0s 118us/step - loss: 4.9713 - mse: 4.9713\n",
      "Epoch 67/500\n",
      "506/506 [==============================] - 0s 124us/step - loss: 3.8874 - mse: 3.8874\n",
      "Epoch 68/500\n",
      "506/506 [==============================] - 0s 120us/step - loss: 4.1926 - mse: 4.1926\n",
      "Epoch 69/500\n",
      "506/506 [==============================] - 0s 99us/step - loss: 3.5255 - mse: 3.5255\n",
      "Epoch 70/500\n",
      "506/506 [==============================] - 0s 123us/step - loss: 3.6422 - mse: 3.6422\n",
      "Epoch 71/500\n",
      "506/506 [==============================] - 0s 108us/step - loss: 4.5510 - mse: 4.5510\n",
      "Epoch 72/500\n",
      "506/506 [==============================] - 0s 104us/step - loss: 4.7077 - mse: 4.7077\n",
      "Epoch 73/500\n",
      "506/506 [==============================] - 0s 105us/step - loss: 5.8605 - mse: 5.8605\n",
      "Epoch 74/500\n",
      "506/506 [==============================] - 0s 103us/step - loss: 3.8962 - mse: 3.8962\n",
      "Epoch 75/500\n",
      "506/506 [==============================] - 0s 114us/step - loss: 4.8108 - mse: 4.8108\n",
      "Epoch 76/500\n",
      "506/506 [==============================] - 0s 131us/step - loss: 4.4720 - mse: 4.4720\n",
      "Epoch 77/500\n",
      "506/506 [==============================] - 0s 128us/step - loss: 3.8062 - mse: 3.8062\n",
      "Epoch 78/500\n",
      "506/506 [==============================] - 0s 128us/step - loss: 4.0663 - mse: 4.0663\n",
      "Epoch 79/500\n",
      "506/506 [==============================] - 0s 116us/step - loss: 4.0311 - mse: 4.0311\n",
      "Epoch 80/500\n",
      "506/506 [==============================] - 0s 105us/step - loss: 5.1253 - mse: 5.1253\n",
      "Epoch 81/500\n",
      "506/506 [==============================] - 0s 97us/step - loss: 4.2621 - mse: 4.2621\n",
      "Epoch 82/500\n",
      "506/506 [==============================] - 0s 91us/step - loss: 4.0748 - mse: 4.0748\n",
      "Epoch 83/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "506/506 [==============================] - 0s 97us/step - loss: 4.1255 - mse: 4.1255\n",
      "Epoch 84/500\n",
      "506/506 [==============================] - 0s 99us/step - loss: 3.8221 - mse: 3.8221\n",
      "Epoch 85/500\n",
      "506/506 [==============================] - 0s 120us/step - loss: 3.7960 - mse: 3.7960\n",
      "Epoch 86/500\n",
      "506/506 [==============================] - 0s 100us/step - loss: 3.2510 - mse: 3.2510\n",
      "Epoch 87/500\n",
      "506/506 [==============================] - 0s 101us/step - loss: 4.1063 - mse: 4.1063\n",
      "Epoch 88/500\n",
      "506/506 [==============================] - 0s 102us/step - loss: 4.1776 - mse: 4.1776\n",
      "Epoch 89/500\n",
      "506/506 [==============================] - 0s 97us/step - loss: 3.5056 - mse: 3.5056\n",
      "Epoch 90/500\n",
      "506/506 [==============================] - 0s 88us/step - loss: 3.4145 - mse: 3.4145\n",
      "Epoch 91/500\n",
      "506/506 [==============================] - 0s 91us/step - loss: 3.4333 - mse: 3.4333\n",
      "Epoch 92/500\n",
      "506/506 [==============================] - 0s 83us/step - loss: 3.0657 - mse: 3.0657\n",
      "Epoch 93/500\n",
      "506/506 [==============================] - 0s 120us/step - loss: 2.9609 - mse: 2.9609\n",
      "Epoch 94/500\n",
      "506/506 [==============================] - 0s 220us/step - loss: 3.1520 - mse: 3.1520\n",
      "Epoch 95/500\n",
      "506/506 [==============================] - 0s 126us/step - loss: 3.2403 - mse: 3.2403\n",
      "Epoch 96/500\n",
      "506/506 [==============================] - 0s 103us/step - loss: 3.1691 - mse: 3.1691\n",
      "Epoch 97/500\n",
      "506/506 [==============================] - 0s 116us/step - loss: 4.6835 - mse: 4.6835\n",
      "Epoch 98/500\n",
      "506/506 [==============================] - 0s 124us/step - loss: 2.8015 - mse: 2.8015\n",
      "Epoch 99/500\n",
      "506/506 [==============================] - 0s 140us/step - loss: 3.4658 - mse: 3.4658\n",
      "Epoch 100/500\n",
      "506/506 [==============================] - 0s 108us/step - loss: 4.7419 - mse: 4.7419\n",
      "Epoch 101/500\n",
      "506/506 [==============================] - 0s 101us/step - loss: 3.6860 - mse: 3.6860\n",
      "Epoch 102/500\n",
      "506/506 [==============================] - 0s 117us/step - loss: 4.3354 - mse: 4.3354\n",
      "Epoch 103/500\n",
      "506/506 [==============================] - 0s 102us/step - loss: 2.8860 - mse: 2.8860\n",
      "Epoch 104/500\n",
      "506/506 [==============================] - 0s 92us/step - loss: 3.0197 - mse: 3.0197\n",
      "Epoch 105/500\n",
      "506/506 [==============================] - 0s 208us/step - loss: 3.0165 - mse: 3.0165\n",
      "Epoch 106/500\n",
      "506/506 [==============================] - 0s 199us/step - loss: 3.3312 - mse: 3.3312\n",
      "Epoch 107/500\n",
      "506/506 [==============================] - 0s 105us/step - loss: 3.1552 - mse: 3.1552\n",
      "Epoch 108/500\n",
      "506/506 [==============================] - 0s 150us/step - loss: 3.1089 - mse: 3.1089\n",
      "Epoch 109/500\n",
      "506/506 [==============================] - 0s 118us/step - loss: 2.7416 - mse: 2.7416\n",
      "Epoch 110/500\n",
      "506/506 [==============================] - 0s 104us/step - loss: 3.7023 - mse: 3.7023\n",
      "Epoch 111/500\n",
      "506/506 [==============================] - 0s 104us/step - loss: 6.4300 - mse: 6.4300\n",
      "Epoch 112/500\n",
      "506/506 [==============================] - 0s 89us/step - loss: 3.2101 - mse: 3.2101\n",
      "Epoch 113/500\n",
      "506/506 [==============================] - 0s 116us/step - loss: 2.5092 - mse: 2.5092\n",
      "Epoch 114/500\n",
      "506/506 [==============================] - 0s 133us/step - loss: 3.4028 - mse: 3.4028\n",
      "Epoch 115/500\n",
      "506/506 [==============================] - 0s 130us/step - loss: 3.4284 - mse: 3.4284\n",
      "Epoch 116/500\n",
      "506/506 [==============================] - 0s 174us/step - loss: 3.1704 - mse: 3.1704\n",
      "Epoch 117/500\n",
      "506/506 [==============================] - 0s 197us/step - loss: 3.4917 - mse: 3.4917\n",
      "Epoch 118/500\n",
      "506/506 [==============================] - 0s 178us/step - loss: 2.7177 - mse: 2.7177\n",
      "Epoch 119/500\n",
      "506/506 [==============================] - 0s 147us/step - loss: 2.7150 - mse: 2.7150\n",
      "Epoch 120/500\n",
      "506/506 [==============================] - 0s 123us/step - loss: 2.6248 - mse: 2.6248\n",
      "Epoch 121/500\n",
      "506/506 [==============================] - 0s 142us/step - loss: 2.4980 - mse: 2.4980\n",
      "Epoch 122/500\n",
      "506/506 [==============================] - 0s 123us/step - loss: 2.8638 - mse: 2.8638\n",
      "Epoch 123/500\n",
      "506/506 [==============================] - 0s 121us/step - loss: 3.1269 - mse: 3.1269\n",
      "Epoch 124/500\n",
      "506/506 [==============================] - 0s 126us/step - loss: 2.9438 - mse: 2.9438\n",
      "Epoch 125/500\n",
      "506/506 [==============================] - 0s 113us/step - loss: 2.9933 - mse: 2.9933\n",
      "Epoch 126/500\n",
      "506/506 [==============================] - 0s 118us/step - loss: 2.9767 - mse: 2.9767\n",
      "Epoch 127/500\n",
      "506/506 [==============================] - 0s 121us/step - loss: 2.9284 - mse: 2.9284\n",
      "Epoch 128/500\n",
      "506/506 [==============================] - 0s 126us/step - loss: 2.7233 - mse: 2.7233\n",
      "Epoch 129/500\n",
      "506/506 [==============================] - 0s 124us/step - loss: 3.7701 - mse: 3.7701\n",
      "Epoch 130/500\n",
      "506/506 [==============================] - 0s 138us/step - loss: 7.1945 - mse: 7.1945\n",
      "Epoch 131/500\n",
      "506/506 [==============================] - 0s 176us/step - loss: 2.7968 - mse: 2.7968\n",
      "Epoch 132/500\n",
      "506/506 [==============================] - 0s 127us/step - loss: 2.2421 - mse: 2.2421\n",
      "Epoch 133/500\n",
      "506/506 [==============================] - 0s 118us/step - loss: 2.9467 - mse: 2.9467\n",
      "Epoch 134/500\n",
      "506/506 [==============================] - 0s 140us/step - loss: 2.6171 - mse: 2.6171\n",
      "Epoch 135/500\n",
      "506/506 [==============================] - 0s 144us/step - loss: 4.1674 - mse: 4.1674\n",
      "Epoch 136/500\n",
      "506/506 [==============================] - 0s 154us/step - loss: 2.5164 - mse: 2.5164\n",
      "Epoch 137/500\n",
      "506/506 [==============================] - 0s 168us/step - loss: 2.5934 - mse: 2.5934\n",
      "Epoch 138/500\n",
      "506/506 [==============================] - 0s 124us/step - loss: 2.1642 - mse: 2.1642\n",
      "Epoch 139/500\n",
      "506/506 [==============================] - 0s 110us/step - loss: 2.7693 - mse: 2.7693\n",
      "Epoch 140/500\n",
      "506/506 [==============================] - 0s 112us/step - loss: 2.3988 - mse: 2.3988\n",
      "Epoch 141/500\n",
      "506/506 [==============================] - 0s 119us/step - loss: 2.2772 - mse: 2.2772\n",
      "Epoch 142/500\n",
      "506/506 [==============================] - 0s 114us/step - loss: 2.2857 - mse: 2.2857\n",
      "Epoch 143/500\n",
      "506/506 [==============================] - 0s 124us/step - loss: 5.1492 - mse: 5.1492\n",
      "Epoch 144/500\n",
      "506/506 [==============================] - 0s 132us/step - loss: 3.5111 - mse: 3.5111\n",
      "Epoch 145/500\n",
      "506/506 [==============================] - 0s 124us/step - loss: 2.3530 - mse: 2.3530\n",
      "Epoch 146/500\n",
      "506/506 [==============================] - 0s 112us/step - loss: 2.2102 - mse: 2.2102\n",
      "Epoch 147/500\n",
      "506/506 [==============================] - 0s 105us/step - loss: 2.6844 - mse: 2.6844\n",
      "Epoch 148/500\n",
      "506/506 [==============================] - 0s 110us/step - loss: 2.4906 - mse: 2.4906\n",
      "Epoch 149/500\n",
      "506/506 [==============================] - 0s 114us/step - loss: 2.8165 - mse: 2.8165\n",
      "Epoch 150/500\n",
      "506/506 [==============================] - 0s 107us/step - loss: 2.2654 - mse: 2.2654\n",
      "Epoch 151/500\n",
      "506/506 [==============================] - 0s 114us/step - loss: 2.0768 - mse: 2.0768\n",
      "Epoch 152/500\n",
      "506/506 [==============================] - 0s 113us/step - loss: 5.2425 - mse: 5.2425\n",
      "Epoch 153/500\n",
      "506/506 [==============================] - 0s 104us/step - loss: 2.6920 - mse: 2.6920\n",
      "Epoch 154/500\n",
      "506/506 [==============================] - 0s 102us/step - loss: 3.2178 - mse: 3.2178\n",
      "Epoch 155/500\n",
      "506/506 [==============================] - 0s 104us/step - loss: 2.4749 - mse: 2.4749\n",
      "Epoch 156/500\n",
      "506/506 [==============================] - 0s 115us/step - loss: 4.1977 - mse: 4.1977\n",
      "Epoch 157/500\n",
      "506/506 [==============================] - 0s 122us/step - loss: 2.2362 - mse: 2.2362\n",
      "Epoch 158/500\n",
      "506/506 [==============================] - 0s 118us/step - loss: 2.0531 - mse: 2.0531\n",
      "Epoch 159/500\n",
      "506/506 [==============================] - 0s 102us/step - loss: 2.0685 - mse: 2.0685\n",
      "Epoch 160/500\n",
      "506/506 [==============================] - 0s 120us/step - loss: 2.2215 - mse: 2.2215\n",
      "Epoch 161/500\n",
      "506/506 [==============================] - 0s 143us/step - loss: 2.0200 - mse: 2.0200\n",
      "Epoch 162/500\n",
      "506/506 [==============================] - 0s 129us/step - loss: 2.0041 - mse: 2.0041\n",
      "Epoch 163/500\n",
      "506/506 [==============================] - 0s 120us/step - loss: 2.4547 - mse: 2.4547\n",
      "Epoch 164/500\n",
      "506/506 [==============================] - 0s 126us/step - loss: 2.3202 - mse: 2.3202\n",
      "Epoch 165/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "506/506 [==============================] - 0s 122us/step - loss: 2.1381 - mse: 2.1381\n",
      "Epoch 166/500\n",
      "506/506 [==============================] - 0s 114us/step - loss: 2.1210 - mse: 2.1210\n",
      "Epoch 167/500\n",
      "506/506 [==============================] - 0s 117us/step - loss: 1.7555 - mse: 1.7555\n",
      "Epoch 168/500\n",
      "506/506 [==============================] - 0s 118us/step - loss: 2.6095 - mse: 2.6095\n",
      "Epoch 169/500\n",
      "506/506 [==============================] - 0s 122us/step - loss: 3.9393 - mse: 3.9393\n",
      "Epoch 170/500\n",
      "506/506 [==============================] - 0s 112us/step - loss: 2.9658 - mse: 2.9658\n",
      "Epoch 171/500\n",
      "506/506 [==============================] - 0s 124us/step - loss: 2.5025 - mse: 2.5025\n",
      "Epoch 172/500\n",
      "506/506 [==============================] - 0s 93us/step - loss: 2.4602 - mse: 2.4602\n",
      "Epoch 173/500\n",
      "506/506 [==============================] - 0s 108us/step - loss: 1.8768 - mse: 1.8768\n",
      "Epoch 174/500\n",
      "506/506 [==============================] - 0s 95us/step - loss: 2.4825 - mse: 2.4825\n",
      "Epoch 175/500\n",
      "506/506 [==============================] - 0s 120us/step - loss: 2.4469 - mse: 2.4469\n",
      "Epoch 176/500\n",
      "506/506 [==============================] - 0s 120us/step - loss: 2.2723 - mse: 2.2723\n",
      "Epoch 177/500\n",
      "506/506 [==============================] - 0s 136us/step - loss: 2.3466 - mse: 2.3466\n",
      "Epoch 178/500\n",
      "506/506 [==============================] - 0s 122us/step - loss: 2.0309 - mse: 2.0309\n",
      "Epoch 179/500\n",
      "506/506 [==============================] - 0s 169us/step - loss: 2.6608 - mse: 2.6608\n",
      "Epoch 180/500\n",
      "506/506 [==============================] - 0s 116us/step - loss: 3.5394 - mse: 3.5394\n",
      "Epoch 181/500\n",
      "506/506 [==============================] - 0s 205us/step - loss: 2.0825 - mse: 2.0825\n",
      "Epoch 182/500\n",
      "506/506 [==============================] - 0s 168us/step - loss: 2.3072 - mse: 2.3072\n",
      "Epoch 183/500\n",
      "506/506 [==============================] - 0s 211us/step - loss: 2.1098 - mse: 2.1098\n",
      "Epoch 184/500\n",
      "506/506 [==============================] - 0s 128us/step - loss: 2.0036 - mse: 2.0036\n",
      "Epoch 185/500\n",
      "506/506 [==============================] - 0s 85us/step - loss: 1.7276 - mse: 1.7276\n",
      "Epoch 186/500\n",
      "506/506 [==============================] - 0s 204us/step - loss: 2.1474 - mse: 2.1474\n",
      "Epoch 187/500\n",
      "506/506 [==============================] - 0s 158us/step - loss: 3.6735 - mse: 3.6735\n",
      "Epoch 188/500\n",
      "506/506 [==============================] - 0s 209us/step - loss: 2.2921 - mse: 2.2921\n",
      "Epoch 189/500\n",
      "506/506 [==============================] - 0s 131us/step - loss: 1.9179 - mse: 1.9179\n",
      "Epoch 190/500\n",
      "506/506 [==============================] - 0s 116us/step - loss: 1.8017 - mse: 1.8017\n",
      "Epoch 191/500\n",
      "506/506 [==============================] - 0s 124us/step - loss: 1.7861 - mse: 1.7861\n",
      "Epoch 192/500\n",
      "506/506 [==============================] - 0s 131us/step - loss: 2.0303 - mse: 2.0303\n",
      "Epoch 193/500\n",
      "506/506 [==============================] - 0s 126us/step - loss: 2.7425 - mse: 2.7425\n",
      "Epoch 194/500\n",
      "506/506 [==============================] - 0s 134us/step - loss: 1.9583 - mse: 1.9583\n",
      "Epoch 195/500\n",
      "506/506 [==============================] - 0s 110us/step - loss: 1.7263 - mse: 1.7263\n",
      "Epoch 196/500\n",
      "506/506 [==============================] - 0s 91us/step - loss: 1.9190 - mse: 1.9190\n",
      "Epoch 197/500\n",
      "506/506 [==============================] - 0s 104us/step - loss: 1.7726 - mse: 1.7726\n",
      "Epoch 198/500\n",
      "506/506 [==============================] - 0s 100us/step - loss: 2.2300 - mse: 2.2300\n",
      "Epoch 199/500\n",
      "506/506 [==============================] - 0s 93us/step - loss: 2.0790 - mse: 2.0790\n",
      "Epoch 200/500\n",
      "506/506 [==============================] - 0s 89us/step - loss: 2.5655 - mse: 2.5655\n",
      "Epoch 201/500\n",
      "506/506 [==============================] - 0s 91us/step - loss: 3.3062 - mse: 3.3062\n",
      "Epoch 202/500\n",
      "506/506 [==============================] - 0s 89us/step - loss: 1.9621 - mse: 1.9621\n",
      "Epoch 203/500\n",
      "506/506 [==============================] - 0s 91us/step - loss: 1.9213 - mse: 1.9213\n",
      "Epoch 204/500\n",
      "506/506 [==============================] - 0s 178us/step - loss: 2.0939 - mse: 2.0939\n",
      "Epoch 205/500\n",
      "506/506 [==============================] - 0s 144us/step - loss: 2.5936 - mse: 2.5936\n",
      "Epoch 206/500\n",
      "506/506 [==============================] - 0s 110us/step - loss: 1.7091 - mse: 1.7091\n",
      "Epoch 207/500\n",
      "506/506 [==============================] - 0s 103us/step - loss: 1.8018 - mse: 1.8018\n",
      "Epoch 208/500\n",
      "506/506 [==============================] - 0s 106us/step - loss: 3.0202 - mse: 3.0202\n",
      "Epoch 209/500\n",
      "506/506 [==============================] - 0s 176us/step - loss: 1.6797 - mse: 1.6797\n",
      "Epoch 210/500\n",
      "506/506 [==============================] - 0s 122us/step - loss: 1.9059 - mse: 1.9059\n",
      "Epoch 211/500\n",
      "506/506 [==============================] - 0s 123us/step - loss: 2.0973 - mse: 2.0973\n",
      "Epoch 212/500\n",
      "506/506 [==============================] - 0s 120us/step - loss: 2.2040 - mse: 2.2040\n",
      "Epoch 213/500\n",
      "506/506 [==============================] - 0s 125us/step - loss: 2.3263 - mse: 2.3263\n",
      "Epoch 214/500\n",
      "506/506 [==============================] - 0s 112us/step - loss: 2.2437 - mse: 2.2437\n",
      "Epoch 215/500\n",
      "506/506 [==============================] - 0s 97us/step - loss: 2.0088 - mse: 2.0088\n",
      "Epoch 216/500\n",
      "506/506 [==============================] - 0s 94us/step - loss: 2.0408 - mse: 2.0408\n",
      "Epoch 217/500\n",
      "506/506 [==============================] - 0s 211us/step - loss: 1.8005 - mse: 1.8005\n",
      "Epoch 218/500\n",
      "506/506 [==============================] - 0s 102us/step - loss: 1.9122 - mse: 1.9122\n",
      "Epoch 219/500\n",
      "506/506 [==============================] - 0s 96us/step - loss: 1.7975 - mse: 1.7975\n",
      "Epoch 220/500\n",
      "506/506 [==============================] - 0s 106us/step - loss: 1.7894 - mse: 1.7894\n",
      "Epoch 221/500\n",
      "506/506 [==============================] - 0s 111us/step - loss: 1.8012 - mse: 1.8012\n",
      "Epoch 222/500\n",
      "506/506 [==============================] - 0s 102us/step - loss: 1.5342 - mse: 1.5342\n",
      "Epoch 223/500\n",
      "506/506 [==============================] - 0s 90us/step - loss: 1.9008 - mse: 1.9008\n",
      "Epoch 224/500\n",
      "506/506 [==============================] - 0s 118us/step - loss: 1.7521 - mse: 1.7521\n",
      "Epoch 225/500\n",
      "506/506 [==============================] - 0s 195us/step - loss: 2.9599 - mse: 2.9599\n",
      "Epoch 226/500\n",
      "506/506 [==============================] - 0s 138us/step - loss: 2.0606 - mse: 2.0606\n",
      "Epoch 227/500\n",
      "506/506 [==============================] - 0s 86us/step - loss: 2.3957 - mse: 2.3957\n",
      "Epoch 228/500\n",
      "506/506 [==============================] - 0s 97us/step - loss: 1.7808 - mse: 1.7808\n",
      "Epoch 229/500\n",
      "506/506 [==============================] - 0s 101us/step - loss: 1.6114 - mse: 1.6114\n",
      "Epoch 230/500\n",
      "506/506 [==============================] - 0s 93us/step - loss: 1.7396 - mse: 1.7396\n",
      "Epoch 231/500\n",
      "506/506 [==============================] - 0s 114us/step - loss: 2.2990 - mse: 2.2990\n",
      "Epoch 232/500\n",
      "506/506 [==============================] - 0s 122us/step - loss: 1.7403 - mse: 1.7403\n",
      "Epoch 233/500\n",
      "506/506 [==============================] - 0s 134us/step - loss: 1.4784 - mse: 1.4784\n",
      "Epoch 234/500\n",
      "506/506 [==============================] - 0s 130us/step - loss: 1.4425 - mse: 1.4425\n",
      "Epoch 235/500\n",
      "506/506 [==============================] - 0s 95us/step - loss: 1.6648 - mse: 1.6648\n",
      "Epoch 236/500\n",
      "506/506 [==============================] - 0s 93us/step - loss: 2.1265 - mse: 2.1265\n",
      "Epoch 237/500\n",
      "506/506 [==============================] - 0s 107us/step - loss: 1.6918 - mse: 1.6918\n",
      "Epoch 238/500\n",
      "506/506 [==============================] - 0s 122us/step - loss: 1.4752 - mse: 1.4752\n",
      "Epoch 239/500\n",
      "506/506 [==============================] - 0s 101us/step - loss: 1.7833 - mse: 1.7833\n",
      "Epoch 240/500\n",
      "506/506 [==============================] - 0s 97us/step - loss: 2.1075 - mse: 2.1075\n",
      "Epoch 241/500\n",
      "506/506 [==============================] - 0s 104us/step - loss: 2.8271 - mse: 2.8271\n",
      "Epoch 242/500\n",
      "506/506 [==============================] - 0s 99us/step - loss: 1.9325 - mse: 1.9325\n",
      "Epoch 243/500\n",
      "506/506 [==============================] - 0s 95us/step - loss: 1.6138 - mse: 1.6138\n",
      "Epoch 244/500\n",
      "506/506 [==============================] - 0s 91us/step - loss: 1.5800 - mse: 1.5800\n",
      "Epoch 245/500\n",
      "506/506 [==============================] - 0s 94us/step - loss: 1.4621 - mse: 1.4621\n",
      "Epoch 246/500\n",
      "506/506 [==============================] - 0s 108us/step - loss: 1.6701 - mse: 1.6701\n",
      "Epoch 247/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "506/506 [==============================] - 0s 99us/step - loss: 1.8753 - mse: 1.8753\n",
      "Epoch 248/500\n",
      "506/506 [==============================] - 0s 102us/step - loss: 1.6665 - mse: 1.6665\n",
      "Epoch 249/500\n",
      "506/506 [==============================] - 0s 91us/step - loss: 1.5830 - mse: 1.5830\n",
      "Epoch 250/500\n",
      "506/506 [==============================] - 0s 98us/step - loss: 1.8725 - mse: 1.8725\n",
      "Epoch 251/500\n",
      "506/506 [==============================] - 0s 101us/step - loss: 1.6905 - mse: 1.6905\n",
      "Epoch 252/500\n",
      "506/506 [==============================] - 0s 106us/step - loss: 3.4088 - mse: 3.4088\n",
      "Epoch 253/500\n",
      "506/506 [==============================] - 0s 106us/step - loss: 1.7967 - mse: 1.7967\n",
      "Epoch 254/500\n",
      "506/506 [==============================] - 0s 112us/step - loss: 1.9930 - mse: 1.9930\n",
      "Epoch 255/500\n",
      "506/506 [==============================] - 0s 143us/step - loss: 1.5236 - mse: 1.5236\n",
      "Epoch 256/500\n",
      "506/506 [==============================] - 0s 171us/step - loss: 1.5123 - mse: 1.5123\n",
      "Epoch 257/500\n",
      "506/506 [==============================] - 0s 150us/step - loss: 1.6524 - mse: 1.6524\n",
      "Epoch 258/500\n",
      "506/506 [==============================] - 0s 124us/step - loss: 1.5291 - mse: 1.5291\n",
      "Epoch 259/500\n",
      "506/506 [==============================] - 0s 119us/step - loss: 2.8400 - mse: 2.8400\n",
      "Epoch 260/500\n",
      "506/506 [==============================] - 0s 108us/step - loss: 1.7568 - mse: 1.7568\n",
      "Epoch 261/500\n",
      "506/506 [==============================] - 0s 97us/step - loss: 3.9154 - mse: 3.9154\n",
      "Epoch 262/500\n",
      "506/506 [==============================] - 0s 88us/step - loss: 1.8925 - mse: 1.8925\n",
      "Epoch 263/500\n",
      "506/506 [==============================] - 0s 101us/step - loss: 1.5958 - mse: 1.5958\n",
      "Epoch 264/500\n",
      "506/506 [==============================] - 0s 101us/step - loss: 1.3623 - mse: 1.3623\n",
      "Epoch 265/500\n",
      "506/506 [==============================] - 0s 97us/step - loss: 1.3649 - mse: 1.3649\n",
      "Epoch 266/500\n",
      "506/506 [==============================] - 0s 117us/step - loss: 1.5384 - mse: 1.5384\n",
      "Epoch 267/500\n",
      "506/506 [==============================] - 0s 106us/step - loss: 2.8022 - mse: 2.8022\n",
      "Epoch 268/500\n",
      "506/506 [==============================] - 0s 106us/step - loss: 1.5916 - mse: 1.5916\n",
      "Epoch 269/500\n",
      "506/506 [==============================] - 0s 98us/step - loss: 1.2847 - mse: 1.2847\n",
      "Epoch 270/500\n",
      "506/506 [==============================] - 0s 198us/step - loss: 1.2785 - mse: 1.2785\n",
      "Epoch 271/500\n",
      "506/506 [==============================] - 0s 144us/step - loss: 1.4025 - mse: 1.4025\n",
      "Epoch 272/500\n",
      "506/506 [==============================] - 0s 126us/step - loss: 1.2769 - mse: 1.2769\n",
      "Epoch 273/500\n",
      "506/506 [==============================] - 0s 110us/step - loss: 1.4140 - mse: 1.4140\n",
      "Epoch 274/500\n",
      "506/506 [==============================] - 0s 122us/step - loss: 3.6844 - mse: 3.6844\n",
      "Epoch 275/500\n",
      "506/506 [==============================] - 0s 195us/step - loss: 2.1212 - mse: 2.1212\n",
      "Epoch 276/500\n",
      "506/506 [==============================] - 0s 178us/step - loss: 1.5396 - mse: 1.5396\n",
      "Epoch 277/500\n",
      "506/506 [==============================] - 0s 181us/step - loss: 1.2886 - mse: 1.2886\n",
      "Epoch 278/500\n",
      "506/506 [==============================] - 0s 152us/step - loss: 2.2029 - mse: 2.2029\n",
      "Epoch 279/500\n",
      "506/506 [==============================] - 0s 128us/step - loss: 1.5157 - mse: 1.5157\n",
      "Epoch 280/500\n",
      "506/506 [==============================] - 0s 125us/step - loss: 1.4260 - mse: 1.4260\n",
      "Epoch 281/500\n",
      "506/506 [==============================] - 0s 130us/step - loss: 1.3832 - mse: 1.3832\n",
      "Epoch 282/500\n",
      "506/506 [==============================] - 0s 124us/step - loss: 1.7231 - mse: 1.7231\n",
      "Epoch 283/500\n",
      "506/506 [==============================] - 0s 122us/step - loss: 1.8665 - mse: 1.8665\n",
      "Epoch 284/500\n",
      "506/506 [==============================] - 0s 134us/step - loss: 1.4033 - mse: 1.4033\n",
      "Epoch 285/500\n",
      "506/506 [==============================] - 0s 136us/step - loss: 1.9222 - mse: 1.9222\n",
      "Epoch 286/500\n",
      "506/506 [==============================] - 0s 132us/step - loss: 1.3742 - mse: 1.3742\n",
      "Epoch 287/500\n",
      "506/506 [==============================] - 0s 134us/step - loss: 1.5193 - mse: 1.5193\n",
      "Epoch 288/500\n",
      "506/506 [==============================] - 0s 128us/step - loss: 1.2907 - mse: 1.2907\n",
      "Epoch 289/500\n",
      "506/506 [==============================] - 0s 126us/step - loss: 1.2870 - mse: 1.2870\n",
      "Epoch 290/500\n",
      "506/506 [==============================] - 0s 110us/step - loss: 1.2705 - mse: 1.2705\n",
      "Epoch 291/500\n",
      "506/506 [==============================] - 0s 128us/step - loss: 1.7113 - mse: 1.7113\n",
      "Epoch 292/500\n",
      "506/506 [==============================] - 0s 126us/step - loss: 1.5199 - mse: 1.5199\n",
      "Epoch 293/500\n",
      "506/506 [==============================] - 0s 150us/step - loss: 1.3663 - mse: 1.3663\n",
      "Epoch 294/500\n",
      "506/506 [==============================] - 0s 149us/step - loss: 1.6408 - mse: 1.6408\n",
      "Epoch 295/500\n",
      "506/506 [==============================] - 0s 148us/step - loss: 1.4517 - mse: 1.4517\n",
      "Epoch 296/500\n",
      "506/506 [==============================] - 0s 152us/step - loss: 1.2549 - mse: 1.2549\n",
      "Epoch 297/500\n",
      "506/506 [==============================] - 0s 113us/step - loss: 1.5030 - mse: 1.5030\n",
      "Epoch 298/500\n",
      "506/506 [==============================] - 0s 116us/step - loss: 1.4199 - mse: 1.4199\n",
      "Epoch 299/500\n",
      "506/506 [==============================] - 0s 124us/step - loss: 1.3017 - mse: 1.3017\n",
      "Epoch 300/500\n",
      "506/506 [==============================] - 0s 140us/step - loss: 1.1882 - mse: 1.1882\n",
      "Epoch 301/500\n",
      "506/506 [==============================] - 0s 125us/step - loss: 1.2771 - mse: 1.2771\n",
      "Epoch 302/500\n",
      "506/506 [==============================] - 0s 118us/step - loss: 1.2618 - mse: 1.2618\n",
      "Epoch 303/500\n",
      "506/506 [==============================] - 0s 130us/step - loss: 1.4972 - mse: 1.4972\n",
      "Epoch 304/500\n",
      "506/506 [==============================] - 0s 116us/step - loss: 1.6124 - mse: 1.6124\n",
      "Epoch 305/500\n",
      "506/506 [==============================] - 0s 112us/step - loss: 1.3814 - mse: 1.3814\n",
      "Epoch 306/500\n",
      "506/506 [==============================] - 0s 118us/step - loss: 1.4229 - mse: 1.4229\n",
      "Epoch 307/500\n",
      "506/506 [==============================] - 0s 112us/step - loss: 1.5448 - mse: 1.5448\n",
      "Epoch 308/500\n",
      "506/506 [==============================] - 0s 123us/step - loss: 1.5188 - mse: 1.5188\n",
      "Epoch 309/500\n",
      "506/506 [==============================] - 0s 121us/step - loss: 1.3038 - mse: 1.3038\n",
      "Epoch 310/500\n",
      "506/506 [==============================] - 0s 116us/step - loss: 1.4503 - mse: 1.4503\n",
      "Epoch 311/500\n",
      "506/506 [==============================] - 0s 116us/step - loss: 1.5963 - mse: 1.5963\n",
      "Epoch 312/500\n",
      "506/506 [==============================] - 0s 126us/step - loss: 1.4322 - mse: 1.4322\n",
      "Epoch 313/500\n",
      "506/506 [==============================] - 0s 150us/step - loss: 1.4222 - mse: 1.4222\n",
      "Epoch 314/500\n",
      "506/506 [==============================] - 0s 141us/step - loss: 1.2535 - mse: 1.2535\n",
      "Epoch 315/500\n",
      "506/506 [==============================] - 0s 206us/step - loss: 1.2372 - mse: 1.2372\n",
      "Epoch 316/500\n",
      "506/506 [==============================] - 0s 127us/step - loss: 1.0812 - mse: 1.0812\n",
      "Epoch 317/500\n",
      "506/506 [==============================] - 0s 120us/step - loss: 2.3758 - mse: 2.3758\n",
      "Epoch 318/500\n",
      "506/506 [==============================] - 0s 131us/step - loss: 1.6784 - mse: 1.6784\n",
      "Epoch 319/500\n",
      "506/506 [==============================] - 0s 111us/step - loss: 2.0657 - mse: 2.0657\n",
      "Epoch 320/500\n",
      "506/506 [==============================] - 0s 112us/step - loss: 2.5789 - mse: 2.5789\n",
      "Epoch 321/500\n",
      "506/506 [==============================] - 0s 118us/step - loss: 1.7470 - mse: 1.7470\n",
      "Epoch 322/500\n",
      "506/506 [==============================] - 0s 154us/step - loss: 1.3921 - mse: 1.3921\n",
      "Epoch 323/500\n",
      "506/506 [==============================] - 0s 122us/step - loss: 1.6200 - mse: 1.6200\n",
      "Epoch 324/500\n",
      "506/506 [==============================] - 0s 118us/step - loss: 1.2792 - mse: 1.2792\n",
      "Epoch 325/500\n",
      "506/506 [==============================] - 0s 110us/step - loss: 1.2503 - mse: 1.2503\n",
      "Epoch 326/500\n",
      "506/506 [==============================] - 0s 114us/step - loss: 1.3104 - mse: 1.3104\n",
      "Epoch 327/500\n",
      "506/506 [==============================] - 0s 116us/step - loss: 1.3033 - mse: 1.3033\n",
      "Epoch 328/500\n",
      "506/506 [==============================] - 0s 120us/step - loss: 1.3639 - mse: 1.3639\n",
      "Epoch 329/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "506/506 [==============================] - 0s 114us/step - loss: 1.9149 - mse: 1.9149\n",
      "Epoch 330/500\n",
      "506/506 [==============================] - 0s 138us/step - loss: 1.3519 - mse: 1.3519\n",
      "Epoch 331/500\n",
      "506/506 [==============================] - 0s 126us/step - loss: 1.5907 - mse: 1.5907\n",
      "Epoch 332/500\n",
      "506/506 [==============================] - 0s 154us/step - loss: 2.0857 - mse: 2.0857\n",
      "Epoch 333/500\n",
      "506/506 [==============================] - 0s 172us/step - loss: 1.7567 - mse: 1.7567\n",
      "Epoch 334/500\n",
      "506/506 [==============================] - 0s 180us/step - loss: 1.4155 - mse: 1.4155\n",
      "Epoch 335/500\n",
      "506/506 [==============================] - 0s 134us/step - loss: 1.5887 - mse: 1.5887\n",
      "Epoch 336/500\n",
      "506/506 [==============================] - 0s 110us/step - loss: 1.3582 - mse: 1.3582\n",
      "Epoch 337/500\n",
      "506/506 [==============================] - 0s 110us/step - loss: 1.3093 - mse: 1.3093\n",
      "Epoch 338/500\n",
      "506/506 [==============================] - 0s 103us/step - loss: 1.9795 - mse: 1.9795\n",
      "Epoch 339/500\n",
      "506/506 [==============================] - 0s 112us/step - loss: 2.2651 - mse: 2.2651\n",
      "Epoch 340/500\n",
      "506/506 [==============================] - 0s 123us/step - loss: 1.5279 - mse: 1.5279\n",
      "Epoch 341/500\n",
      "506/506 [==============================] - 0s 124us/step - loss: 1.3374 - mse: 1.3374\n",
      "Epoch 342/500\n",
      "506/506 [==============================] - 0s 118us/step - loss: 1.5687 - mse: 1.5687\n",
      "Epoch 343/500\n",
      "506/506 [==============================] - 0s 125us/step - loss: 1.5503 - mse: 1.5503\n",
      "Epoch 344/500\n",
      "506/506 [==============================] - 0s 140us/step - loss: 1.3761 - mse: 1.3761\n",
      "Epoch 345/500\n",
      "506/506 [==============================] - 0s 125us/step - loss: 1.0913 - mse: 1.0913\n",
      "Epoch 346/500\n",
      "506/506 [==============================] - 0s 122us/step - loss: 1.2999 - mse: 1.2999\n",
      "Epoch 347/500\n",
      "506/506 [==============================] - 0s 118us/step - loss: 1.3120 - mse: 1.3120\n",
      "Epoch 348/500\n",
      "506/506 [==============================] - 0s 126us/step - loss: 1.5261 - mse: 1.5261\n",
      "Epoch 349/500\n",
      "506/506 [==============================] - 0s 130us/step - loss: 3.3435 - mse: 3.3435\n",
      "Epoch 350/500\n",
      "506/506 [==============================] - 0s 132us/step - loss: 1.1671 - mse: 1.1671\n",
      "Epoch 351/500\n",
      "506/506 [==============================] - 0s 134us/step - loss: 1.2749 - mse: 1.2749\n",
      "Epoch 352/500\n",
      "506/506 [==============================] - 0s 142us/step - loss: 1.8908 - mse: 1.8908\n",
      "Epoch 353/500\n",
      "506/506 [==============================] - 0s 145us/step - loss: 1.8267 - mse: 1.8267\n",
      "Epoch 354/500\n",
      "506/506 [==============================] - 0s 143us/step - loss: 1.5931 - mse: 1.5931\n",
      "Epoch 355/500\n",
      "506/506 [==============================] - 0s 126us/step - loss: 1.1710 - mse: 1.1710\n",
      "Epoch 356/500\n",
      "506/506 [==============================] - 0s 128us/step - loss: 1.3952 - mse: 1.3952\n",
      "Epoch 357/500\n",
      "506/506 [==============================] - 0s 116us/step - loss: 1.0373 - mse: 1.0373\n",
      "Epoch 358/500\n",
      "506/506 [==============================] - 0s 117us/step - loss: 1.2744 - mse: 1.2744\n",
      "Epoch 359/500\n",
      "506/506 [==============================] - 0s 140us/step - loss: 1.3527 - mse: 1.3527\n",
      "Epoch 360/500\n",
      "506/506 [==============================] - 0s 124us/step - loss: 0.9557 - mse: 0.9557\n",
      "Epoch 361/500\n",
      "506/506 [==============================] - 0s 127us/step - loss: 1.0277 - mse: 1.0277\n",
      "Epoch 362/500\n",
      "506/506 [==============================] - 0s 111us/step - loss: 1.2053 - mse: 1.2053\n",
      "Epoch 363/500\n",
      "506/506 [==============================] - 0s 124us/step - loss: 1.2766 - mse: 1.2766\n",
      "Epoch 364/500\n",
      "506/506 [==============================] - 0s 112us/step - loss: 1.1361 - mse: 1.1361\n",
      "Epoch 365/500\n",
      "506/506 [==============================] - 0s 110us/step - loss: 1.0642 - mse: 1.0642\n",
      "Epoch 366/500\n",
      "506/506 [==============================] - 0s 126us/step - loss: 1.2854 - mse: 1.2854\n",
      "Epoch 367/500\n",
      "506/506 [==============================] - 0s 123us/step - loss: 1.0016 - mse: 1.0016\n",
      "Epoch 368/500\n",
      "506/506 [==============================] - 0s 132us/step - loss: 1.1491 - mse: 1.1491\n",
      "Epoch 369/500\n",
      "506/506 [==============================] - 0s 112us/step - loss: 1.2902 - mse: 1.2902\n",
      "Epoch 370/500\n",
      "506/506 [==============================] - 0s 132us/step - loss: 1.9535 - mse: 1.9535\n",
      "Epoch 371/500\n",
      "506/506 [==============================] - 0s 134us/step - loss: 1.1704 - mse: 1.1704\n",
      "Epoch 372/500\n",
      "506/506 [==============================] - 0s 148us/step - loss: 0.9490 - mse: 0.9490\n",
      "Epoch 373/500\n",
      "506/506 [==============================] - 0s 181us/step - loss: 1.0879 - mse: 1.0879\n",
      "Epoch 374/500\n",
      "506/506 [==============================] - 0s 154us/step - loss: 1.1525 - mse: 1.1525\n",
      "Epoch 375/500\n",
      "506/506 [==============================] - 0s 145us/step - loss: 1.1937 - mse: 1.1937\n",
      "Epoch 376/500\n",
      "506/506 [==============================] - 0s 138us/step - loss: 1.3962 - mse: 1.3962\n",
      "Epoch 377/500\n",
      "506/506 [==============================] - 0s 136us/step - loss: 1.5233 - mse: 1.5233\n",
      "Epoch 378/500\n",
      "506/506 [==============================] - 0s 143us/step - loss: 1.4060 - mse: 1.4060\n",
      "Epoch 379/500\n",
      "506/506 [==============================] - 0s 145us/step - loss: 1.1460 - mse: 1.1460\n",
      "Epoch 380/500\n",
      "506/506 [==============================] - 0s 142us/step - loss: 0.9875 - mse: 0.9875\n",
      "Epoch 381/500\n",
      "506/506 [==============================] - 0s 136us/step - loss: 1.0570 - mse: 1.0570\n",
      "Epoch 382/500\n",
      "506/506 [==============================] - 0s 138us/step - loss: 1.2707 - mse: 1.2707\n",
      "Epoch 383/500\n",
      "506/506 [==============================] - 0s 140us/step - loss: 0.9895 - mse: 0.9895\n",
      "Epoch 384/500\n",
      "506/506 [==============================] - 0s 140us/step - loss: 0.9534 - mse: 0.9534\n",
      "Epoch 385/500\n",
      "506/506 [==============================] - 0s 135us/step - loss: 2.0092 - mse: 2.0092\n",
      "Epoch 386/500\n",
      "506/506 [==============================] - 0s 139us/step - loss: 1.1499 - mse: 1.1499\n",
      "Epoch 387/500\n",
      "506/506 [==============================] - 0s 148us/step - loss: 1.2600 - mse: 1.2600\n",
      "Epoch 388/500\n",
      "506/506 [==============================] - 0s 153us/step - loss: 1.5743 - mse: 1.5743\n",
      "Epoch 389/500\n",
      "506/506 [==============================] - 0s 144us/step - loss: 2.7598 - mse: 2.7598\n",
      "Epoch 390/500\n",
      "506/506 [==============================] - 0s 146us/step - loss: 1.1528 - mse: 1.1528\n",
      "Epoch 391/500\n",
      "506/506 [==============================] - 0s 140us/step - loss: 1.2304 - mse: 1.2304\n",
      "Epoch 392/500\n",
      "506/506 [==============================] - 0s 142us/step - loss: 1.1495 - mse: 1.1495\n",
      "Epoch 393/500\n",
      "506/506 [==============================] - 0s 140us/step - loss: 1.0877 - mse: 1.0877\n",
      "Epoch 394/500\n",
      "506/506 [==============================] - 0s 142us/step - loss: 1.1172 - mse: 1.1172\n",
      "Epoch 395/500\n",
      "506/506 [==============================] - 0s 136us/step - loss: 1.1297 - mse: 1.1297\n",
      "Epoch 396/500\n",
      "506/506 [==============================] - 0s 144us/step - loss: 1.1210 - mse: 1.1210\n",
      "Epoch 397/500\n",
      "506/506 [==============================] - 0s 136us/step - loss: 1.1086 - mse: 1.1086\n",
      "Epoch 398/500\n",
      "506/506 [==============================] - 0s 142us/step - loss: 1.1690 - mse: 1.1690\n",
      "Epoch 399/500\n",
      "506/506 [==============================] - 0s 142us/step - loss: 2.9216 - mse: 2.9216\n",
      "Epoch 400/500\n",
      "506/506 [==============================] - 0s 168us/step - loss: 1.3879 - mse: 1.3879\n",
      "Epoch 401/500\n",
      "506/506 [==============================] - 0s 148us/step - loss: 1.2049 - mse: 1.2049\n",
      "Epoch 402/500\n",
      "506/506 [==============================] - 0s 140us/step - loss: 1.1964 - mse: 1.1964\n",
      "Epoch 403/500\n",
      "506/506 [==============================] - 0s 135us/step - loss: 1.2459 - mse: 1.2459\n",
      "Epoch 404/500\n",
      "506/506 [==============================] - 0s 150us/step - loss: 1.0440 - mse: 1.0440\n",
      "Epoch 405/500\n",
      "506/506 [==============================] - 0s 141us/step - loss: 1.0003 - mse: 1.0003\n",
      "Epoch 406/500\n",
      "506/506 [==============================] - 0s 157us/step - loss: 1.4611 - mse: 1.4611\n",
      "Epoch 407/500\n",
      "506/506 [==============================] - 0s 155us/step - loss: 1.1714 - mse: 1.1714\n",
      "Epoch 408/500\n",
      "506/506 [==============================] - 0s 154us/step - loss: 0.9037 - mse: 0.9037\n",
      "Epoch 409/500\n",
      "506/506 [==============================] - 0s 142us/step - loss: 1.1880 - mse: 1.1880\n",
      "Epoch 410/500\n",
      "506/506 [==============================] - 0s 138us/step - loss: 0.8514 - mse: 0.8514\n",
      "Epoch 411/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "506/506 [==============================] - 0s 139us/step - loss: 1.0207 - mse: 1.0207\n",
      "Epoch 412/500\n",
      "506/506 [==============================] - 0s 140us/step - loss: 1.4893 - mse: 1.4893\n",
      "Epoch 413/500\n",
      "506/506 [==============================] - 0s 171us/step - loss: 1.1440 - mse: 1.1440\n",
      "Epoch 414/500\n",
      "506/506 [==============================] - 0s 130us/step - loss: 1.0070 - mse: 1.0070\n",
      "Epoch 415/500\n",
      "506/506 [==============================] - 0s 132us/step - loss: 1.0649 - mse: 1.0649\n",
      "Epoch 416/500\n",
      "506/506 [==============================] - 0s 130us/step - loss: 0.9619 - mse: 0.9619\n",
      "Epoch 417/500\n",
      "506/506 [==============================] - 0s 128us/step - loss: 1.0879 - mse: 1.0879\n",
      "Epoch 418/500\n",
      "506/506 [==============================] - 0s 138us/step - loss: 1.0233 - mse: 1.0233\n",
      "Epoch 419/500\n",
      "506/506 [==============================] - 0s 131us/step - loss: 1.0402 - mse: 1.0402\n",
      "Epoch 420/500\n",
      "506/506 [==============================] - 0s 122us/step - loss: 1.0081 - mse: 1.0081\n",
      "Epoch 421/500\n",
      "506/506 [==============================] - 0s 122us/step - loss: 0.9362 - mse: 0.9362\n",
      "Epoch 422/500\n",
      "506/506 [==============================] - 0s 185us/step - loss: 0.8935 - mse: 0.8935\n",
      "Epoch 423/500\n",
      "506/506 [==============================] - 0s 122us/step - loss: 1.0145 - mse: 1.0145\n",
      "Epoch 424/500\n",
      "506/506 [==============================] - 0s 120us/step - loss: 0.9595 - mse: 0.9595\n",
      "Epoch 425/500\n",
      "506/506 [==============================] - 0s 122us/step - loss: 1.0803 - mse: 1.0803\n",
      "Epoch 426/500\n",
      "506/506 [==============================] - 0s 111us/step - loss: 1.4937 - mse: 1.4937\n",
      "Epoch 427/500\n",
      "506/506 [==============================] - 0s 144us/step - loss: 1.0023 - mse: 1.0023\n",
      "Epoch 428/500\n",
      "506/506 [==============================] - 0s 116us/step - loss: 1.1697 - mse: 1.1697\n",
      "Epoch 429/500\n",
      "506/506 [==============================] - 0s 118us/step - loss: 1.0242 - mse: 1.0242\n",
      "Epoch 430/500\n",
      "506/506 [==============================] - 0s 108us/step - loss: 1.3803 - mse: 1.3803\n",
      "Epoch 431/500\n",
      "506/506 [==============================] - 0s 116us/step - loss: 1.9285 - mse: 1.9285\n",
      "Epoch 432/500\n",
      "506/506 [==============================] - 0s 125us/step - loss: 1.1504 - mse: 1.1504\n",
      "Epoch 433/500\n",
      "506/506 [==============================] - 0s 119us/step - loss: 1.0274 - mse: 1.0274\n",
      "Epoch 434/500\n",
      "506/506 [==============================] - 0s 124us/step - loss: 0.8866 - mse: 0.8866\n",
      "Epoch 435/500\n",
      "506/506 [==============================] - 0s 190us/step - loss: 1.2030 - mse: 1.2030\n",
      "Epoch 436/500\n",
      "506/506 [==============================] - 0s 102us/step - loss: 1.2424 - mse: 1.2424\n",
      "Epoch 437/500\n",
      "506/506 [==============================] - 0s 106us/step - loss: 1.1553 - mse: 1.1553\n",
      "Epoch 438/500\n",
      "506/506 [==============================] - 0s 104us/step - loss: 0.9602 - mse: 0.9602\n",
      "Epoch 439/500\n",
      "506/506 [==============================] - 0s 107us/step - loss: 1.1307 - mse: 1.1307\n",
      "Epoch 440/500\n",
      "506/506 [==============================] - 0s 190us/step - loss: 0.8304 - mse: 0.8304\n",
      "Epoch 441/500\n",
      "506/506 [==============================] - 0s 126us/step - loss: 0.8987 - mse: 0.8987\n",
      "Epoch 442/500\n",
      "506/506 [==============================] - 0s 138us/step - loss: 0.8541 - mse: 0.8541\n",
      "Epoch 443/500\n",
      "506/506 [==============================] - 0s 136us/step - loss: 0.8790 - mse: 0.8790\n",
      "Epoch 444/500\n",
      "506/506 [==============================] - 0s 134us/step - loss: 1.0099 - mse: 1.0099\n",
      "Epoch 445/500\n",
      "506/506 [==============================] - 0s 106us/step - loss: 1.6698 - mse: 1.6698\n",
      "Epoch 446/500\n",
      "506/506 [==============================] - 0s 108us/step - loss: 0.9385 - mse: 0.9385\n",
      "Epoch 447/500\n",
      "506/506 [==============================] - 0s 110us/step - loss: 1.4670 - mse: 1.4670\n",
      "Epoch 448/500\n",
      "506/506 [==============================] - 0s 150us/step - loss: 0.9497 - mse: 0.9497\n",
      "Epoch 449/500\n",
      "506/506 [==============================] - 0s 206us/step - loss: 1.2408 - mse: 1.2408\n",
      "Epoch 450/500\n",
      "506/506 [==============================] - 0s 112us/step - loss: 1.1409 - mse: 1.1409\n",
      "Epoch 451/500\n",
      "506/506 [==============================] - 0s 99us/step - loss: 0.9199 - mse: 0.9199\n",
      "Epoch 452/500\n",
      "506/506 [==============================] - 0s 114us/step - loss: 0.9935 - mse: 0.9935\n",
      "Epoch 453/500\n",
      "506/506 [==============================] - 0s 91us/step - loss: 0.9401 - mse: 0.9401\n",
      "Epoch 454/500\n",
      "506/506 [==============================] - 0s 102us/step - loss: 0.8232 - mse: 0.8232\n",
      "Epoch 455/500\n",
      "506/506 [==============================] - 0s 102us/step - loss: 0.9839 - mse: 0.9839\n",
      "Epoch 456/500\n",
      "506/506 [==============================] - 0s 174us/step - loss: 0.8168 - mse: 0.8168\n",
      "Epoch 457/500\n",
      "506/506 [==============================] - 0s 108us/step - loss: 1.6175 - mse: 1.6175\n",
      "Epoch 458/500\n",
      "506/506 [==============================] - 0s 108us/step - loss: 1.0269 - mse: 1.0269\n",
      "Epoch 459/500\n",
      "506/506 [==============================] - 0s 115us/step - loss: 0.9100 - mse: 0.9100\n",
      "Epoch 460/500\n",
      "506/506 [==============================] - 0s 112us/step - loss: 0.9313 - mse: 0.9313\n",
      "Epoch 461/500\n",
      "506/506 [==============================] - 0s 122us/step - loss: 1.7147 - mse: 1.7147\n",
      "Epoch 462/500\n",
      "506/506 [==============================] - 0s 131us/step - loss: 1.1181 - mse: 1.1181\n",
      "Epoch 463/500\n",
      "506/506 [==============================] - 0s 130us/step - loss: 1.2868 - mse: 1.2868\n",
      "Epoch 464/500\n",
      "506/506 [==============================] - 0s 124us/step - loss: 1.1886 - mse: 1.1886\n",
      "Epoch 465/500\n",
      "506/506 [==============================] - 0s 119us/step - loss: 1.4066 - mse: 1.4066\n",
      "Epoch 466/500\n",
      "506/506 [==============================] - 0s 122us/step - loss: 0.9902 - mse: 0.9902\n",
      "Epoch 467/500\n",
      "506/506 [==============================] - 0s 131us/step - loss: 0.8664 - mse: 0.8664\n",
      "Epoch 468/500\n",
      "506/506 [==============================] - 0s 121us/step - loss: 0.8583 - mse: 0.8583\n",
      "Epoch 469/500\n",
      "506/506 [==============================] - 0s 118us/step - loss: 0.8711 - mse: 0.8711\n",
      "Epoch 470/500\n",
      "506/506 [==============================] - 0s 123us/step - loss: 1.0002 - mse: 1.0002\n",
      "Epoch 471/500\n",
      "506/506 [==============================] - 0s 124us/step - loss: 1.0875 - mse: 1.0875\n",
      "Epoch 472/500\n",
      "506/506 [==============================] - 0s 110us/step - loss: 1.1744 - mse: 1.1744\n",
      "Epoch 473/500\n",
      "506/506 [==============================] - 0s 127us/step - loss: 0.9470 - mse: 0.9470\n",
      "Epoch 474/500\n",
      "506/506 [==============================] - 0s 126us/step - loss: 0.9401 - mse: 0.9401\n",
      "Epoch 475/500\n",
      "506/506 [==============================] - 0s 126us/step - loss: 0.8801 - mse: 0.8801\n",
      "Epoch 476/500\n",
      "506/506 [==============================] - 0s 129us/step - loss: 1.0739 - mse: 1.0739\n",
      "Epoch 477/500\n",
      "506/506 [==============================] - 0s 124us/step - loss: 0.8011 - mse: 0.8011\n",
      "Epoch 478/500\n",
      "506/506 [==============================] - 0s 124us/step - loss: 0.8266 - mse: 0.8266\n",
      "Epoch 479/500\n",
      "506/506 [==============================] - 0s 120us/step - loss: 1.6939 - mse: 1.6939\n",
      "Epoch 480/500\n",
      "506/506 [==============================] - 0s 124us/step - loss: 1.1237 - mse: 1.1237\n",
      "Epoch 481/500\n",
      "506/506 [==============================] - 0s 127us/step - loss: 0.8705 - mse: 0.8705\n",
      "Epoch 482/500\n",
      "506/506 [==============================] - 0s 128us/step - loss: 1.0562 - mse: 1.0562\n",
      "Epoch 483/500\n",
      "506/506 [==============================] - 0s 133us/step - loss: 0.8083 - mse: 0.8083\n",
      "Epoch 484/500\n",
      "506/506 [==============================] - 0s 122us/step - loss: 1.1050 - mse: 1.1050\n",
      "Epoch 485/500\n",
      "506/506 [==============================] - 0s 121us/step - loss: 1.8579 - mse: 1.8579\n",
      "Epoch 486/500\n",
      "506/506 [==============================] - 0s 113us/step - loss: 2.4292 - mse: 2.4292\n",
      "Epoch 487/500\n",
      "506/506 [==============================] - 0s 126us/step - loss: 1.0571 - mse: 1.0571\n",
      "Epoch 488/500\n",
      "506/506 [==============================] - 0s 203us/step - loss: 1.0359 - mse: 1.0359\n",
      "Epoch 489/500\n",
      "506/506 [==============================] - 0s 116us/step - loss: 0.8374 - mse: 0.8374\n",
      "Epoch 490/500\n",
      "506/506 [==============================] - 0s 110us/step - loss: 1.1862 - mse: 1.1862\n",
      "Epoch 491/500\n",
      "506/506 [==============================] - 0s 114us/step - loss: 1.1595 - mse: 1.1595\n",
      "Epoch 492/500\n",
      "506/506 [==============================] - 0s 112us/step - loss: 0.8575 - mse: 0.8575\n",
      "Epoch 493/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "506/506 [==============================] - 0s 113us/step - loss: 0.6595 - mse: 0.6595\n",
      "Epoch 494/500\n",
      "506/506 [==============================] - 0s 109us/step - loss: 1.4131 - mse: 1.4131\n",
      "Epoch 495/500\n",
      "506/506 [==============================] - 0s 150us/step - loss: 1.4390 - mse: 1.4390\n",
      "Epoch 496/500\n",
      "506/506 [==============================] - 0s 185us/step - loss: 1.3222 - mse: 1.3222\n",
      "Epoch 497/500\n",
      "506/506 [==============================] - 0s 105us/step - loss: 1.1097 - mse: 1.1097\n",
      "Epoch 498/500\n",
      "506/506 [==============================] - 0s 108us/step - loss: 0.8973 - mse: 0.8973\n",
      "Epoch 499/500\n",
      "506/506 [==============================] - 0s 148us/step - loss: 0.7656 - mse: 0.7656\n",
      "Epoch 500/500\n",
      "506/506 [==============================] - 0s 157us/step - loss: 1.1343 - mse: 1.1343\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_, y_, epochs=500, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x28e9a3ade48>]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.history['loss'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 回答一下理论题目"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. What does a neuron compute?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "一个神经元是一个线性函数 y=Wx+b, 后面可能还跟着一个激活函数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  2. Why we use non-linear activation funcitons in neural networks?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.如果没有引入非线性激活函数，全是线性函数，那么无论多少层神经网络，都能化简为一层神经网络。  \n",
    "2.世界是复杂的，不是纯线性函数就可以拟合的，比如引入非线性函数，才可以更好地拟合复杂的世界。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. What is the 'Logistic Loss' ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logitstic Loss是Cross Entropy Loss在二分类场景中的特例。  \n",
    "loss = -(ylog(y^) + (1-y)log(1-y^))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Assume that you are building a binary classifier for detecting if an image containing cats, which activation functions would you recommen using for the output layer ?\n",
    "\n",
    "A. ReLU    \n",
    "B. Leaky ReLU    \n",
    "C. sigmoid    \n",
    "D. tanh  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "推荐使用sigmoid激活函数。sigmoid的取值范围是[0,1]，比较符合二分类场景。  \n",
    "relu范围是[0, +∞],leaky relu也是从0附近到+∞。  \n",
    "tanh范围是[-1,1].  \n",
    "相比之下，sigmoid最适合二分类。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Why we don't use zero initialization for all parameters ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "神经网络具有对称性。如果初始化全0，那么在这样的对称性结构下，反向传播/梯度下降计算后，相同层的多个神经元参数会相同，模型难以收敛。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Can you implement the softmax function using python ? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def softmax(y_input):\n",
    "    # 行为种类，每列为一个样本标签\n",
    "    temp = np.exp(y_input)\n",
    "    return temp/np.sum(temp, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.01798621, 0.04742587, 0.11920292, 0.26894142],\n",
       "       [0.98201379, 0.95257413, 0.88079708, 0.73105858]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax([[1,2,3,4], [5,5,5,5]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.实践题"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In this practical part, you will build a simple digits recognizer to check if the digit in the image is larger than 5. This assignmnet will guide you step by step to finish your first small project in this course ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1 - Packages  \n",
    "sklearn is a famous package for machine learning.   \n",
    "matplotlib is a common package for vasualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2 - Overvie of the dataset  \n",
    "    - a training set has m_train images labeled as 0 if the digit < 5 or 1 if the digit >= 5\n",
    "    - a test set contains m_test images labels as if the digit < 5 or 1 if the digit >= 5\n",
    "    - eah image if of shape (num_px, num_px ). Thus, each image is square(height=num_px and  width = num_px)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the data \n",
    "digits = datasets.load_digits()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x720 with 40 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Vilizating the data\n",
    "plt.figure(figsize=(15,10))\n",
    "for i in range(1,41):\n",
    "    plt.subplot(4,10,i)\n",
    "    plt.imshow(digits.data[i-1].reshape([8,8]),cmap=plt.cm.gray_r)\n",
    "    plt.text(3,10,str(digits.target[i-1]))\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training set and test set \n",
    "X_train, X_test, y_train, y_test = train_test_split(digits.data, digits.target, test_size=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reformulate the label. \n",
    "# If the digit is smaller than 5, the label is 0.\n",
    "# If the digit is larger than 5, the label is 1.\n",
    "\n",
    "y_train[y_train < 5 ] = 0\n",
    "y_train[y_train >= 5] = 1\n",
    "y_test[y_test < 5] = 0\n",
    "y_test[y_test >= 5] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1347, 64)\n",
      "(450, 64)\n",
      "(1347,)\n",
      "(450,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3- Architecture of the neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](./networks.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 直接用sklearn内部的逻辑斯蒂回归函数计算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\myx\\Anaconda3\\envs\\tf2.0\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "lr = LogisticRegression()\n",
    "lr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0,\n",
       "       0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0,\n",
       "       0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0,\n",
       "       0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1,\n",
       "       1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0,\n",
       "       1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0,\n",
       "       0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1,\n",
       "       0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1,\n",
       "       1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1,\n",
       "       1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1,\n",
       "       0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0,\n",
       "       0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0,\n",
       "       1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1,\n",
       "       1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1,\n",
       "       1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0,\n",
       "       0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0,\n",
       "       0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0,\n",
       "       1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0,\n",
       "       0, 0, 1, 0, 0, 0, 1, 0, 0, 1])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pre = lr.predict(X_test)\n",
    "y_pre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8955555555555555"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 计算预测的正确率\n",
    "np.mean(y_pre == y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8955555555555555"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 用自带的函数计算预测正确率\n",
    "lr.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用sklearn的神经网络模型计算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='relu', alpha=1e-05, batch_size='auto', beta_1=0.9,\n",
       "              beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "              hidden_layer_sizes=(10, 2), learning_rate='constant',\n",
       "              learning_rate_init=0.001, max_fun=15000, max_iter=200,\n",
       "              momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
       "              power_t=0.5, random_state=1, shuffle=True, solver='lbfgs',\n",
       "              tol=0.0001, validation_fraction=0.1, verbose=False,\n",
       "              warm_start=False)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "clf = MLPClassifier(solver='lbfgs', alpha=1e-5,\n",
    "                     hidden_layer_sizes=(10, 2), random_state=1)\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9555555555555556"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 计算正确率\n",
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Mathematical expression of the algorithm:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For one example $x^{(i)}$:   \n",
    " $$ z^{(i)} = w^T * x^{(i)} +b $$   \n",
    " $$ y^{(i)} = a^{(i)} = sigmoid(z^{(i)})$$   \n",
    " $$L(a^{(i)},y^{(i)}) = -y^{(i)} log(a^{(i)})-(1-y^{(i)})log(1-a^{(i)})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The total cost over all training examples:\n",
    "$$ J = \\frac{1}{m}\\sum_{i=1}^{m}L(a^{(i)},y^{(i)}) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4 - Building the algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.1- Activation function    \n",
    "###### Exercise:\n",
    "Finish the sigmoid funciton "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    '''\n",
    "    Compute the sigmoid of z\n",
    "    Arguments: z -- a scalar or numpy array of any size.\n",
    "    \n",
    "    Return:\n",
    "    s -- sigmoid(z)\n",
    "    '''\n",
    "    s = 1 / (1 + np.exp(-z))\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sigmoid([0,2]) = [0.5        0.88079708]\n",
      "sigmoid([[0,2], [0,2]]) = [[0.5        0.88079708]\n",
      " [0.5        0.88079708]]\n"
     ]
    }
   ],
   "source": [
    "# Test your code \n",
    "# The result should be [0.5 0.88079708]\n",
    "print(\"sigmoid([0,2]) = \" + str(sigmoid(np.array([0,2]))))\n",
    "print(\"sigmoid([[0,2], [0,2]]) = \" + str(sigmoid(np.array([[0,2], [0,2]]))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.1-Initializaing parameters\n",
    "###### Exercise:\n",
    "Finishe the initialize_parameters function below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random innitialize the parameters\n",
    "\n",
    "def initialize_parameters(dim):\n",
    "    '''\n",
    "    Argument: dim -- size of the w vector\n",
    "    \n",
    "    Returns:\n",
    "    w -- initialized vector of shape (dim,1)\n",
    "    b -- initializaed scalar\n",
    "    '''\n",
    "    \n",
    "    w = np.random.random((dim,1))\n",
    "    b = 0\n",
    "\n",
    "    assert(w.shape == (dim,1))\n",
    "    assert(isinstance(b,float) or isinstance(b,int))\n",
    "    \n",
    "    return w,b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3-Forward and backward propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Some mathematical expressions\n",
    "Forward Propagation:   \n",
    ". X    \n",
    ". A = $\\sigma(w^T*X+b) = (a^{(1)},a^{(2)},...,a^{(m)}$   \n",
    ". J = $-\\frac{1}{m} \\sum_{i=1}^{m}y^{(i)}log(a^{(i)}+(1-y^{(i)})log(1-a^{(i)})$       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some derivative: \n",
    "$$\\frac{\\partial{J}}{\\partial{w}} = \\frac{1}{m}X*(A-Y)^T$$   \n",
    "$$\\frac{\\partial{J}}{\\partial{b}} = \\frac{1}{m}\\sum_{i=1}^m(a^{(i)}-y^{(i)}) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Exercise:\n",
    "Finish the function below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def propagate(w,b,X,Y):\n",
    "    '''\n",
    "    Implement the cost function and its gradient for the propagation\n",
    "    \n",
    "    Arguments:\n",
    "    w - weights\n",
    "    b - bias\n",
    "    X - data\n",
    "    Y - ground truth\n",
    "    '''\n",
    "    m = X.shape[1]\n",
    "    A = sigmoid(np.dot(w.T, X) + b)\n",
    "    temp = Y * np.log(A) + (1 - Y) * np.log(1 - A)\n",
    "\n",
    "    cost = -1 / m * np.sum(Y * np.log(A) + (1 - Y) * np.log(1 - A), axis=1)\n",
    "    \n",
    "    dw = 1 / m * np.dot(X, (A - Y).T)\n",
    "    db = 1 / m * np.sum(A - Y, axis=1)\n",
    "    \n",
    "    assert(dw.shape == w.shape)\n",
    "    assert(db.dtype == float)\n",
    "    cost = np.squeeze(cost)\n",
    "    assert(cost.shape == ())\n",
    "    \n",
    "    grads = {'dw':dw,\n",
    "             'db':db}\n",
    "    return grads, cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.4 -Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Exercise:\n",
    "Minimizing the cost function using gradient descent.   \n",
    "$$\\theta = \\theta - \\alpha*d\\theta$$ where $\\alpha$ is the learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize(w, b, X, Y, num_iterations, learning_rate, print_cost=False):\n",
    "    '''\n",
    "    This function optimize w and b by running a gradient descen algorithm\n",
    "    \n",
    "    Arguments:\n",
    "    w - weights\n",
    "    b - bias\n",
    "    X - data\n",
    "    Y - ground truth\n",
    "    num_iterations -- number of iterations of the optimization loop\n",
    "    learning_rate -- learning rate of the gradient descent update rule\n",
    "    print_cost -- True to print the loss every 100 steps\n",
    "    \n",
    "    Returns:\n",
    "    params - dictionary containing the weights w and bias b\n",
    "    grads -- dictionary containing the gradients of the weights and bias with respect to the cost function\n",
    "    costs -- list of all the costs computed during the optimization, this will be used to plot the learning curve.\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    batch_size = 50\n",
    "    for j in range(num_iterations):\n",
    "        costs = []\n",
    "        for i in range(0, X.shape[1], batch_size):\n",
    "            X_ = X[:,i:i+batch_size]\n",
    "            Y_ = Y[:,i:i+batch_size]\n",
    "\n",
    "            grads, cost = propagate(w,b,X_,Y_)\n",
    "\n",
    "            dw = grads['dw']\n",
    "            db = grads['db']\n",
    "\n",
    "            w -= learning_rate * dw\n",
    "            b -= learning_rate * db\n",
    "\n",
    "            costs.append(cost)\n",
    "        \n",
    "        cost = sum(costs)/len(costs)\n",
    "        if print_cost and j % 100 == 0:\n",
    "            print (\"Cost after iteration %i: %f\" %(j, cost))\n",
    "    \n",
    "    params = {\"w\":w,\n",
    "              \"b\":b}\n",
    "    \n",
    "    grads = {\"dw\":dw,\n",
    "             \"db\":db}\n",
    "     \n",
    "    return params, grads, cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Exercise\n",
    "The previous function will output the learned w and b. We are able to use w and b to predict the labels for a dataset X. Implement the predict() function.    \n",
    "Two steps to finish this task:   \n",
    "1. Calculate $\\hat{Y} = A = \\sigma(w^T*X+b)$   \n",
    "2. Convert the entries of a into 0 (if activation <= 0.5) or 1 (if activation > 0.5), stores the predictions in a vector Y_prediction. If you wish, you can use an if/else statement in a for loop (though there is also a way to vectorize this)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(w, b, X):\n",
    "    '''\n",
    "    Predict whether the label is 0 or 1 using learned logistic regression parameters (w, b)\n",
    "    \n",
    "    Arguments:\n",
    "    w -- weights\n",
    "    b -- bias \n",
    "    X -- data \n",
    "    \n",
    "    Returns:\n",
    "    Y_prediction -- a numpy array (vector) containing all predictions (0/1) for the examples in X\n",
    "    '''\n",
    "    m = X.shape[1]\n",
    "    Y_prediction = np.zeros((1,m))\n",
    "    w = w.reshape(X.shape[0],1)\n",
    "    \n",
    "    A = sigmoid(np.dot(w.T, X) + b)\n",
    "    \n",
    "    for i in range(A.shape[1]):\n",
    "        Y_prediction[0,i] = 0 if A[0,i] <= 0.5 else 1\n",
    "    \n",
    "    assert(Y_prediction.shape == (1,m))\n",
    "    \n",
    "    return Y_prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5- Merge all functions into a model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations !! You have finished all the necessary components for constructing a model. Now, Let's take the challenge to merge all the implemented function into one model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(X_train, Y_train, X_test, Y_test, num_iterations, learning_rate, print_cost):\n",
    "    \"\"\"\n",
    "    Build the logistic regression model by calling all the functions you have implemented.\n",
    "    Arguments:\n",
    "    X_train - training set\n",
    "    Y_train - training label\n",
    "    X_test - test set\n",
    "    Y_test - test label\n",
    "    num_iteration - hyperparameter representing the number of iterations to optimize the parameters\n",
    "    learning_rate -- hyperparameter representing the learning rate used in the update rule of optimize()\n",
    "    print_cost -- Set to true to print the cost every 100 iterations\n",
    "    \n",
    "    Returns:\n",
    "    d - dictionary should contain following information w,b,training_accuracy, test_accuracy,cost\n",
    "    eg: d = {\"w\":w,\n",
    "             \"b\":b,\n",
    "             \"training_accuracy\": traing_accuracy,\n",
    "             \"test_accuracy\":test_accuracy,\n",
    "             \"cost\":cost}\n",
    "    \"\"\"\n",
    "#     # X标准化处理\n",
    "#     X_train /= X_train.max()\n",
    "#     X_test /= X_train.max()\n",
    "#     # XY参数格式调整\n",
    "#     X_train = X_train.T\n",
    "#     Y_train = np.expand_dims(Y_train, axis=0)\n",
    "#     X_test = X_test.T\n",
    "#     Y_test = np.expand_dims(Y_test, axis=0)\n",
    "    \n",
    "    # 参数初始化\n",
    "    w, b = initialize_parameters(X_train.shape[0])\n",
    "    \n",
    "    # 训练模型\n",
    "    params, grads, costs = optimize(w, b, X_train, Y_train, num_iterations, learning_rate, print_cost)\n",
    "    w = params['w']\n",
    "    b = params['b']\n",
    "            \n",
    "    # 训练集精度\n",
    "    Y_train_ = predict(w, b, X_train)\n",
    "    traing_accuracy = np.mean(Y_train_ == Y_train)\n",
    "           \n",
    "    # 测试集\n",
    "    Y_pre = predict(w, b, X_test)\n",
    "    test_accuracy = np.mean(Y_pre == Y_test)\n",
    "    \n",
    "    d = {\"w\":w,\n",
    "         \"b\":b,\n",
    "         \"training_accuracy\": traing_accuracy,\n",
    "         \"test_accuracy\":test_accuracy,\n",
    "         \"cost\":costs}\n",
    "    \n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获取数据\n",
    "X_train, X_test, y_train, y_test = train_test_split(digits.data, digits.target, test_size=0.25)\n",
    "y_train[y_train < 5 ] = 0\n",
    "y_train[y_train >= 5] = 1\n",
    "y_test[y_test < 5] = 0\n",
    "y_test[y_test >= 5] = 1\n",
    "\n",
    "# 数据预处理\n",
    "# X标准化处理\n",
    "X_train /= X_train.max()\n",
    "X_test /= X_train.max()\n",
    "# XY参数格式调整\n",
    "X_train = X_train.T\n",
    "y_train = np.expand_dims(y_train, axis=0)\n",
    "X_test = X_test.T\n",
    "y_test = np.expand_dims(y_test, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: 2.038772\n",
      "Cost after iteration 100: 0.281880\n",
      "Cost after iteration 200: 0.263894\n",
      "Cost after iteration 300: 0.257891\n",
      "Cost after iteration 400: 0.255150\n",
      "Cost after iteration 500: 0.253671\n",
      "Cost after iteration 600: 0.252778\n",
      "Cost after iteration 700: 0.252191\n",
      "Cost after iteration 800: 0.251779\n",
      "Cost after iteration 900: 0.251472\n",
      "Cost after iteration 1000: 0.251235\n",
      "Cost after iteration 1100: 0.251043\n",
      "Cost after iteration 1200: 0.250884\n",
      "Cost after iteration 1300: 0.250747\n",
      "Cost after iteration 1400: 0.250628\n",
      "Cost after iteration 1500: 0.250522\n",
      "Cost after iteration 1600: 0.250427\n",
      "Cost after iteration 1700: 0.250339\n",
      "Cost after iteration 1800: 0.250257\n",
      "Cost after iteration 1900: 0.250181\n",
      "Cost after iteration 2000: 0.250110\n",
      "Cost after iteration 2100: 0.250042\n",
      "Cost after iteration 2200: 0.249977\n",
      "Cost after iteration 2300: 0.249915\n",
      "Cost after iteration 2400: 0.249856\n",
      "Cost after iteration 2500: 0.249799\n",
      "Cost after iteration 2600: 0.249744\n",
      "Cost after iteration 2700: 0.249691\n",
      "Cost after iteration 2800: 0.249639\n",
      "Cost after iteration 2900: 0.249589\n",
      "Cost after iteration 3000: 0.249541\n",
      "Cost after iteration 3100: 0.249493\n",
      "Cost after iteration 3200: 0.249448\n",
      "Cost after iteration 3300: 0.249403\n",
      "Cost after iteration 3400: 0.249360\n",
      "Cost after iteration 3500: 0.249317\n",
      "Cost after iteration 3600: 0.249276\n",
      "Cost after iteration 3700: 0.249236\n",
      "Cost after iteration 3800: 0.249197\n",
      "Cost after iteration 3900: 0.249158\n",
      "Cost after iteration 4000: 0.249121\n",
      "Cost after iteration 4100: 0.249084\n",
      "Cost after iteration 4200: 0.249049\n",
      "Cost after iteration 4300: 0.249014\n",
      "Cost after iteration 4400: 0.248980\n",
      "Cost after iteration 4500: 0.248946\n",
      "Cost after iteration 4600: 0.248913\n",
      "Cost after iteration 4700: 0.248881\n",
      "Cost after iteration 4800: 0.248850\n",
      "Cost after iteration 4900: 0.248819\n",
      "{'w': array([[ 0.90582477],\n",
      "       [ 1.41667418],\n",
      "       [ 0.12539664],\n",
      "       [-0.55012586],\n",
      "       [ 1.44198985],\n",
      "       [ 1.93318069],\n",
      "       [ 2.4024893 ],\n",
      "       [-2.25917051],\n",
      "       [ 2.72516119],\n",
      "       [-2.45660077],\n",
      "       [ 2.17824503],\n",
      "       [ 1.23345879],\n",
      "       [-0.73499049],\n",
      "       [-1.31586604],\n",
      "       [ 0.87102015],\n",
      "       [ 2.47299964],\n",
      "       [-0.2504697 ],\n",
      "       [ 1.6017777 ],\n",
      "       [ 2.16521713],\n",
      "       [-0.40334247],\n",
      "       [-3.67292472],\n",
      "       [-0.74116719],\n",
      "       [-2.19134692],\n",
      "       [-1.80644709],\n",
      "       [-0.28180952],\n",
      "       [-3.27477608],\n",
      "       [ 0.42583167],\n",
      "       [ 2.91194897],\n",
      "       [ 0.28061382],\n",
      "       [ 2.71842566],\n",
      "       [-2.9138889 ],\n",
      "       [-0.4764152 ],\n",
      "       [ 0.73985135],\n",
      "       [-4.2143363 ],\n",
      "       [-0.14265004],\n",
      "       [ 3.25482387],\n",
      "       [-1.65507939],\n",
      "       [-0.63145549],\n",
      "       [ 1.06255352],\n",
      "       [ 0.10880534],\n",
      "       [ 0.81345678],\n",
      "       [ 0.17393175],\n",
      "       [ 0.88949703],\n",
      "       [-0.56572986],\n",
      "       [ 2.25692215],\n",
      "       [ 1.11321867],\n",
      "       [ 1.37997556],\n",
      "       [ 0.85361644],\n",
      "       [ 0.873953  ],\n",
      "       [-1.35028705],\n",
      "       [ 0.89909879],\n",
      "       [-1.04758086],\n",
      "       [-4.86912646],\n",
      "       [-0.74395601],\n",
      "       [ 1.18936096],\n",
      "       [ 0.73945446],\n",
      "       [ 0.29444957],\n",
      "       [-1.08853628],\n",
      "       [ 0.48187695],\n",
      "       [-0.71958275],\n",
      "       [-2.30815247],\n",
      "       [-1.07145295],\n",
      "       [-1.15778018],\n",
      "       [-4.27538341]]), 'b': array([-0.30954654]), 'training_accuracy': 0.9094283593170007, 'test_accuracy': 0.9111111111111111, 'cost': 0.2487895012147262}\n"
     ]
    }
   ],
   "source": [
    "d = model(X_train, y_train, X_test, y_test, 5000, 1e-1, True)\n",
    "print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 1., 1., 1., 0., 1., 1., 1., 0., 0., 1., 0., 1., 1., 0., 0.,\n",
       "        1., 0., 1., 1., 0., 0., 1., 1., 0., 0., 1., 0., 1., 0., 1., 0.,\n",
       "        0., 1., 0., 0., 1., 0., 1., 1., 0., 0., 1., 0., 0., 0., 0., 1.,\n",
       "        1., 0., 1., 1., 1., 0., 1., 0., 0., 1., 0., 0., 1., 1., 0., 1.,\n",
       "        0., 0., 0., 0., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0.,\n",
       "        1., 1., 0., 1., 0., 1., 0., 1., 0., 1., 1., 0., 0., 1., 1., 0.,\n",
       "        0., 0., 1., 0., 0., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 0.,\n",
       "        1., 0., 0., 1., 1., 0., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1.,\n",
       "        1., 0., 1., 0., 0., 1., 0., 0., 1., 1., 1., 1., 1., 0., 0., 1.,\n",
       "        0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 0., 1., 1., 0., 1.,\n",
       "        1., 1., 0., 0., 1., 0., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0.,\n",
       "        0., 1., 1., 0., 0., 1., 1., 0., 1., 1., 1., 0., 0., 0., 0., 0.,\n",
       "        1., 0., 1., 0., 0., 1., 0., 0., 1., 1., 1., 1., 0., 0., 0., 1.,\n",
       "        1., 0., 1., 0., 1., 1., 1., 0., 1., 0., 0., 0., 0., 1., 1., 1.,\n",
       "        1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 0., 1., 0., 0., 0., 0.,\n",
       "        1., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 1., 1., 1., 1., 0.,\n",
       "        1., 0., 0., 1., 0., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 0.,\n",
       "        1., 1., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 0., 0.,\n",
       "        1., 0., 1., 0., 1., 0., 0., 1., 0., 0., 1., 0., 1., 0., 1., 0.,\n",
       "        1., 0., 0., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1.,\n",
       "        0., 0., 1., 1., 0., 1., 0., 0., 1., 0., 0., 0., 1., 1., 1., 1.,\n",
       "        1., 0., 1., 0., 0., 1., 0., 0., 1., 0., 0., 0., 1., 0., 1., 0.,\n",
       "        0., 0., 1., 1., 0., 0., 0., 0., 1., 1., 1., 1., 0., 0., 0., 1.,\n",
       "        1., 0., 0., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 0.,\n",
       "        1., 1., 0., 1., 0., 1., 1., 0., 0., 1., 0., 1., 1., 0., 0., 1.,\n",
       "        1., 1., 0., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 1., 0., 1.,\n",
       "        1., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0.,\n",
       "        1., 0., 1., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
       "        1., 0.]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pre = predict(d['w'], d['b'], X_test)\n",
    "y_pre"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.选做题"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations on building your first logistic regression model. It is your time to analyze it further."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.1 Observe the effect of learning rate on the leraning process.   \n",
    "Hits: plot the learning curve with different learning rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "学习率越大，学习进度越快。即同样的训练轮次，学习率大的loss值小。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0005\n",
      "0.001\n",
      "0.0015\n",
      "0.002\n",
      "0.0025\n",
      "0.003\n",
      "0.0035\n",
      "0.004\n",
      "0.0045000000000000005\n",
      "0.005000000000000001\n",
      "0.0055\n",
      "0.006\n",
      "0.006500000000000001\n",
      "0.007000000000000001\n",
      "0.0075\n",
      "0.008\n",
      "0.0085\n",
      "0.009000000000000001\n",
      "0.009500000000000001\n",
      "0.01\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "lrs = np.linspace(5e-4, 1e-2, 20, endpoint=True)\n",
    "costs = []\n",
    "for i in lrs:\n",
    "    print(i)\n",
    "    d = model(X_train, y_train, X_test, y_test, 500, i, False)\n",
    "    costs.append(d['cost'])\n",
    "plt.plot(lrs, costs)\n",
    "plt.xlabel('Learning Rate')\n",
    "plt.ylabel('loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.2 Observe the effect of iteration_num on the test accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "训练轮次越多，正确率越高。\n",
    "曲线图中产生的抖动猜测是w参数随机初始化的影响。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "400\n",
      "600\n",
      "800\n",
      "1000\n",
      "1200\n",
      "1400\n",
      "1600\n",
      "1800\n",
      "2000\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "iters = np.linspace(200, 2000, 10, endpoint=True).astype(np.int)\n",
    "accs = []\n",
    "for i in iters:\n",
    "    print(i)\n",
    "    d = model(X_train, y_train, X_test, y_test, i, 1e-3, False)\n",
    "    accs.append(d['test_accuracy'])\n",
    "plt.plot(iters, accs)\n",
    "plt.xlabel('Iterration Num')\n",
    "plt.ylabel('Test Accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge ! ! !\n",
    "\n",
    "The original data have images labeled 0,1,2,3,4,5,6,7,8,9. In our logistic model, we only detect if the digit in the image is larger or smaller than 5. Now, Let's go for a more challenging problem. Try to use softmax function to build a model to recognize which digit (0,1,2,3,4,5,6,7,8,9) is in the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters_2(dim1, dim2):\n",
    "\n",
    "    w = np.random.random((dim1,dim2))\n",
    "    b = np.zeros((dim2, 1))\n",
    "\n",
    "    return w,b\n",
    "\n",
    "def softmax(z):\n",
    "    temp = np.exp(z)\n",
    "    return temp/np.sum(temp, axis=0)\n",
    "\n",
    "# 重写反向传播函数\n",
    "def propagate_2(w,b,X,Y):\n",
    "    '''\n",
    "    Implement the cost function and its gradient for the propagation\n",
    "    \n",
    "    Arguments:\n",
    "    w - weights\n",
    "    b - bias\n",
    "    X - data\n",
    "    Y - ground truth\n",
    "    '''\n",
    "    m = X.shape[1]\n",
    "    A = softmax(np.dot(w.T, X) + b)\n",
    "    temp = Y * np.log(A) + (1 - Y) * np.log(1 - A)\n",
    "\n",
    "    # 此处需要将二分类的损失函数改成多分类的损失函数\n",
    "#     cost = -1 / m * np.sum(Y * np.log(A) + (1 - Y) * np.log(1 - A))\n",
    "    cost = -1 / m * np.sum(Y * np.log(A))\n",
    "    \n",
    "    dw = 1 / m * np.dot(X, (A - Y).T)\n",
    "    db = 1 / m * np.sum(A - Y, axis=(0,1))\n",
    "    \n",
    "    assert(dw.shape == w.shape)\n",
    "    assert(db.dtype == float)\n",
    "    cost = np.squeeze(cost)\n",
    "    assert(cost.shape == ())\n",
    "    \n",
    "    grads = {'dw':dw,\n",
    "             'db':db}\n",
    "    return grads, cost\n",
    "\n",
    "def optimize_2(w, b, X, Y, num_iterations, learning_rate, print_cost=False):\n",
    "    '''\n",
    "    This function optimize w and b by running a gradient descen algorithm\n",
    "    \n",
    "    Arguments:\n",
    "    w - weights\n",
    "    b - bias\n",
    "    X - data\n",
    "    Y - ground truth\n",
    "    num_iterations -- number of iterations of the optimization loop\n",
    "    learning_rate -- learning rate of the gradient descent update rule\n",
    "    print_cost -- True to print the loss every 100 steps\n",
    "    \n",
    "    Returns:\n",
    "    params - dictionary containing the weights w and bias b\n",
    "    grads -- dictionary containing the gradients of the weights and bias with respect to the cost function\n",
    "    costs -- list of all the costs computed during the optimization, this will be used to plot the learning curve.\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    batch_size = 50\n",
    "    for j in range(num_iterations):\n",
    "        costs = []\n",
    "        for i in range(0, X.shape[1], batch_size):\n",
    "            X_ = X[:,i:i+batch_size]\n",
    "            Y_ = Y[:,i:i+batch_size]\n",
    "\n",
    "            grads, cost = propagate_2(w,b,X_,Y_)\n",
    "\n",
    "            dw = grads['dw']\n",
    "            db = grads['db']\n",
    "\n",
    "            w -= learning_rate * dw\n",
    "            b -= learning_rate * db\n",
    "\n",
    "            costs.append(cost)\n",
    "        \n",
    "        cost = sum(costs)/len(costs)\n",
    "        if print_cost and j % 100 == 0:\n",
    "            print (\"Cost after iteration %i: %f\" %(j, cost))\n",
    "    \n",
    "    params = {\"w\":w,\n",
    "              \"b\":b}\n",
    "    \n",
    "    grads = {\"dw\":dw,\n",
    "             \"db\":db}\n",
    "     \n",
    "    return params, grads, cost\n",
    "\n",
    "def predict_2(w, b, X):\n",
    "    '''\n",
    "    Predict whether the label is 0 or 1 using learned logistic regression parameters (w, b)\n",
    "    \n",
    "    Arguments:\n",
    "    w -- weights\n",
    "    b -- bias \n",
    "    X -- data \n",
    "    \n",
    "    Returns:\n",
    "    Y_prediction -- a numpy array (vector) containing all predictions (0/1) for the examples in X\n",
    "    '''\n",
    "    m = X.shape[1]\n",
    "    Y_prediction = np.zeros((1,m))\n",
    "#     w = w.reshape(X.shape[0],1)\n",
    "    \n",
    "    A = softmax(np.dot(w.T, X) + b)\n",
    "    \n",
    "    Y_prediction[0] = A.argmax(axis=0)\n",
    "    \n",
    "    assert(Y_prediction.shape == (1,m))\n",
    "    \n",
    "    return Y_prediction\n",
    "\n",
    "def model_2(X_train, Y_train, X_test, Y_test, num_iterations, learning_rate, print_cost):\n",
    "    \"\"\"\n",
    "    Build the logistic regression model by calling all the functions you have implemented.\n",
    "    Arguments:\n",
    "    X_train - training set\n",
    "    Y_train - training label\n",
    "    X_test - test set\n",
    "    Y_test - test label\n",
    "    num_iteration - hyperparameter representing the number of iterations to optimize the parameters\n",
    "    learning_rate -- hyperparameter representing the learning rate used in the update rule of optimize()\n",
    "    print_cost -- Set to true to print the cost every 100 iterations\n",
    "    \n",
    "    Returns:\n",
    "    d - dictionary should contain following information w,b,training_accuracy, test_accuracy,cost\n",
    "    eg: d = {\"w\":w,\n",
    "             \"b\":b,\n",
    "             \"training_accuracy\": traing_accuracy,\n",
    "             \"test_accuracy\":test_accuracy,\n",
    "             \"cost\":cost}\n",
    "    \"\"\"\n",
    "    \n",
    "    # 参数初始化\n",
    "    w, b = initialize_parameters_2(X_train.shape[0], Y_train.shape[0])\n",
    "    \n",
    "    # 训练模型\n",
    "    params, grads, costs = optimize_2(w, b, X_train, Y_train, num_iterations, learning_rate, print_cost)\n",
    "    w = params['w']\n",
    "    b = params['b']\n",
    "            \n",
    "    # 训练集精度\n",
    "    Y_train_ = predict_2(w, b, X_train)\n",
    "    traing_accuracy = np.mean(Y_train_ == np.expand_dims(Y_train.argmax(axis=0), axis=0))\n",
    "           \n",
    "    # 测试集\n",
    "    Y_pre = predict_2(w, b, X_test)\n",
    "    test_accuracy = np.mean(Y_pre == np.expand_dims(Y_test.argmax(axis=0), axis=0))\n",
    "    \n",
    "    d = {\"w\":w,\n",
    "         \"b\":b,\n",
    "         \"training_accuracy\": traing_accuracy,\n",
    "         \"test_accuracy\":test_accuracy,\n",
    "         \"cost\":costs}\n",
    "    \n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获取数据\n",
    "X_train, X_test, y_train, y_test = train_test_split(digits.data, digits.target, test_size=0.25)\n",
    "\n",
    "# 数据预处理\n",
    "# X标准化处理\n",
    "X_train /= X_train.max()\n",
    "X_test /= X_train.max()\n",
    "# XY参数格式调整\n",
    "X_train = X_train.T\n",
    "y_train = np.eye(10)[y_train].T\n",
    "X_test = X_test.T\n",
    "y_test = np.eye(10)[y_test].T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: 2.695583\n",
      "Cost after iteration 100: 0.610572\n",
      "Cost after iteration 200: 0.386751\n",
      "Cost after iteration 300: 0.301874\n",
      "Cost after iteration 400: 0.255266\n",
      "Cost after iteration 500: 0.225094\n",
      "Cost after iteration 600: 0.203648\n",
      "Cost after iteration 700: 0.187448\n",
      "Cost after iteration 800: 0.174671\n",
      "Cost after iteration 900: 0.164267\n",
      "Cost after iteration 1000: 0.155581\n",
      "Cost after iteration 1100: 0.148184\n",
      "Cost after iteration 1200: 0.141784\n",
      "Cost after iteration 1300: 0.136171\n",
      "Cost after iteration 1400: 0.131193\n",
      "Cost after iteration 1500: 0.126735\n",
      "Cost after iteration 1600: 0.122711\n",
      "Cost after iteration 1700: 0.119053\n",
      "Cost after iteration 1800: 0.115705\n",
      "Cost after iteration 1900: 0.112625\n",
      "Cost after iteration 2000: 0.109777\n",
      "Cost after iteration 2100: 0.107133\n",
      "Cost after iteration 2200: 0.104667\n",
      "Cost after iteration 2300: 0.102360\n",
      "Cost after iteration 2400: 0.100195\n",
      "Cost after iteration 2500: 0.098156\n",
      "Cost after iteration 2600: 0.096231\n",
      "Cost after iteration 2700: 0.094409\n",
      "Cost after iteration 2800: 0.092681\n",
      "Cost after iteration 2900: 0.091038\n",
      "Cost after iteration 3000: 0.089474\n",
      "Cost after iteration 3100: 0.087981\n",
      "Cost after iteration 3200: 0.086555\n",
      "Cost after iteration 3300: 0.085189\n",
      "Cost after iteration 3400: 0.083880\n",
      "Cost after iteration 3500: 0.082623\n",
      "Cost after iteration 3600: 0.081416\n",
      "Cost after iteration 3700: 0.080253\n",
      "Cost after iteration 3800: 0.079133\n",
      "Cost after iteration 3900: 0.078054\n",
      "Cost after iteration 4000: 0.077011\n",
      "Cost after iteration 4100: 0.076004\n",
      "Cost after iteration 4200: 0.075029\n",
      "Cost after iteration 4300: 0.074086\n",
      "Cost after iteration 4400: 0.073173\n",
      "Cost after iteration 4500: 0.072287\n",
      "Cost after iteration 4600: 0.071428\n",
      "Cost after iteration 4700: 0.070594\n",
      "Cost after iteration 4800: 0.069784\n",
      "Cost after iteration 4900: 0.068997\n",
      "{'w': array([[ 6.88152470e-01,  8.46262600e-01,  4.99773945e-01,\n",
      "         6.69600368e-01,  7.26525969e-01,  8.96771936e-01,\n",
      "         1.96889907e-01,  5.01625332e-01,  3.60349612e-01,\n",
      "         3.41583387e-01],\n",
      "       [ 8.47516027e-01, -8.92201911e-02,  7.95606481e-01,\n",
      "         5.86989750e-01,  8.17928518e-01,  5.31107133e-01,\n",
      "         3.53228205e-01,  8.11031826e-01,  6.89595132e-01,\n",
      "         5.15152192e-01],\n",
      "       [ 1.80169764e-01,  2.17412906e-01,  1.46386501e+00,\n",
      "         8.36999364e-01, -1.02880105e-01,  2.06966532e+00,\n",
      "        -2.85249512e-01,  7.75369745e-01,  2.17912495e-01,\n",
      "         8.49461239e-01],\n",
      "       [ 1.02727176e+00,  4.91179225e-01,  1.16842399e+00,\n",
      "         1.00660904e+00, -8.43979020e-01,  1.04311162e+00,\n",
      "         1.27072405e-01,  1.11075773e+00, -7.99003405e-03,\n",
      "         8.07513368e-01],\n",
      "       [ 5.74534553e-01, -2.19725518e+00,  4.64797439e-01,\n",
      "         2.42598649e+00, -5.79494825e-01,  1.29302868e+00,\n",
      "        -1.44904314e-02,  1.35706283e+00,  6.45270492e-01,\n",
      "         2.41203135e-01],\n",
      "       [-2.94268687e-01,  1.05526386e+00, -3.50856722e-01,\n",
      "         1.57148295e+00, -1.46403831e+00,  2.86660723e+00,\n",
      "        -5.20372480e-01,  1.32581136e+00,  1.74293463e-01,\n",
      "        -2.22425679e-01],\n",
      "       [-2.00390051e-01,  7.51686000e-01,  1.78223906e-01,\n",
      "         8.88220776e-01, -4.70764403e-01,  2.12487567e+00,\n",
      "        -3.26978499e-02,  1.95712897e+00, -8.17988014e-01,\n",
      "         9.29379138e-01],\n",
      "       [ 3.38846783e-01,  1.49508540e-01,  5.89091221e-01,\n",
      "         3.96515717e-01,  6.80112691e-01,  3.30967645e-01,\n",
      "         7.12364679e-01,  1.24834193e+00,  7.82629331e-01,\n",
      "         7.12720206e-01],\n",
      "       [ 4.40482221e-01,  5.87007622e-02,  5.39885014e-01,\n",
      "         7.30765745e-01,  4.81105036e-01,  2.53935717e-01,\n",
      "         9.11869517e-01,  4.36115827e-01,  8.26729405e-01,\n",
      "         1.08845915e-01],\n",
      "       [-1.69968717e-01, -7.78002952e-01,  1.68661605e+00,\n",
      "         9.85805392e-01,  4.69175131e-01,  1.14594224e+00,\n",
      "        -3.71874769e-01,  6.35861586e-01,  4.17247746e-01,\n",
      "         6.56136722e-01],\n",
      "       [ 9.38426850e-01, -1.82351809e+00,  1.36755284e+00,\n",
      "         1.31773424e+00, -1.73544321e-01,  1.52295495e+00,\n",
      "        -5.07323790e-01,  9.87479946e-01,  1.46348217e+00,\n",
      "         1.66957554e+00],\n",
      "       [ 1.49409593e+00, -4.08769876e-01,  5.46453269e-01,\n",
      "         4.14985950e-01, -8.78836942e-02,  2.59056071e-01,\n",
      "         3.78430003e-01,  1.50045957e+00,  1.60034162e-01,\n",
      "         4.10034729e-01],\n",
      "       [ 8.39773940e-01,  1.29607442e+00,  1.80297915e+00,\n",
      "         8.81334643e-01, -1.06292182e+00,  2.45348990e-01,\n",
      "        -9.94202123e-01,  2.36588870e+00, -6.38386348e-01,\n",
      "         2.19488120e-02],\n",
      "       [ 9.73826536e-01,  7.63227803e-01,  5.55053940e-01,\n",
      "         1.92481923e+00, -9.56509531e-01, -7.55086747e-01,\n",
      "        -5.44331056e-01,  5.48239371e-01,  1.52317756e+00,\n",
      "         1.25128433e+00],\n",
      "       [-1.66400612e-01, -1.73258967e-01,  5.59687627e-01,\n",
      "         1.45370840e+00, -3.98676603e-02,  2.53680416e-01,\n",
      "        -3.57943442e-02,  1.00843697e+00,  7.69180965e-01,\n",
      "         1.62627204e+00],\n",
      "       [ 6.46877287e-01,  2.36877302e-01,  7.91541686e-01,\n",
      "         6.05114097e-02,  7.72932756e-01, -1.05368199e-01,\n",
      "         1.26432922e-01,  1.33337287e+00,  9.11239108e-01,\n",
      "         5.54502877e-01],\n",
      "       [ 9.36624119e-01,  7.83964069e-01,  1.08166995e-02,\n",
      "         2.30540283e-01,  2.17654957e-01,  5.25592225e-01,\n",
      "         4.59747613e-01,  3.30036805e-01,  5.43262442e-02,\n",
      "         9.39783764e-01],\n",
      "       [ 5.29618960e-01,  6.20425437e-01,  3.82583517e-01,\n",
      "         5.45275259e-01,  1.32768565e-01,  4.14316953e-01,\n",
      "        -3.42142303e-01, -3.32486404e-01,  9.42239993e-01,\n",
      "         8.95127238e-01],\n",
      "       [ 1.06142353e+00,  1.13514394e-01, -5.35972413e-01,\n",
      "        -2.49877365e+00,  8.43704013e-01,  1.83743865e+00,\n",
      "         1.61675130e+00, -1.53721562e+00,  1.08055423e+00,\n",
      "         1.40918363e+00],\n",
      "       [ 2.93563135e-01,  4.15431923e+00, -7.20093359e-01,\n",
      "        -1.41254081e+00,  2.01810662e+00,  3.33976424e-01,\n",
      "         8.22036251e-01, -7.77630415e-01,  6.97033047e-01,\n",
      "         4.36365515e-01],\n",
      "       [-9.29147176e-01,  3.06578402e+00,  1.16463265e+00,\n",
      "         1.43282731e+00,  1.28950951e+00, -1.88381225e+00,\n",
      "        -1.11925515e+00,  1.10733300e+00,  3.16379292e-01,\n",
      "         1.13371865e+00],\n",
      "       [ 1.95137622e+00,  1.19463926e-01,  6.36507932e-01,\n",
      "        -3.10021549e-01,  8.45826080e-01, -2.71405462e+00,\n",
      "        -1.51916578e+00,  1.10762909e+00,  1.99829357e+00,\n",
      "         3.52361533e+00],\n",
      "       [ 9.56324107e-01, -1.32177540e-01,  1.05443834e+00,\n",
      "         5.71922921e-01,  6.54937274e-01, -9.82099187e-01,\n",
      "        -2.64195119e-01,  9.65089004e-01,  7.80347921e-01,\n",
      "         9.41816613e-01],\n",
      "       [ 2.38786014e-01,  7.77391206e-01,  3.49562510e-01,\n",
      "         3.80436619e-01,  8.95385171e-01,  4.20071194e-01,\n",
      "         1.75676136e-01,  1.72787064e-01,  6.86949708e-01,\n",
      "         3.47515678e-01],\n",
      "       [ 5.65834648e-01,  4.55067163e-01,  3.20416533e-01,\n",
      "         1.88828099e-01,  4.46156191e-01,  8.14091989e-02,\n",
      "         2.94463903e-01,  5.27323471e-02,  7.38949043e-01,\n",
      "         5.83999615e-01],\n",
      "       [ 1.20719054e+00,  9.42348364e-01, -6.25347978e-02,\n",
      "         3.68050018e-01,  1.25372313e+00,  1.44270954e+00,\n",
      "         9.71618995e-01, -5.25369078e-01, -2.76108582e-01,\n",
      "         7.10443266e-01],\n",
      "       [ 1.40162278e+00,  9.91677863e-01, -2.43877691e+00,\n",
      "        -2.00109229e+00,  2.56101680e+00,  2.39854505e+00,\n",
      "         1.03230638e+00, -4.85330720e-02,  6.63527381e-03,\n",
      "         1.22465650e+00],\n",
      "       [-1.21975177e+00,  1.43686775e+00, -2.22494956e+00,\n",
      "         4.14154762e-01,  2.41751495e-01,  1.15824365e+00,\n",
      "         4.85722477e-01, -8.31153446e-01,  3.00678827e+00,\n",
      "         2.97692907e+00],\n",
      "       [-2.30776039e+00,  8.29041693e-01, -2.33507544e-03,\n",
      "         1.50232662e+00,  7.77837298e-01,  8.96671147e-01,\n",
      "        -2.25961077e-01,  2.02079961e-01,  6.97733957e-02,\n",
      "         2.16495625e+00],\n",
      "       [ 5.21933764e-01,  5.44365901e-01,  1.79691683e-01,\n",
      "        -1.82538445e+00,  8.38347706e-01,  1.87698001e-01,\n",
      "         1.49943320e-01,  1.95859071e+00,  1.51827348e-02,\n",
      "         2.33575459e+00],\n",
      "       [ 1.43674633e+00, -2.13703485e-02,  3.05380887e-02,\n",
      "        -9.29510035e-01,  2.63433537e+00, -9.05697744e-01,\n",
      "        -3.97825539e-01,  1.64941293e+00, -1.62698743e-01,\n",
      "         7.39871598e-01],\n",
      "       [ 5.48031758e-01,  1.23935569e-01,  4.42538208e-01,\n",
      "         9.23443021e-01,  2.35247500e-01,  1.69016683e-01,\n",
      "         7.23756310e-01,  6.57252125e-01,  9.24739016e-01,\n",
      "         7.34893461e-01],\n",
      "       [ 5.70016566e-01,  6.60378252e-01,  3.69665582e-01,\n",
      "         6.42307416e-01,  6.31618452e-01,  6.64997931e-01,\n",
      "         3.64422212e-01,  5.18903673e-01,  9.46439925e-01,\n",
      "         9.01351651e-01],\n",
      "       [ 2.01979778e+00, -1.77479784e-01,  4.90068452e-01,\n",
      "        -3.25157506e-01,  1.61333050e+00,  7.19873572e-01,\n",
      "         8.29280822e-01,  3.94672915e-01, -4.01123417e-01,\n",
      "        -4.24226816e-01],\n",
      "       [ 1.28203134e+00,  1.37366940e+00, -1.22894000e+00,\n",
      "        -8.22347714e-01,  2.12853415e+00,  5.71960555e-01,\n",
      "         1.81613835e+00,  8.61991140e-01, -6.32889727e-01,\n",
      "        -2.74531858e-01],\n",
      "       [-7.77850709e-01,  4.15051433e-01, -4.40887886e-01,\n",
      "         2.26515685e-01,  2.93934409e-01,  1.26375800e-01,\n",
      "         1.67251637e+00,  3.09205788e-01,  2.40809045e+00,\n",
      "         1.24016711e+00],\n",
      "       [-2.33894238e+00,  1.09701573e+00,  3.29258959e-01,\n",
      "         1.90876036e+00,  1.82854989e+00, -1.03138082e-01,\n",
      "         7.18661126e-01,  1.41555687e+00,  1.49832040e+00,\n",
      "        -1.11200495e+00],\n",
      "       [ 1.27534621e-01,  3.27422938e-01, -9.70577718e-01,\n",
      "         3.27968511e-01,  1.90884113e+00,  1.24287680e+00,\n",
      "         6.41258866e-01,  2.49937412e+00, -8.18269562e-01,\n",
      "         1.02042478e-01],\n",
      "       [ 1.80681772e+00, -4.07147977e-01, -2.25842825e-01,\n",
      "         6.24959098e-02,  1.38970421e+00,  9.35380912e-01,\n",
      "         1.13336550e+00,  1.92356071e+00, -8.56298524e-01,\n",
      "         8.71979703e-01],\n",
      "       [ 8.51639363e-01,  2.28769098e-01,  6.82126593e-01,\n",
      "         7.68040388e-01,  3.03096326e-01,  5.61383122e-01,\n",
      "         6.69263988e-01,  2.42146558e-01,  5.86750770e-01,\n",
      "         8.85723732e-01],\n",
      "       [ 2.74629750e-01,  2.63627281e-01,  8.72172402e-01,\n",
      "         4.98567030e-01,  2.81800607e-01,  6.87304429e-02,\n",
      "         6.00414333e-01,  5.86316743e-01,  3.80544572e-01,\n",
      "         6.54489889e-01],\n",
      "       [ 5.74593508e-01, -4.72838899e-01,  1.22299245e+00,\n",
      "         2.85256084e-01,  2.09711669e+00,  2.82526323e-01,\n",
      "         2.77976643e-01,  6.12556873e-01,  3.46648022e-01,\n",
      "         4.41826762e-01],\n",
      "       [ 1.83158428e+00, -6.30111362e-01,  1.06949545e+00,\n",
      "        -8.42056028e-01,  7.97693092e-01, -9.36267204e-01,\n",
      "         3.13510281e+00,  1.63948377e-01,  2.14252449e+00,\n",
      "        -2.13325397e+00],\n",
      "       [-2.21191548e-01,  1.59784931e+00,  2.75770547e+00,\n",
      "        -1.55339194e+00,  2.42427972e+00, -1.17858253e+00,\n",
      "         1.29140567e+00,  1.51349939e+00,  1.19856198e+00,\n",
      "        -2.58647638e+00],\n",
      "       [-9.50216908e-01,  1.29625873e+00, -3.75534972e-01,\n",
      "         6.96641801e-01,  3.20348056e+00, -3.58005663e-01,\n",
      "         7.77347914e-01,  1.46723181e+00,  8.47332266e-01,\n",
      "        -1.44072201e+00],\n",
      "       [ 1.15516675e+00, -4.87376687e-01, -2.25575481e+00,\n",
      "         2.00673807e+00,  1.21852086e+00,  3.61566330e-01,\n",
      "         6.45272035e-01,  3.04565043e-01,  1.48593236e+00,\n",
      "        -2.95501002e-01],\n",
      "       [ 6.30124171e-01, -1.21243065e+00, -7.69236246e-01,\n",
      "         2.49590156e+00,  7.87633149e-01,  9.86996180e-01,\n",
      "         2.15411420e+00,  7.94494698e-03, -1.68612736e-03,\n",
      "         8.21210330e-01],\n",
      "       [ 1.09945344e-01,  2.54225498e-01,  9.03087612e-01,\n",
      "         7.10102384e-02,  6.19774021e-01,  8.16789452e-01,\n",
      "         4.26823246e-01,  9.28373416e-01,  5.87875024e-01,\n",
      "         4.66023833e-01],\n",
      "       [ 3.07431334e-01,  5.19792339e-01,  1.02918868e-02,\n",
      "         6.75530852e-01,  5.95498100e-01,  3.69367246e-01,\n",
      "         5.83973459e-01,  1.11085455e-01,  7.37320007e-01,\n",
      "         1.32225837e-01],\n",
      "       [ 1.21609245e-01, -1.07420558e-01,  1.25734569e+00,\n",
      "         5.69303283e-01,  1.23989113e+00,  3.03694526e-01,\n",
      "        -2.64486588e-01, -5.13310360e-02, -5.42286970e-02,\n",
      "         7.12979395e-01],\n",
      "       [ 1.54402198e+00,  3.20728233e-01,  1.44321907e+00,\n",
      "         2.37556009e-01, -6.92631776e-01,  3.72238580e-01,\n",
      "         1.32451990e+00, -1.93779348e-01,  2.01200229e+00,\n",
      "        -4.55446146e-01],\n",
      "       [ 5.90668664e-01,  1.33422243e+00,  3.03401478e+00,\n",
      "        -6.71408838e-01,  8.74595569e-01, -5.97669659e-02,\n",
      "         1.54510823e+00,  1.36006378e+00, -1.18646712e+00,\n",
      "        -9.67033248e-01],\n",
      "       [ 1.49216558e+00,  1.72494344e+00,  1.98909723e+00,\n",
      "         1.05974382e+00,  1.33681605e+00,  6.40720111e-01,\n",
      "        -1.11167148e-01, -8.93280477e-01, -8.56582679e-01,\n",
      "        -3.07843196e-01],\n",
      "       [ 7.97396188e-01,  3.49503940e-01,  2.02492480e+00,\n",
      "         1.20800639e+00, -9.67753808e-01,  3.96250968e-01,\n",
      "         1.32340691e+00, -1.81337018e+00,  9.57668802e-01,\n",
      "        -7.53330275e-01],\n",
      "       [ 4.72632567e-01, -1.08694496e-01,  1.50104876e+00,\n",
      "         2.16162661e+00, -3.77624703e-01, -2.17473698e-01,\n",
      "         2.10798948e+00, -5.04999205e-01, -1.87060860e-02,\n",
      "         2.12771296e-01],\n",
      "       [ 6.85551434e-01,  8.19752922e-01,  8.61554555e-01,\n",
      "         3.77393776e-01,  1.19388643e-01,  4.33543563e-01,\n",
      "         5.09237354e-01,  7.81905775e-01,  8.10068442e-01,\n",
      "         6.40762616e-01],\n",
      "       [ 3.88409916e-01,  5.49117395e-01,  9.16326022e-01,\n",
      "         3.71149175e-01,  5.38606787e-02,  8.82967498e-01,\n",
      "         3.74117778e-01,  5.52852139e-01,  4.89433646e-01,\n",
      "         3.69543059e-01],\n",
      "       [ 2.11720691e-01,  2.02541883e-01,  1.09691372e+00,\n",
      "         5.49095901e-01,  5.55133812e-01,  9.58371601e-01,\n",
      "         3.01824970e-01,  3.71006233e-02,  4.49314349e-01,\n",
      "         3.43716515e-01],\n",
      "       [ 2.03652437e-01, -5.94262676e-01,  1.36100735e+00,\n",
      "         1.67072825e+00, -5.53600395e-01,  2.66039286e+00,\n",
      "        -2.73034195e-01,  1.06579488e+00, -1.18043430e+00,\n",
      "         6.03116337e-01],\n",
      "       [ 9.72595983e-01,  1.30610900e-01,  2.13279852e-01,\n",
      "         1.33109404e+00, -7.98576790e-01,  1.54616331e+00,\n",
      "        -5.63584842e-01, -1.74974292e-01,  4.92307020e-01,\n",
      "         6.93570400e-01],\n",
      "       [ 5.49748859e-01,  5.72053780e-01,  1.25252962e+00,\n",
      "         9.37749306e-01, -2.82105768e-01,  1.08247673e-01,\n",
      "         7.66973170e-01, -1.91909482e+00,  1.67250398e+00,\n",
      "         1.18914250e+00],\n",
      "       [-4.48478488e-05,  1.57271396e+00,  1.99952427e+00,\n",
      "         7.45958760e-01, -9.67191591e-01, -2.40144639e-01,\n",
      "         1.75113585e+00, -1.07725341e+00,  4.14411347e-01,\n",
      "         5.23686700e-02],\n",
      "       [-1.74616762e-01,  1.07621215e+00,  2.36798223e+00,\n",
      "         2.82887082e-01,  1.27459814e-01, -3.29557079e-01,\n",
      "         5.56203849e-01, -9.50318767e-02, -2.02021605e-02,\n",
      "         1.29943906e-01],\n",
      "       [ 4.63875190e-01,  1.60429898e+00,  1.21474034e+00,\n",
      "        -3.08353620e-01,  7.36698055e-01,  5.18494255e-02,\n",
      "         5.46942665e-01,  5.45703078e-01,  3.80815893e-01,\n",
      "         7.19427518e-01]]), 'b': array([[-1.92981269e-17],\n",
      "       [-1.92981269e-17],\n",
      "       [-1.92981269e-17],\n",
      "       [-1.92981269e-17],\n",
      "       [-1.92981269e-17],\n",
      "       [-1.92981269e-17],\n",
      "       [-1.92981269e-17],\n",
      "       [-1.92981269e-17],\n",
      "       [-1.92981269e-17],\n",
      "       [-1.92981269e-17]]), 'training_accuracy': 0.9881217520415738, 'test_accuracy': 0.9644444444444444, 'cost': 0.0682382107578833}\n"
     ]
    }
   ],
   "source": [
    "d = model_2(X_train, y_train, X_test, y_test, 5000, 1e-2, True)\n",
    "print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 1., 0.],\n",
       "       [1., 0., 0.]])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# one-hot编码 \n",
    "a = np.asarray([0,1,2,1,0])\n",
    "b = np.eye(3)[a]\n",
    "b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Congratulations ! You have completed assigment 4. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
