{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 复习上课内容以及复现课程代码"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在本部分，你需要复习上课内容和课程代码后，自己复现课程代码。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Node:\n",
    "    # 该类为所有其他图节点类的父类\n",
    "    def __init__(self, inputs=[]):\n",
    "        #定义每个节点的输入和输出\n",
    "        self.inputs = inputs\n",
    "        self.outputs = []\n",
    "        \n",
    "        #每个节点都是其输入节点的输出节点\n",
    "        for n in self.inputs:\n",
    "            # 把自身加入到输入节点的输出节点列表中\n",
    "            n.outputs.append(self)\n",
    "\n",
    "        self.value = None\n",
    "        \n",
    "        self.gradients = {}\n",
    "        # 梯度是dict形式，key是input节点，value是输入节点对这个节点的偏导\n",
    "        # keys are the inputs to this node, and their\n",
    "        # values are the partials of this node with \n",
    "        # respect to that input.\n",
    "        # \\partial{node}{input_i}\n",
    "        \n",
    "\n",
    "    def forward(self):\n",
    "        #前向传播函数 继承该类的其他类会覆写该函数\n",
    "        '''\n",
    "        Forward propagation. \n",
    "        Compute the output value vased on 'inbound_nodes' and store the \n",
    "        result in self.value\n",
    "        '''\n",
    "\n",
    "        raise NotImplemented\n",
    "    \n",
    "\n",
    "    def backward(self):\n",
    "        #反向传播函数，继承该类的其他类会覆写该函数\n",
    "\n",
    "        raise NotImplemented\n",
    "        \n",
    "class Input(Node):\n",
    "    # 输入节点，包括神经网络输入节点，权重节点，和偏差节点\n",
    "    def __init__(self):\n",
    "        '''\n",
    "        An Input node has no inbound nodes.\n",
    "        So no need to pass anything to the Node instantiator.\n",
    "        '''\n",
    "        Node.__init__(self)\n",
    "\n",
    "    def forward(self, value=None):\n",
    "        '''\n",
    "        Only input node is the node where the value may be passed\n",
    "        as an argument to forward().\n",
    "        All other node implementations should get the value of the \n",
    "        previous node from self.inbound_nodes\n",
    "        \n",
    "        Example: \n",
    "        val0: self.inbound_nodes[0].value\n",
    "        '''\n",
    "        #定义节点数值\n",
    "        if value is not None:\n",
    "            self.value = value\n",
    "            ## It's is input node, when need to forward, this node initiate self's value.\n",
    "\n",
    "        # Input subclass just holds a value, such as a data feature or a model parameter(weight/bias)\n",
    "        \n",
    "    def backward(self):\n",
    "        #计算节点梯度\n",
    "        self.gradients = {self:0} # initialization \n",
    "        for n in self.outputs:\n",
    "            #以下计算该节点的输出节点对该节点的梯度\n",
    "            grad_cost = n.gradients[self]\n",
    "            self.gradients[self] = grad_cost * 1\n",
    "            \n",
    "        \n",
    "        # input N --> N1, N2\n",
    "        # \\partial L / \\partial N \n",
    "        # ==> \\partial L / \\partial N1 * \\ partial N1 / \\partial N\n",
    "\n",
    "\n",
    "class Add(Node):\n",
    "    def __init__(self, *nodes):\n",
    "        Node.__init__(self, nodes)\n",
    "\n",
    "\n",
    "    def forward(self):\n",
    "        # 把它所有input的value值累加\n",
    "        self.value = sum(map(lambda n: n.value, self.inputs))\n",
    "        ## when execute forward, this node caculate value as defined.\n",
    "\n",
    "class Linear(Node):\n",
    "    #全连接网络层的计算\n",
    "    def __init__(self, nodes, weights, bias):\n",
    "        Node.__init__(self, [nodes, weights, bias])\n",
    "\n",
    "    def forward(self):\n",
    "        #前向传播计算 y = w*x + b\n",
    "        inputs = self.inputs[0].value\n",
    "        weights = self.inputs[1].value\n",
    "        bias = self.inputs[2].value\n",
    "\n",
    "        self.value = np.dot(inputs, weights) + bias\n",
    "        \n",
    "    def backward(self):\n",
    "        #反向传播计算\n",
    "        # initial a partial for each of the inbound_nodes.\n",
    "        self.gradients = {n: np.zeros_like(n.value) for n in self.inputs}\n",
    "\n",
    "        for n in self.outputs:\n",
    "            # Get the partial of the cost w.r.t this node.\n",
    "            grad_cost = n.gradients[self]\n",
    "            #以下分别计算对inputs， weights, bias的梯度\n",
    "            self.gradients[self.inputs[0]] = np.dot(grad_cost, self.inputs[1].value.T)\n",
    "            self.gradients[self.inputs[1]] = np.dot(self.inputs[0].value.T, grad_cost)\n",
    "            self.gradients[self.inputs[2]] = np.sum(grad_cost, axis=0, keepdims=False)\n",
    "\n",
    "        # WX + B / W ==> X\n",
    "        # WX + B / X ==> W\n",
    "\n",
    "class Sigmoid(Node):\n",
    "    #定义sigmoid函数\n",
    "    def __init__(self, node):\n",
    "        Node.__init__(self, [node])\n",
    "\n",
    "\n",
    "    def _sigmoid(self, x):\n",
    "        return 1./(1 + np.exp(-1 * x))\n",
    "\n",
    "    def forward(self):\n",
    "        #前向 即为sigmoid函数计算\n",
    "        self.x = self.inputs[0].value   # [0] input is a list\n",
    "        self.value = self._sigmoid(self.x)\n",
    "\n",
    "    def backward(self):\n",
    "        #反向传播计算梯度\n",
    "        self.partial = self._sigmoid(self.x) * (1 - self._sigmoid(self.x))\n",
    "        \n",
    "        # y = 1 / (1 + e^-x)\n",
    "        # y' = 1 / (1 + e^-x) (1 - 1 / (1 + e^-x))\n",
    "        \n",
    "        self.gradients = {n: np.zeros_like(n.value) for n in self.inputs}\n",
    "\n",
    "        for n in self.outputs:\n",
    "            grad_cost = n.gradients[self]  # Get the partial of the cost with respect to this node.\n",
    "\n",
    "            self.gradients[self.inputs[0]] = grad_cost * self.partial\n",
    "            # use * to keep all the dimension same!.\n",
    "\n",
    "\n",
    "\n",
    "class MSE(Node):\n",
    "    # 定义平均平方误差\n",
    "    def __init__(self, y, a):\n",
    "        Node.__init__(self, [y, a])\n",
    "\n",
    "\n",
    "    def forward(self):\n",
    "        #前向传播计算\n",
    "        y = self.inputs[0].value.reshape(-1, 1)\n",
    "        a = self.inputs[1].value.reshape(-1, 1)\n",
    "        assert(y.shape == a.shape)\n",
    "\n",
    "        self.m = self.inputs[0].value.shape[0]\n",
    "        self.diff = y - a\n",
    "\n",
    "        self.value = np.mean(self.diff**2)\n",
    "\n",
    "\n",
    "    def backward(self):\n",
    "        #反向计算相应梯度\n",
    "        self.gradients[self.inputs[0]] = (2 / self.m) * self.diff\n",
    "        self.gradients[self.inputs[1]] = (-2 / self.m) * self.diff\n",
    "\n",
    "\n",
    "def forward_and_backward(outputnode, graph):\n",
    "    # execute all the forward method of sorted_nodes.\n",
    "\n",
    "    ## In practice, it's common to feed in mutiple data example in each forward pass rather than just 1. Because the examples can be processed in parallel. The number of examples is called batch size.\n",
    "    for n in graph:\n",
    "        n.forward()\n",
    "        ## each node execute forward, get self.value based on the topological sort result.\n",
    "\n",
    "    for n in  graph[::-1]:\n",
    "        n.backward()\n",
    "\n",
    "    #return outputnode.value\n",
    "\n",
    "###   v -->  a -->  C\n",
    "##    b --> C\n",
    "##    b --> v -- a --> C\n",
    "##    v --> v ---> a -- > C\n",
    "\n",
    "def topological_sort(feed_dict):\n",
    "    \"\"\"\n",
    "    Sort generic nodes in topological order using Kahn's Algorithm.\n",
    "    `feed_dict`: A dictionary where the key is a `Input` node and the value is the respective value feed to that node.\n",
    "    Returns a list of sorted nodes.\n",
    "    \"\"\"\n",
    "\n",
    "    input_nodes = [n for n in feed_dict.keys()]\n",
    "\n",
    "    G = {}\n",
    "    nodes = [n for n in input_nodes]\n",
    "    while len(nodes) > 0:\n",
    "        n = nodes.pop(0)\n",
    "        if n not in G:\n",
    "            G[n] = {'in': set(), 'out': set()}\n",
    "        for m in n.outputs:\n",
    "            if m not in G:\n",
    "                G[m] = {'in': set(), 'out': set()}\n",
    "            G[n]['out'].add(m)\n",
    "            G[m]['in'].add(n)\n",
    "            nodes.append(m)\n",
    "\n",
    "    L = []\n",
    "    S = set(input_nodes)\n",
    "    while len(S) > 0:\n",
    "        n = S.pop()\n",
    "\n",
    "        if isinstance(n, Input):\n",
    "            n.value = feed_dict[n]\n",
    "            ## if n is Input Node, set n'value as \n",
    "            ## feed_dict[n]\n",
    "            ## else, n's value is caculate as its\n",
    "            ## inbounds\n",
    "\n",
    "        L.append(n)\n",
    "        for m in n.outputs:\n",
    "            G[n]['out'].remove(m)\n",
    "            G[m]['in'].remove(n)\n",
    "            # if no other incoming edges add to S\n",
    "            if len(G[m]['in']) == 0:\n",
    "                S.add(m)\n",
    "    return L\n",
    "\n",
    "\n",
    "def sgd_update(trainables, learning_rate=1e-2):\n",
    "    # there are so many other update / optimization methods\n",
    "    # such as Adam, Mom, \n",
    "    for t in trainables:\n",
    "        t.value += -1 * learning_rate * t.gradients[t]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "data = load_boston()\n",
    "losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of examples = 506\n",
      "Epoch: 1, Loss: 183.679\n",
      "Epoch: 101, Loss: 7.998\n",
      "Epoch: 201, Loss: 5.202\n",
      "Epoch: 301, Loss: 3.858\n",
      "Epoch: 401, Loss: 3.791\n",
      "Epoch: 501, Loss: 3.808\n",
      "Epoch: 601, Loss: 3.414\n",
      "Epoch: 701, Loss: 3.212\n",
      "Epoch: 801, Loss: 3.340\n",
      "Epoch: 901, Loss: 3.504\n",
      "Epoch: 1001, Loss: 2.619\n",
      "Epoch: 1101, Loss: 3.069\n",
      "Epoch: 1201, Loss: 3.383\n",
      "Epoch: 1301, Loss: 2.894\n",
      "Epoch: 1401, Loss: 2.873\n",
      "Epoch: 1501, Loss: 3.299\n",
      "Epoch: 1601, Loss: 2.903\n",
      "Epoch: 1701, Loss: 2.696\n",
      "Epoch: 1801, Loss: 3.373\n",
      "Epoch: 1901, Loss: 2.970\n",
      "Epoch: 2001, Loss: 3.157\n",
      "Epoch: 2101, Loss: 3.259\n",
      "Epoch: 2201, Loss: 3.522\n",
      "Epoch: 2301, Loss: 2.366\n",
      "Epoch: 2401, Loss: 2.537\n",
      "Epoch: 2501, Loss: 2.595\n",
      "Epoch: 2601, Loss: 3.338\n",
      "Epoch: 2701, Loss: 2.944\n",
      "Epoch: 2801, Loss: 2.721\n",
      "Epoch: 2901, Loss: 2.577\n",
      "Epoch: 3001, Loss: 2.784\n",
      "Epoch: 3101, Loss: 2.878\n",
      "Epoch: 3201, Loss: 3.013\n",
      "Epoch: 3301, Loss: 3.510\n",
      "Epoch: 3401, Loss: 2.728\n",
      "Epoch: 3501, Loss: 3.065\n",
      "Epoch: 3601, Loss: 3.041\n",
      "Epoch: 3701, Loss: 2.962\n",
      "Epoch: 3801, Loss: 3.105\n",
      "Epoch: 3901, Loss: 2.886\n",
      "Epoch: 4001, Loss: 2.691\n",
      "Epoch: 4101, Loss: 3.245\n",
      "Epoch: 4201, Loss: 3.049\n",
      "Epoch: 4301, Loss: 2.845\n",
      "Epoch: 4401, Loss: 2.536\n",
      "Epoch: 4501, Loss: 2.977\n",
      "Epoch: 4601, Loss: 2.763\n",
      "Epoch: 4701, Loss: 2.749\n",
      "Epoch: 4801, Loss: 2.787\n",
      "Epoch: 4901, Loss: 2.848\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Check out the new network architecture and dataset!\n",
    "Notice that the weights and biases are\n",
    "generated randomly.\n",
    "No need to change anything, but feel free to tweak\n",
    "to test your network, play around with the epochs, batch size, etc!\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.utils import shuffle, resample\n",
    "#from miniflow import *\n",
    "\n",
    "# Load data\n",
    "data = load_boston()\n",
    "X_ = data['data']\n",
    "y_ = data['target']\n",
    "\n",
    "# Normalize data\n",
    "X_ = (X_ - np.mean(X_, axis=0)) / np.std(X_, axis=0)\n",
    "\n",
    "n_features = X_.shape[1]\n",
    "n_hidden = 10\n",
    "W1_ = np.random.randn(n_features, n_hidden)\n",
    "b1_ = np.zeros(n_hidden)\n",
    "W2_ = np.random.randn(n_hidden, 1)\n",
    "b2_ = np.zeros(1)\n",
    "\n",
    "# Neural network\n",
    "X, y = Input(), Input()\n",
    "W1, b1 = Input(), Input()\n",
    "W2, b2 = Input(), Input()\n",
    "\n",
    "l1 = Linear(X, W1, b1)\n",
    "s1 = Sigmoid(l1)\n",
    "l2 = Linear(s1, W2, b2)\n",
    "cost = MSE(y, l2)\n",
    "\n",
    "feed_dict = {\n",
    "    X: X_,\n",
    "    y: y_,\n",
    "    W1: W1_,\n",
    "    b1: b1_,\n",
    "    W2: W2_,\n",
    "    b2: b2_\n",
    "}\n",
    "\n",
    "epochs = 5000\n",
    "# Total number of examples\n",
    "m = X_.shape[0]\n",
    "batch_size = 16\n",
    "steps_per_epoch = m // batch_size\n",
    "\n",
    "graph = topological_sort(feed_dict)\n",
    "trainables = [W1, b1, W2, b2]\n",
    "\n",
    "print(\"Total number of examples = {}\".format(m))\n",
    "\n",
    "# Step 4\n",
    "for i in range(epochs):\n",
    "    loss = 0\n",
    "    for j in range(steps_per_epoch):\n",
    "        # Step 1\n",
    "        # Randomly sample a batch of examples\n",
    "        X_batch, y_batch = resample(X_, y_, n_samples=batch_size)\n",
    "\n",
    "        # Reset value of X and y Inputs\n",
    "        X.value = X_batch\n",
    "        y.value = y_batch\n",
    "\n",
    "        # Step 2\n",
    "        _ = None\n",
    "        forward_and_backward(_, graph) # set output node not important.\n",
    "\n",
    "        # Step 3\n",
    "        rate = 1e-2\n",
    "    \n",
    "        sgd_update(trainables, rate)\n",
    "\n",
    "        loss += graph[-1].value\n",
    "    \n",
    "    if i % 100 == 0: \n",
    "        print(\"Epoch: {}, Loss: {:.3f}\".format(i+1, loss/steps_per_epoch))\n",
    "        losses.append(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(outputNode,graph):\n",
    "    for n in graph:\n",
    "        n.forward()\n",
    "    return outputNode.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[23.42450975],\n",
       "       [22.40777964],\n",
       "       [27.25478659],\n",
       "       [19.96765148],\n",
       "       [21.46174182],\n",
       "       [13.34363569],\n",
       "       [50.39736562],\n",
       "       [32.81268678],\n",
       "       [19.68591933],\n",
       "       [20.37913184],\n",
       "       [20.48932325],\n",
       "       [18.86990291],\n",
       "       [19.33166824],\n",
       "       [20.48932325],\n",
       "       [36.57321351],\n",
       "       [19.94384745]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forward(l2,graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x23cbc9bbef0>]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(range(len(losses)), losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 6.58202748],\n",
       "       [10.72925089],\n",
       "       [ 5.16823646],\n",
       "       [ 7.81252185],\n",
       "       [13.61672136],\n",
       "       [ 5.01858109],\n",
       "       [ 8.49381454],\n",
       "       [12.17890184],\n",
       "       [-4.65025291],\n",
       "       [10.73876586]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W2.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_ = data['data']\n",
    "# Normalize data\n",
    "X_ = (X_ - np.mean(X_, axis=0)) / np.std(X_, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\myx\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(units=64, activation='sigmoid', input_dim=13))\n",
    "model.add(Dense(units=30, activation='sigmoid', input_dim=64))\n",
    "model.add(Dense(units=1))\n",
    "\n",
    "model.compile(loss='mse',\n",
    "              optimizer='sgd',\n",
    "              metrics=['mse'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "506/506 [==============================] - 0s 552us/step - loss: 142.0431 - mean_squared_error: 142.0431\n",
      "Epoch 2/500\n",
      "506/506 [==============================] - 0s 93us/step - loss: 59.4669 - mean_squared_error: 59.4669\n",
      "Epoch 3/500\n",
      "506/506 [==============================] - 0s 93us/step - loss: 46.6998 - mean_squared_error: 46.6998\n",
      "Epoch 4/500\n",
      "506/506 [==============================] - 0s 91us/step - loss: 38.3405 - mean_squared_error: 38.3405\n",
      "Epoch 5/500\n",
      "506/506 [==============================] - 0s 85us/step - loss: 31.3586 - mean_squared_error: 31.3586\n",
      "Epoch 6/500\n",
      "506/506 [==============================] - 0s 89us/step - loss: 26.7903 - mean_squared_error: 26.7903\n",
      "Epoch 7/500\n",
      "506/506 [==============================] - 0s 87us/step - loss: 24.9205 - mean_squared_error: 24.9205\n",
      "Epoch 8/500\n",
      "506/506 [==============================] - 0s 84us/step - loss: 23.5046 - mean_squared_error: 23.5046\n",
      "Epoch 9/500\n",
      "506/506 [==============================] - 0s 87us/step - loss: 21.6917 - mean_squared_error: 21.6917\n",
      "Epoch 10/500\n",
      "506/506 [==============================] - 0s 87us/step - loss: 21.1115 - mean_squared_error: 21.1115\n",
      "Epoch 11/500\n",
      "506/506 [==============================] - 0s 79us/step - loss: 20.4972 - mean_squared_error: 20.4972\n",
      "Epoch 12/500\n",
      "506/506 [==============================] - 0s 76us/step - loss: 19.9324 - mean_squared_error: 19.9324\n",
      "Epoch 13/500\n",
      "506/506 [==============================] - 0s 85us/step - loss: 20.6248 - mean_squared_error: 20.6248\n",
      "Epoch 14/500\n",
      "506/506 [==============================] - 0s 86us/step - loss: 18.1082 - mean_squared_error: 18.1082\n",
      "Epoch 15/500\n",
      "506/506 [==============================] - 0s 87us/step - loss: 18.3209 - mean_squared_error: 18.3209\n",
      "Epoch 16/500\n",
      "506/506 [==============================] - 0s 85us/step - loss: 17.0280 - mean_squared_error: 17.0280\n",
      "Epoch 17/500\n",
      "506/506 [==============================] - 0s 91us/step - loss: 16.7967 - mean_squared_error: 16.7967\n",
      "Epoch 18/500\n",
      "506/506 [==============================] - 0s 97us/step - loss: 16.9181 - mean_squared_error: 16.9181\n",
      "Epoch 19/500\n",
      "506/506 [==============================] - 0s 85us/step - loss: 16.5594 - mean_squared_error: 16.5594\n",
      "Epoch 20/500\n",
      "506/506 [==============================] - 0s 86us/step - loss: 15.6061 - mean_squared_error: 15.6061\n",
      "Epoch 21/500\n",
      "506/506 [==============================] - 0s 89us/step - loss: 15.9451 - mean_squared_error: 15.9451\n",
      "Epoch 22/500\n",
      "506/506 [==============================] - 0s 91us/step - loss: 14.8649 - mean_squared_error: 14.8649\n",
      "Epoch 23/500\n",
      "506/506 [==============================] - 0s 87us/step - loss: 14.9339 - mean_squared_error: 14.9339\n",
      "Epoch 24/500\n",
      "506/506 [==============================] - 0s 83us/step - loss: 14.4541 - mean_squared_error: 14.4541\n",
      "Epoch 25/500\n",
      "506/506 [==============================] - 0s 81us/step - loss: 14.4736 - mean_squared_error: 14.4736\n",
      "Epoch 26/500\n",
      "506/506 [==============================] - 0s 85us/step - loss: 14.4611 - mean_squared_error: 14.4611\n",
      "Epoch 27/500\n",
      "506/506 [==============================] - 0s 83us/step - loss: 13.9500 - mean_squared_error: 13.9500\n",
      "Epoch 28/500\n",
      "506/506 [==============================] - 0s 87us/step - loss: 14.0742 - mean_squared_error: 14.0742\n",
      "Epoch 29/500\n",
      "506/506 [==============================] - 0s 84us/step - loss: 13.8229 - mean_squared_error: 13.8229\n",
      "Epoch 30/500\n",
      "506/506 [==============================] - 0s 85us/step - loss: 13.4642 - mean_squared_error: 13.4642\n",
      "Epoch 31/500\n",
      "506/506 [==============================] - 0s 81us/step - loss: 13.2500 - mean_squared_error: 13.2500\n",
      "Epoch 32/500\n",
      "506/506 [==============================] - 0s 83us/step - loss: 14.0682 - mean_squared_error: 14.0682\n",
      "Epoch 33/500\n",
      "506/506 [==============================] - 0s 83us/step - loss: 14.0354 - mean_squared_error: 14.0354\n",
      "Epoch 34/500\n",
      "506/506 [==============================] - 0s 83us/step - loss: 13.4748 - mean_squared_error: 13.4748\n",
      "Epoch 35/500\n",
      "506/506 [==============================] - 0s 87us/step - loss: 13.0141 - mean_squared_error: 13.0141\n",
      "Epoch 36/500\n",
      "506/506 [==============================] - 0s 97us/step - loss: 12.9143 - mean_squared_error: 12.9143\n",
      "Epoch 37/500\n",
      "506/506 [==============================] - 0s 87us/step - loss: 13.3057 - mean_squared_error: 13.3057\n",
      "Epoch 38/500\n",
      "506/506 [==============================] - 0s 83us/step - loss: 12.9169 - mean_squared_error: 12.9169\n",
      "Epoch 39/500\n",
      "506/506 [==============================] - 0s 100us/step - loss: 13.9674 - mean_squared_error: 13.9674\n",
      "Epoch 40/500\n",
      "506/506 [==============================] - 0s 103us/step - loss: 12.7099 - mean_squared_error: 12.7099\n",
      "Epoch 41/500\n",
      "506/506 [==============================] - 0s 126us/step - loss: 12.5961 - mean_squared_error: 12.5961\n",
      "Epoch 42/500\n",
      "506/506 [==============================] - 0s 107us/step - loss: 12.6913 - mean_squared_error: 12.6913\n",
      "Epoch 43/500\n",
      "506/506 [==============================] - 0s 81us/step - loss: 12.5114 - mean_squared_error: 12.5114\n",
      "Epoch 44/500\n",
      "506/506 [==============================] - 0s 81us/step - loss: 12.3736 - mean_squared_error: 12.3736\n",
      "Epoch 45/500\n",
      "506/506 [==============================] - 0s 85us/step - loss: 12.8138 - mean_squared_error: 12.8138\n",
      "Epoch 46/500\n",
      "506/506 [==============================] - 0s 86us/step - loss: 12.2358 - mean_squared_error: 12.2358\n",
      "Epoch 47/500\n",
      "506/506 [==============================] - 0s 77us/step - loss: 12.2115 - mean_squared_error: 12.2115\n",
      "Epoch 48/500\n",
      "506/506 [==============================] - 0s 89us/step - loss: 12.0977 - mean_squared_error: 12.0977\n",
      "Epoch 49/500\n",
      "506/506 [==============================] - 0s 81us/step - loss: 12.2503 - mean_squared_error: 12.2503\n",
      "Epoch 50/500\n",
      "506/506 [==============================] - 0s 81us/step - loss: 12.4770 - mean_squared_error: 12.4770\n",
      "Epoch 51/500\n",
      "506/506 [==============================] - 0s 83us/step - loss: 11.9909 - mean_squared_error: 11.9909\n",
      "Epoch 52/500\n",
      "506/506 [==============================] - 0s 79us/step - loss: 12.1913 - mean_squared_error: 12.1913\n",
      "Epoch 53/500\n",
      "506/506 [==============================] - 0s 79us/step - loss: 12.0824 - mean_squared_error: 12.0824\n",
      "Epoch 54/500\n",
      "506/506 [==============================] - 0s 87us/step - loss: 11.8299 - mean_squared_error: 11.8299\n",
      "Epoch 55/500\n",
      "506/506 [==============================] - 0s 78us/step - loss: 11.4536 - mean_squared_error: 11.4536\n",
      "Epoch 56/500\n",
      "506/506 [==============================] - 0s 82us/step - loss: 11.6757 - mean_squared_error: 11.6757\n",
      "Epoch 57/500\n",
      "506/506 [==============================] - 0s 80us/step - loss: 11.1798 - mean_squared_error: 11.1798\n",
      "Epoch 58/500\n",
      "506/506 [==============================] - 0s 85us/step - loss: 11.6923 - mean_squared_error: 11.6923\n",
      "Epoch 59/500\n",
      "506/506 [==============================] - 0s 95us/step - loss: 11.1570 - mean_squared_error: 11.1570\n",
      "Epoch 60/500\n",
      "506/506 [==============================] - 0s 97us/step - loss: 11.3681 - mean_squared_error: 11.3681\n",
      "Epoch 61/500\n",
      "506/506 [==============================] - 0s 101us/step - loss: 11.2665 - mean_squared_error: 11.2665\n",
      "Epoch 62/500\n",
      "506/506 [==============================] - 0s 87us/step - loss: 10.9255 - mean_squared_error: 10.9255\n",
      "Epoch 63/500\n",
      "506/506 [==============================] - 0s 87us/step - loss: 11.1852 - mean_squared_error: 11.1852\n",
      "Epoch 64/500\n",
      "506/506 [==============================] - 0s 83us/step - loss: 10.8309 - mean_squared_error: 10.8309\n",
      "Epoch 65/500\n",
      "506/506 [==============================] - 0s 89us/step - loss: 11.2296 - mean_squared_error: 11.2296\n",
      "Epoch 66/500\n",
      "506/506 [==============================] - 0s 108us/step - loss: 11.1125 - mean_squared_error: 11.1125\n",
      "Epoch 67/500\n",
      "506/506 [==============================] - 0s 93us/step - loss: 10.9471 - mean_squared_error: 10.9471\n",
      "Epoch 68/500\n",
      "506/506 [==============================] - 0s 87us/step - loss: 11.0333 - mean_squared_error: 11.0333\n",
      "Epoch 69/500\n",
      "506/506 [==============================] - 0s 78us/step - loss: 10.8337 - mean_squared_error: 10.8337\n",
      "Epoch 70/500\n",
      "506/506 [==============================] - 0s 86us/step - loss: 10.8536 - mean_squared_error: 10.8536\n",
      "Epoch 71/500\n",
      "506/506 [==============================] - 0s 81us/step - loss: 11.1748 - mean_squared_error: 11.1748\n",
      "Epoch 72/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "506/506 [==============================] - 0s 165us/step - loss: 10.6758 - mean_squared_error: 10.6758\n",
      "Epoch 73/500\n",
      "506/506 [==============================] - 0s 158us/step - loss: 10.5160 - mean_squared_error: 10.5160\n",
      "Epoch 74/500\n",
      "506/506 [==============================] - 0s 112us/step - loss: 10.5997 - mean_squared_error: 10.5997\n",
      "Epoch 75/500\n",
      "506/506 [==============================] - 0s 79us/step - loss: 10.7965 - mean_squared_error: 10.7965\n",
      "Epoch 76/500\n",
      "506/506 [==============================] - 0s 83us/step - loss: 10.0754 - mean_squared_error: 10.0754\n",
      "Epoch 77/500\n",
      "506/506 [==============================] - 0s 201us/step - loss: 10.4741 - mean_squared_error: 10.4741\n",
      "Epoch 78/500\n",
      "506/506 [==============================] - 0s 118us/step - loss: 10.6879 - mean_squared_error: 10.6879\n",
      "Epoch 79/500\n",
      "506/506 [==============================] - 0s 154us/step - loss: 10.2030 - mean_squared_error: 10.2030\n",
      "Epoch 80/500\n",
      "506/506 [==============================] - 0s 89us/step - loss: 10.2626 - mean_squared_error: 10.2626\n",
      "Epoch 81/500\n",
      "506/506 [==============================] - 0s 81us/step - loss: 9.9408 - mean_squared_error: 9.9408\n",
      "Epoch 82/500\n",
      "506/506 [==============================] - 0s 81us/step - loss: 10.0259 - mean_squared_error: 10.0259\n",
      "Epoch 83/500\n",
      "506/506 [==============================] - 0s 180us/step - loss: 9.9930 - mean_squared_error: 9.9930\n",
      "Epoch 84/500\n",
      "506/506 [==============================] - 0s 170us/step - loss: 9.8633 - mean_squared_error: 9.8633\n",
      "Epoch 85/500\n",
      "506/506 [==============================] - 0s 96us/step - loss: 10.0063 - mean_squared_error: 10.0063\n",
      "Epoch 86/500\n",
      "506/506 [==============================] - 0s 86us/step - loss: 9.6402 - mean_squared_error: 9.6402\n",
      "Epoch 87/500\n",
      "506/506 [==============================] - 0s 81us/step - loss: 9.8039 - mean_squared_error: 9.8039\n",
      "Epoch 88/500\n",
      "506/506 [==============================] - 0s 77us/step - loss: 9.5612 - mean_squared_error: 9.5612\n",
      "Epoch 89/500\n",
      "506/506 [==============================] - 0s 85us/step - loss: 9.7084 - mean_squared_error: 9.7084\n",
      "Epoch 90/500\n",
      "506/506 [==============================] - 0s 83us/step - loss: 9.2558 - mean_squared_error: 9.2558\n",
      "Epoch 91/500\n",
      "506/506 [==============================] - 0s 82us/step - loss: 9.5760 - mean_squared_error: 9.5760\n",
      "Epoch 92/500\n",
      "506/506 [==============================] - 0s 87us/step - loss: 9.2968 - mean_squared_error: 9.2968\n",
      "Epoch 93/500\n",
      "506/506 [==============================] - 0s 182us/step - loss: 9.5660 - mean_squared_error: 9.5660\n",
      "Epoch 94/500\n",
      "506/506 [==============================] - 0s 148us/step - loss: 9.8676 - mean_squared_error: 9.8676\n",
      "Epoch 95/500\n",
      "506/506 [==============================] - 0s 132us/step - loss: 9.1345 - mean_squared_error: 9.1345\n",
      "Epoch 96/500\n",
      "506/506 [==============================] - 0s 93us/step - loss: 9.1259 - mean_squared_error: 9.1259\n",
      "Epoch 97/500\n",
      "506/506 [==============================] - 0s 95us/step - loss: 8.9831 - mean_squared_error: 8.9831\n",
      "Epoch 98/500\n",
      "506/506 [==============================] - 0s 83us/step - loss: 9.1469 - mean_squared_error: 9.1469\n",
      "Epoch 99/500\n",
      "506/506 [==============================] - 0s 91us/step - loss: 9.0115 - mean_squared_error: 9.0115\n",
      "Epoch 100/500\n",
      "506/506 [==============================] - 0s 101us/step - loss: 9.0767 - mean_squared_error: 9.0767\n",
      "Epoch 101/500\n",
      "506/506 [==============================] - 0s 85us/step - loss: 9.2223 - mean_squared_error: 9.2223\n",
      "Epoch 102/500\n",
      "506/506 [==============================] - 0s 77us/step - loss: 9.0444 - mean_squared_error: 9.0444\n",
      "Epoch 103/500\n",
      "506/506 [==============================] - 0s 81us/step - loss: 9.0051 - mean_squared_error: 9.0051\n",
      "Epoch 104/500\n",
      "506/506 [==============================] - 0s 77us/step - loss: 8.7362 - mean_squared_error: 8.7362\n",
      "Epoch 105/500\n",
      "506/506 [==============================] - 0s 80us/step - loss: 8.6417 - mean_squared_error: 8.6417\n",
      "Epoch 106/500\n",
      "506/506 [==============================] - 0s 87us/step - loss: 8.7629 - mean_squared_error: 8.7629\n",
      "Epoch 107/500\n",
      "506/506 [==============================] - 0s 77us/step - loss: 8.8390 - mean_squared_error: 8.8390\n",
      "Epoch 108/500\n",
      "506/506 [==============================] - 0s 81us/step - loss: 8.6931 - mean_squared_error: 8.6931\n",
      "Epoch 109/500\n",
      "506/506 [==============================] - 0s 75us/step - loss: 8.7319 - mean_squared_error: 8.7319\n",
      "Epoch 110/500\n",
      "506/506 [==============================] - 0s 79us/step - loss: 8.5824 - mean_squared_error: 8.5824\n",
      "Epoch 111/500\n",
      "506/506 [==============================] - 0s 81us/step - loss: 8.5533 - mean_squared_error: 8.5533\n",
      "Epoch 112/500\n",
      "506/506 [==============================] - 0s 79us/step - loss: 8.4599 - mean_squared_error: 8.4599\n",
      "Epoch 113/500\n",
      "506/506 [==============================] - 0s 77us/step - loss: 8.4932 - mean_squared_error: 8.4932\n",
      "Epoch 114/500\n",
      "506/506 [==============================] - 0s 77us/step - loss: 8.4408 - mean_squared_error: 8.4408\n",
      "Epoch 115/500\n",
      "506/506 [==============================] - 0s 79us/step - loss: 8.0885 - mean_squared_error: 8.0885\n",
      "Epoch 116/500\n",
      "506/506 [==============================] - 0s 81us/step - loss: 8.1799 - mean_squared_error: 8.1799\n",
      "Epoch 117/500\n",
      "506/506 [==============================] - 0s 174us/step - loss: 8.3482 - mean_squared_error: 8.3482\n",
      "Epoch 118/500\n",
      "506/506 [==============================] - 0s 168us/step - loss: 7.9152 - mean_squared_error: 7.9152\n",
      "Epoch 119/500\n",
      "506/506 [==============================] - 0s 98us/step - loss: 7.9309 - mean_squared_error: 7.9309\n",
      "Epoch 120/500\n",
      "506/506 [==============================] - 0s 81us/step - loss: 8.2541 - mean_squared_error: 8.2541\n",
      "Epoch 121/500\n",
      "506/506 [==============================] - 0s 77us/step - loss: 8.2259 - mean_squared_error: 8.2259\n",
      "Epoch 122/500\n",
      "506/506 [==============================] - 0s 82us/step - loss: 8.1505 - mean_squared_error: 8.1505\n",
      "Epoch 123/500\n",
      "506/506 [==============================] - 0s 77us/step - loss: 7.8261 - mean_squared_error: 7.8261\n",
      "Epoch 124/500\n",
      "506/506 [==============================] - 0s 81us/step - loss: 7.7611 - mean_squared_error: 7.7611\n",
      "Epoch 125/500\n",
      "506/506 [==============================] - 0s 83us/step - loss: 8.0611 - mean_squared_error: 8.0611\n",
      "Epoch 126/500\n",
      "506/506 [==============================] - 0s 78us/step - loss: 7.6669 - mean_squared_error: 7.6669\n",
      "Epoch 127/500\n",
      "506/506 [==============================] - 0s 76us/step - loss: 7.5382 - mean_squared_error: 7.5382\n",
      "Epoch 128/500\n",
      "506/506 [==============================] - 0s 88us/step - loss: 7.9503 - mean_squared_error: 7.9503\n",
      "Epoch 129/500\n",
      "506/506 [==============================] - 0s 134us/step - loss: 8.1982 - mean_squared_error: 8.1982\n",
      "Epoch 130/500\n",
      "506/506 [==============================] - 0s 89us/step - loss: 7.6824 - mean_squared_error: 7.6824\n",
      "Epoch 131/500\n",
      "506/506 [==============================] - 0s 87us/step - loss: 7.4393 - mean_squared_error: 7.4393\n",
      "Epoch 132/500\n",
      "506/506 [==============================] - 0s 89us/step - loss: 7.3049 - mean_squared_error: 7.3049\n",
      "Epoch 133/500\n",
      "506/506 [==============================] - 0s 180us/step - loss: 7.6362 - mean_squared_error: 7.6362\n",
      "Epoch 134/500\n",
      "506/506 [==============================] - 0s 128us/step - loss: 7.6957 - mean_squared_error: 7.6957\n",
      "Epoch 135/500\n",
      "506/506 [==============================] - 0s 130us/step - loss: 7.2062 - mean_squared_error: 7.2062\n",
      "Epoch 136/500\n",
      "506/506 [==============================] - 0s 133us/step - loss: 7.3735 - mean_squared_error: 7.3735\n",
      "Epoch 137/500\n",
      "506/506 [==============================] - 0s 187us/step - loss: 7.2327 - mean_squared_error: 7.2327\n",
      "Epoch 138/500\n",
      "506/506 [==============================] - 0s 95us/step - loss: 7.2401 - mean_squared_error: 7.2401\n",
      "Epoch 139/500\n",
      "506/506 [==============================] - 0s 83us/step - loss: 7.1356 - mean_squared_error: 7.1356\n",
      "Epoch 140/500\n",
      "506/506 [==============================] - 0s 79us/step - loss: 6.9861 - mean_squared_error: 6.9861\n",
      "Epoch 141/500\n",
      "506/506 [==============================] - 0s 86us/step - loss: 6.8559 - mean_squared_error: 6.8559\n",
      "Epoch 142/500\n",
      "506/506 [==============================] - 0s 81us/step - loss: 7.1046 - mean_squared_error: 7.1046\n",
      "Epoch 143/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "506/506 [==============================] - 0s 82us/step - loss: 6.8299 - mean_squared_error: 6.8299\n",
      "Epoch 144/500\n",
      "506/506 [==============================] - 0s 77us/step - loss: 6.9381 - mean_squared_error: 6.9381\n",
      "Epoch 145/500\n",
      "506/506 [==============================] - 0s 83us/step - loss: 7.0762 - mean_squared_error: 7.0762\n",
      "Epoch 146/500\n",
      "506/506 [==============================] - 0s 83us/step - loss: 6.8471 - mean_squared_error: 6.8471\n",
      "Epoch 147/500\n",
      "506/506 [==============================] - 0s 83us/step - loss: 6.6623 - mean_squared_error: 6.6623\n",
      "Epoch 148/500\n",
      "506/506 [==============================] - 0s 182us/step - loss: 6.8068 - mean_squared_error: 6.8068\n",
      "Epoch 149/500\n",
      "506/506 [==============================] - 0s 132us/step - loss: 6.9022 - mean_squared_error: 6.9022\n",
      "Epoch 150/500\n",
      "506/506 [==============================] - 0s 93us/step - loss: 6.5804 - mean_squared_error: 6.5804\n",
      "Epoch 151/500\n",
      "506/506 [==============================] - 0s 81us/step - loss: 6.7310 - mean_squared_error: 6.7310\n",
      "Epoch 152/500\n",
      "506/506 [==============================] - 0s 93us/step - loss: 6.8516 - mean_squared_error: 6.8516\n",
      "Epoch 153/500\n",
      "506/506 [==============================] - 0s 95us/step - loss: 6.9950 - mean_squared_error: 6.9950\n",
      "Epoch 154/500\n",
      "506/506 [==============================] - 0s 81us/step - loss: 6.3907 - mean_squared_error: 6.3907\n",
      "Epoch 155/500\n",
      "506/506 [==============================] - 0s 85us/step - loss: 6.6062 - mean_squared_error: 6.6062\n",
      "Epoch 156/500\n",
      "506/506 [==============================] - 0s 83us/step - loss: 6.7213 - mean_squared_error: 6.7213\n",
      "Epoch 157/500\n",
      "506/506 [==============================] - 0s 191us/step - loss: 6.3545 - mean_squared_error: 6.3545\n",
      "Epoch 158/500\n",
      "506/506 [==============================] - 0s 106us/step - loss: 6.4898 - mean_squared_error: 6.4898\n",
      "Epoch 159/500\n",
      "506/506 [==============================] - 0s 179us/step - loss: 6.2784 - mean_squared_error: 6.2784\n",
      "Epoch 160/500\n",
      "506/506 [==============================] - 0s 159us/step - loss: 6.5222 - mean_squared_error: 6.5222\n",
      "Epoch 161/500\n",
      "506/506 [==============================] - 0s 136us/step - loss: 6.3524 - mean_squared_error: 6.3524\n",
      "Epoch 162/500\n",
      "506/506 [==============================] - 0s 114us/step - loss: 6.3736 - mean_squared_error: 6.3736\n",
      "Epoch 163/500\n",
      "506/506 [==============================] - 0s 179us/step - loss: 6.3663 - mean_squared_error: 6.3663\n",
      "Epoch 164/500\n",
      "506/506 [==============================] - 0s 101us/step - loss: 6.6671 - mean_squared_error: 6.6671\n",
      "Epoch 165/500\n",
      "506/506 [==============================] - 0s 164us/step - loss: 6.1194 - mean_squared_error: 6.1194\n",
      "Epoch 166/500\n",
      "506/506 [==============================] - 0s 158us/step - loss: 6.0058 - mean_squared_error: 6.0058\n",
      "Epoch 167/500\n",
      "506/506 [==============================] - 0s 89us/step - loss: 5.8999 - mean_squared_error: 5.8999\n",
      "Epoch 168/500\n",
      "506/506 [==============================] - 0s 91us/step - loss: 6.1280 - mean_squared_error: 6.1280\n",
      "Epoch 169/500\n",
      "506/506 [==============================] - 0s 189us/step - loss: 5.9741 - mean_squared_error: 5.9741\n",
      "Epoch 170/500\n",
      "506/506 [==============================] - 0s 139us/step - loss: 5.9744 - mean_squared_error: 5.9744\n",
      "Epoch 171/500\n",
      "506/506 [==============================] - 0s 95us/step - loss: 5.9027 - mean_squared_error: 5.9027\n",
      "Epoch 172/500\n",
      "506/506 [==============================] - 0s 89us/step - loss: 5.8608 - mean_squared_error: 5.8608\n",
      "Epoch 173/500\n",
      "506/506 [==============================] - 0s 89us/step - loss: 6.0155 - mean_squared_error: 6.0155\n",
      "Epoch 174/500\n",
      "506/506 [==============================] - 0s 94us/step - loss: 5.9712 - mean_squared_error: 5.9712\n",
      "Epoch 175/500\n",
      "506/506 [==============================] - 0s 99us/step - loss: 5.9910 - mean_squared_error: 5.9910\n",
      "Epoch 176/500\n",
      "506/506 [==============================] - 0s 83us/step - loss: 6.0348 - mean_squared_error: 6.0348\n",
      "Epoch 177/500\n",
      "506/506 [==============================] - 0s 85us/step - loss: 5.7596 - mean_squared_error: 5.7596\n",
      "Epoch 178/500\n",
      "506/506 [==============================] - 0s 86us/step - loss: 5.7462 - mean_squared_error: 5.7462\n",
      "Epoch 179/500\n",
      "506/506 [==============================] - 0s 99us/step - loss: 5.9082 - mean_squared_error: 5.9082\n",
      "Epoch 180/500\n",
      "506/506 [==============================] - 0s 90us/step - loss: 5.8485 - mean_squared_error: 5.8485\n",
      "Epoch 181/500\n",
      "506/506 [==============================] - 0s 91us/step - loss: 5.8915 - mean_squared_error: 5.8915\n",
      "Epoch 182/500\n",
      "506/506 [==============================] - 0s 90us/step - loss: 5.9013 - mean_squared_error: 5.9013\n",
      "Epoch 183/500\n",
      "506/506 [==============================] - 0s 93us/step - loss: 5.9829 - mean_squared_error: 5.9829\n",
      "Epoch 184/500\n",
      "506/506 [==============================] - 0s 83us/step - loss: 5.8408 - mean_squared_error: 5.8408\n",
      "Epoch 185/500\n",
      "506/506 [==============================] - 0s 87us/step - loss: 5.5086 - mean_squared_error: 5.5086\n",
      "Epoch 186/500\n",
      "506/506 [==============================] - 0s 93us/step - loss: 5.7520 - mean_squared_error: 5.7520\n",
      "Epoch 187/500\n",
      "506/506 [==============================] - 0s 110us/step - loss: 5.4940 - mean_squared_error: 5.4940\n",
      "Epoch 188/500\n",
      "506/506 [==============================] - 0s 95us/step - loss: 5.5497 - mean_squared_error: 5.5497\n",
      "Epoch 189/500\n",
      "506/506 [==============================] - 0s 91us/step - loss: 5.4976 - mean_squared_error: 5.4976\n",
      "Epoch 190/500\n",
      "506/506 [==============================] - 0s 95us/step - loss: 5.5290 - mean_squared_error: 5.5290\n",
      "Epoch 191/500\n",
      "506/506 [==============================] - 0s 91us/step - loss: 5.5123 - mean_squared_error: 5.5123\n",
      "Epoch 192/500\n",
      "506/506 [==============================] - 0s 104us/step - loss: 5.2892 - mean_squared_error: 5.2892\n",
      "Epoch 193/500\n",
      "506/506 [==============================] - 0s 83us/step - loss: 5.5582 - mean_squared_error: 5.5582\n",
      "Epoch 194/500\n",
      "506/506 [==============================] - 0s 91us/step - loss: 5.4662 - mean_squared_error: 5.4662\n",
      "Epoch 195/500\n",
      "506/506 [==============================] - 0s 93us/step - loss: 5.3923 - mean_squared_error: 5.3923\n",
      "Epoch 196/500\n",
      "506/506 [==============================] - 0s 93us/step - loss: 5.5484 - mean_squared_error: 5.5484\n",
      "Epoch 197/500\n",
      "506/506 [==============================] - 0s 93us/step - loss: 5.2377 - mean_squared_error: 5.2377\n",
      "Epoch 198/500\n",
      "506/506 [==============================] - 0s 93us/step - loss: 5.3561 - mean_squared_error: 5.3561\n",
      "Epoch 199/500\n",
      "506/506 [==============================] - 0s 87us/step - loss: 5.4869 - mean_squared_error: 5.4869\n",
      "Epoch 200/500\n",
      "506/506 [==============================] - 0s 85us/step - loss: 5.4425 - mean_squared_error: 5.4425\n",
      "Epoch 201/500\n",
      "506/506 [==============================] - 0s 85us/step - loss: 5.6957 - mean_squared_error: 5.6957\n",
      "Epoch 202/500\n",
      "506/506 [==============================] - 0s 87us/step - loss: 5.2839 - mean_squared_error: 5.2839\n",
      "Epoch 203/500\n",
      "506/506 [==============================] - 0s 83us/step - loss: 5.2041 - mean_squared_error: 5.2041\n",
      "Epoch 204/500\n",
      "506/506 [==============================] - 0s 150us/step - loss: 5.3363 - mean_squared_error: 5.3363\n",
      "Epoch 205/500\n",
      "506/506 [==============================] - 0s 152us/step - loss: 5.3044 - mean_squared_error: 5.3044\n",
      "Epoch 206/500\n",
      "506/506 [==============================] - 0s 88us/step - loss: 5.2276 - mean_squared_error: 5.2276\n",
      "Epoch 207/500\n",
      "506/506 [==============================] - 0s 81us/step - loss: 5.3914 - mean_squared_error: 5.3914\n",
      "Epoch 208/500\n",
      "506/506 [==============================] - 0s 82us/step - loss: 5.2996 - mean_squared_error: 5.2996\n",
      "Epoch 209/500\n",
      "506/506 [==============================] - 0s 89us/step - loss: 5.4527 - mean_squared_error: 5.4527\n",
      "Epoch 210/500\n",
      "506/506 [==============================] - 0s 79us/step - loss: 5.3648 - mean_squared_error: 5.3648\n",
      "Epoch 211/500\n",
      "506/506 [==============================] - 0s 84us/step - loss: 5.1819 - mean_squared_error: 5.1819\n",
      "Epoch 212/500\n",
      "506/506 [==============================] - 0s 81us/step - loss: 5.2091 - mean_squared_error: 5.2091\n",
      "Epoch 213/500\n",
      "506/506 [==============================] - 0s 81us/step - loss: 5.0546 - mean_squared_error: 5.0546\n",
      "Epoch 214/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "506/506 [==============================] - 0s 81us/step - loss: 5.2004 - mean_squared_error: 5.2004\n",
      "Epoch 215/500\n",
      "506/506 [==============================] - 0s 80us/step - loss: 4.8874 - mean_squared_error: 4.8874\n",
      "Epoch 216/500\n",
      "506/506 [==============================] - 0s 81us/step - loss: 4.9756 - mean_squared_error: 4.9756\n",
      "Epoch 217/500\n",
      "506/506 [==============================] - 0s 79us/step - loss: 4.8848 - mean_squared_error: 4.8848\n",
      "Epoch 218/500\n",
      "506/506 [==============================] - 0s 84us/step - loss: 4.8513 - mean_squared_error: 4.8513\n",
      "Epoch 219/500\n",
      "506/506 [==============================] - 0s 83us/step - loss: 4.9859 - mean_squared_error: 4.9859\n",
      "Epoch 220/500\n",
      "506/506 [==============================] - 0s 83us/step - loss: 5.0443 - mean_squared_error: 5.0443\n",
      "Epoch 221/500\n",
      "506/506 [==============================] - 0s 87us/step - loss: 4.8756 - mean_squared_error: 4.8756\n",
      "Epoch 222/500\n",
      "506/506 [==============================] - 0s 89us/step - loss: 4.9924 - mean_squared_error: 4.9924\n",
      "Epoch 223/500\n",
      "506/506 [==============================] - 0s 79us/step - loss: 5.2469 - mean_squared_error: 5.2469\n",
      "Epoch 224/500\n",
      "506/506 [==============================] - 0s 84us/step - loss: 5.0877 - mean_squared_error: 5.0877\n",
      "Epoch 225/500\n",
      "506/506 [==============================] - 0s 77us/step - loss: 5.1467 - mean_squared_error: 5.1467\n",
      "Epoch 226/500\n",
      "506/506 [==============================] - 0s 81us/step - loss: 5.2661 - mean_squared_error: 5.2661\n",
      "Epoch 227/500\n",
      "506/506 [==============================] - 0s 181us/step - loss: 5.0074 - mean_squared_error: 5.0074\n",
      "Epoch 228/500\n",
      "506/506 [==============================] - 0s 126us/step - loss: 4.8811 - mean_squared_error: 4.8811\n",
      "Epoch 229/500\n",
      "506/506 [==============================] - 0s 89us/step - loss: 4.7828 - mean_squared_error: 4.7828\n",
      "Epoch 230/500\n",
      "506/506 [==============================] - 0s 79us/step - loss: 5.1121 - mean_squared_error: 5.1121\n",
      "Epoch 231/500\n",
      "506/506 [==============================] - 0s 81us/step - loss: 4.8275 - mean_squared_error: 4.8275\n",
      "Epoch 232/500\n",
      "506/506 [==============================] - 0s 79us/step - loss: 4.8761 - mean_squared_error: 4.8761\n",
      "Epoch 233/500\n",
      "506/506 [==============================] - 0s 93us/step - loss: 4.7568 - mean_squared_error: 4.7568\n",
      "Epoch 234/500\n",
      "506/506 [==============================] - 0s 93us/step - loss: 4.8206 - mean_squared_error: 4.8206\n",
      "Epoch 235/500\n",
      "506/506 [==============================] - 0s 90us/step - loss: 4.8212 - mean_squared_error: 4.8212\n",
      "Epoch 236/500\n",
      "506/506 [==============================] - 0s 93us/step - loss: 4.8727 - mean_squared_error: 4.8727\n",
      "Epoch 237/500\n",
      "506/506 [==============================] - 0s 95us/step - loss: 4.7368 - mean_squared_error: 4.7368\n",
      "Epoch 238/500\n",
      "506/506 [==============================] - 0s 87us/step - loss: 4.8331 - mean_squared_error: 4.8331\n",
      "Epoch 239/500\n",
      "506/506 [==============================] - 0s 89us/step - loss: 4.6736 - mean_squared_error: 4.6736\n",
      "Epoch 240/500\n",
      "506/506 [==============================] - 0s 91us/step - loss: 4.6106 - mean_squared_error: 4.6106\n",
      "Epoch 241/500\n",
      "506/506 [==============================] - 0s 94us/step - loss: 4.6465 - mean_squared_error: 4.6465\n",
      "Epoch 242/500\n",
      "506/506 [==============================] - 0s 97us/step - loss: 4.6951 - mean_squared_error: 4.6951\n",
      "Epoch 243/500\n",
      "506/506 [==============================] - 0s 95us/step - loss: 4.6806 - mean_squared_error: 4.6806\n",
      "Epoch 244/500\n",
      "506/506 [==============================] - 0s 89us/step - loss: 4.7462 - mean_squared_error: 4.7462\n",
      "Epoch 245/500\n",
      "506/506 [==============================] - 0s 89us/step - loss: 4.5375 - mean_squared_error: 4.5375\n",
      "Epoch 246/500\n",
      "506/506 [==============================] - 0s 91us/step - loss: 4.6759 - mean_squared_error: 4.6759\n",
      "Epoch 247/500\n",
      "506/506 [==============================] - 0s 91us/step - loss: 4.6240 - mean_squared_error: 4.6240\n",
      "Epoch 248/500\n",
      "506/506 [==============================] - 0s 91us/step - loss: 4.5771 - mean_squared_error: 4.5771\n",
      "Epoch 249/500\n",
      "506/506 [==============================] - 0s 94us/step - loss: 4.5429 - mean_squared_error: 4.5429\n",
      "Epoch 250/500\n",
      "506/506 [==============================] - 0s 91us/step - loss: 4.4464 - mean_squared_error: 4.4464\n",
      "Epoch 251/500\n",
      "506/506 [==============================] - 0s 104us/step - loss: 4.6023 - mean_squared_error: 4.6023\n",
      "Epoch 252/500\n",
      "506/506 [==============================] - 0s 87us/step - loss: 4.7419 - mean_squared_error: 4.7419\n",
      "Epoch 253/500\n",
      "506/506 [==============================] - 0s 85us/step - loss: 4.6411 - mean_squared_error: 4.6411\n",
      "Epoch 254/500\n",
      "506/506 [==============================] - 0s 92us/step - loss: 4.4651 - mean_squared_error: 4.4651\n",
      "Epoch 255/500\n",
      "506/506 [==============================] - 0s 104us/step - loss: 5.0993 - mean_squared_error: 5.0993\n",
      "Epoch 256/500\n",
      "506/506 [==============================] - 0s 87us/step - loss: 4.6205 - mean_squared_error: 4.6205\n",
      "Epoch 257/500\n",
      "506/506 [==============================] - 0s 93us/step - loss: 4.5108 - mean_squared_error: 4.5108\n",
      "Epoch 258/500\n",
      "506/506 [==============================] - 0s 179us/step - loss: 4.4021 - mean_squared_error: 4.4021\n",
      "Epoch 259/500\n",
      "506/506 [==============================] - 0s 148us/step - loss: 4.4144 - mean_squared_error: 4.4144\n",
      "Epoch 260/500\n",
      "506/506 [==============================] - 0s 108us/step - loss: 4.5199 - mean_squared_error: 4.5199\n",
      "Epoch 261/500\n",
      "506/506 [==============================] - 0s 85us/step - loss: 4.5313 - mean_squared_error: 4.5313\n",
      "Epoch 262/500\n",
      "506/506 [==============================] - 0s 85us/step - loss: 4.5360 - mean_squared_error: 4.5360\n",
      "Epoch 263/500\n",
      "506/506 [==============================] - 0s 81us/step - loss: 4.3265 - mean_squared_error: 4.3265\n",
      "Epoch 264/500\n",
      "506/506 [==============================] - 0s 81us/step - loss: 4.6416 - mean_squared_error: 4.6416\n",
      "Epoch 265/500\n",
      "506/506 [==============================] - 0s 74us/step - loss: 4.5866 - mean_squared_error: 4.5866\n",
      "Epoch 266/500\n",
      "506/506 [==============================] - 0s 83us/step - loss: 4.3662 - mean_squared_error: 4.3662\n",
      "Epoch 267/500\n",
      "506/506 [==============================] - 0s 190us/step - loss: 4.4980 - mean_squared_error: 4.4980\n",
      "Epoch 268/500\n",
      "506/506 [==============================] - 0s 163us/step - loss: 4.2951 - mean_squared_error: 4.2951\n",
      "Epoch 269/500\n",
      "506/506 [==============================] - 0s 93us/step - loss: 4.2424 - mean_squared_error: 4.2424\n",
      "Epoch 270/500\n",
      "506/506 [==============================] - 0s 91us/step - loss: 4.3522 - mean_squared_error: 4.3522\n",
      "Epoch 271/500\n",
      "506/506 [==============================] - 0s 176us/step - loss: 4.8447 - mean_squared_error: 4.8447\n",
      "Epoch 272/500\n",
      "506/506 [==============================] - 0s 108us/step - loss: 4.3061 - mean_squared_error: 4.3061\n",
      "Epoch 273/500\n",
      "506/506 [==============================] - 0s 104us/step - loss: 4.3464 - mean_squared_error: 4.3464\n",
      "Epoch 274/500\n",
      "506/506 [==============================] - 0s 134us/step - loss: 4.3045 - mean_squared_error: 4.3045\n",
      "Epoch 275/500\n",
      "506/506 [==============================] - 0s 91us/step - loss: 4.4960 - mean_squared_error: 4.4960\n",
      "Epoch 276/500\n",
      "506/506 [==============================] - 0s 89us/step - loss: 4.3734 - mean_squared_error: 4.3734\n",
      "Epoch 277/500\n",
      "506/506 [==============================] - 0s 176us/step - loss: 4.1506 - mean_squared_error: 4.1506\n",
      "Epoch 278/500\n",
      "506/506 [==============================] - 0s 144us/step - loss: 4.4517 - mean_squared_error: 4.4517\n",
      "Epoch 279/500\n",
      "506/506 [==============================] - 0s 134us/step - loss: 4.1406 - mean_squared_error: 4.1406\n",
      "Epoch 280/500\n",
      "506/506 [==============================] - 0s 91us/step - loss: 4.2287 - mean_squared_error: 4.2287\n",
      "Epoch 281/500\n",
      "506/506 [==============================] - 0s 93us/step - loss: 4.2145 - mean_squared_error: 4.2145\n",
      "Epoch 282/500\n",
      "506/506 [==============================] - 0s 89us/step - loss: 4.2187 - mean_squared_error: 4.2187\n",
      "Epoch 283/500\n",
      "506/506 [==============================] - 0s 187us/step - loss: 4.1419 - mean_squared_error: 4.1419\n",
      "Epoch 284/500\n",
      "506/506 [==============================] - 0s 82us/step - loss: 4.3326 - mean_squared_error: 4.3326\n",
      "Epoch 285/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "506/506 [==============================] - 0s 83us/step - loss: 4.0792 - mean_squared_error: 4.0792\n",
      "Epoch 286/500\n",
      "506/506 [==============================] - 0s 75us/step - loss: 4.4224 - mean_squared_error: 4.4224\n",
      "Epoch 287/500\n",
      "506/506 [==============================] - 0s 82us/step - loss: 4.1507 - mean_squared_error: 4.1507\n",
      "Epoch 288/500\n",
      "506/506 [==============================] - 0s 179us/step - loss: 4.1635 - mean_squared_error: 4.1635\n",
      "Epoch 289/500\n",
      "506/506 [==============================] - 0s 158us/step - loss: 4.4838 - mean_squared_error: 4.4838\n",
      "Epoch 290/500\n",
      "506/506 [==============================] - 0s 97us/step - loss: 4.1479 - mean_squared_error: 4.1479\n",
      "Epoch 291/500\n",
      "506/506 [==============================] - 0s 89us/step - loss: 4.0680 - mean_squared_error: 4.0680\n",
      "Epoch 292/500\n",
      "506/506 [==============================] - 0s 94us/step - loss: 3.9875 - mean_squared_error: 3.9875\n",
      "Epoch 293/500\n",
      "506/506 [==============================] - 0s 83us/step - loss: 3.9833 - mean_squared_error: 3.9833\n",
      "Epoch 294/500\n",
      "506/506 [==============================] - 0s 79us/step - loss: 4.0719 - mean_squared_error: 4.0719\n",
      "Epoch 295/500\n",
      "506/506 [==============================] - 0s 77us/step - loss: 4.1047 - mean_squared_error: 4.1047\n",
      "Epoch 296/500\n",
      "506/506 [==============================] - 0s 84us/step - loss: 4.0102 - mean_squared_error: 4.0102\n",
      "Epoch 297/500\n",
      "506/506 [==============================] - 0s 81us/step - loss: 3.9289 - mean_squared_error: 3.9289\n",
      "Epoch 298/500\n",
      "506/506 [==============================] - 0s 83us/step - loss: 4.0526 - mean_squared_error: 4.0526\n",
      "Epoch 299/500\n",
      "506/506 [==============================] - 0s 82us/step - loss: 4.0002 - mean_squared_error: 4.0002\n",
      "Epoch 300/500\n",
      "506/506 [==============================] - 0s 83us/step - loss: 3.9635 - mean_squared_error: 3.9635\n",
      "Epoch 301/500\n",
      "506/506 [==============================] - 0s 99us/step - loss: 3.9902 - mean_squared_error: 3.9902\n",
      "Epoch 302/500\n",
      "506/506 [==============================] - 0s 183us/step - loss: 4.0163 - mean_squared_error: 4.0163\n",
      "Epoch 303/500\n",
      "506/506 [==============================] - 0s 182us/step - loss: 4.2604 - mean_squared_error: 4.2604\n",
      "Epoch 304/500\n",
      "506/506 [==============================] - 0s 138us/step - loss: 4.0461 - mean_squared_error: 4.0461\n",
      "Epoch 305/500\n",
      "506/506 [==============================] - 0s 90us/step - loss: 3.9116 - mean_squared_error: 3.9116\n",
      "Epoch 306/500\n",
      "506/506 [==============================] - 0s 85us/step - loss: 3.9296 - mean_squared_error: 3.9296\n",
      "Epoch 307/500\n",
      "506/506 [==============================] - 0s 86us/step - loss: 3.9874 - mean_squared_error: 3.9874\n",
      "Epoch 308/500\n",
      "506/506 [==============================] - 0s 83us/step - loss: 3.9605 - mean_squared_error: 3.9605\n",
      "Epoch 309/500\n",
      "506/506 [==============================] - 0s 75us/step - loss: 4.2900 - mean_squared_error: 4.2900\n",
      "Epoch 310/500\n",
      "506/506 [==============================] - 0s 100us/step - loss: 3.8930 - mean_squared_error: 3.8930\n",
      "Epoch 311/500\n",
      "506/506 [==============================] - 0s 156us/step - loss: 3.8623 - mean_squared_error: 3.8623\n",
      "Epoch 312/500\n",
      "506/506 [==============================] - 0s 186us/step - loss: 4.2365 - mean_squared_error: 4.2365\n",
      "Epoch 313/500\n",
      "506/506 [==============================] - 0s 102us/step - loss: 4.0637 - mean_squared_error: 4.0637\n",
      "Epoch 314/500\n",
      "506/506 [==============================] - 0s 171us/step - loss: 3.8819 - mean_squared_error: 3.8819\n",
      "Epoch 315/500\n",
      "506/506 [==============================] - 0s 123us/step - loss: 3.8632 - mean_squared_error: 3.8632\n",
      "Epoch 316/500\n",
      "506/506 [==============================] - 0s 97us/step - loss: 3.7452 - mean_squared_error: 3.7452\n",
      "Epoch 317/500\n",
      "506/506 [==============================] - 0s 91us/step - loss: 3.7974 - mean_squared_error: 3.7974\n",
      "Epoch 318/500\n",
      "506/506 [==============================] - 0s 95us/step - loss: 3.9336 - mean_squared_error: 3.9336\n",
      "Epoch 319/500\n",
      "506/506 [==============================] - 0s 95us/step - loss: 3.9303 - mean_squared_error: 3.9303\n",
      "Epoch 320/500\n",
      "506/506 [==============================] - 0s 102us/step - loss: 3.8572 - mean_squared_error: 3.8572\n",
      "Epoch 321/500\n",
      "506/506 [==============================] - 0s 179us/step - loss: 3.7606 - mean_squared_error: 3.7606\n",
      "Epoch 322/500\n",
      "506/506 [==============================] - 0s 187us/step - loss: 4.0181 - mean_squared_error: 4.0181\n",
      "Epoch 323/500\n",
      "506/506 [==============================] - 0s 151us/step - loss: 3.7788 - mean_squared_error: 3.7788\n",
      "Epoch 324/500\n",
      "506/506 [==============================] - 0s 142us/step - loss: 3.6749 - mean_squared_error: 3.6749\n",
      "Epoch 325/500\n",
      "506/506 [==============================] - 0s 96us/step - loss: 3.7806 - mean_squared_error: 3.7806\n",
      "Epoch 326/500\n",
      "506/506 [==============================] - 0s 88us/step - loss: 3.8723 - mean_squared_error: 3.8723\n",
      "Epoch 327/500\n",
      "506/506 [==============================] - 0s 91us/step - loss: 3.6920 - mean_squared_error: 3.6920\n",
      "Epoch 328/500\n",
      "506/506 [==============================] - 0s 87us/step - loss: 3.7464 - mean_squared_error: 3.7464\n",
      "Epoch 329/500\n",
      "506/506 [==============================] - 0s 87us/step - loss: 3.7146 - mean_squared_error: 3.7146\n",
      "Epoch 330/500\n",
      "506/506 [==============================] - 0s 94us/step - loss: 3.8083 - mean_squared_error: 3.8083\n",
      "Epoch 331/500\n",
      "506/506 [==============================] - 0s 91us/step - loss: 3.5430 - mean_squared_error: 3.5430\n",
      "Epoch 332/500\n",
      "506/506 [==============================] - 0s 81us/step - loss: 3.6931 - mean_squared_error: 3.6931\n",
      "Epoch 333/500\n",
      "506/506 [==============================] - 0s 81us/step - loss: 3.7458 - mean_squared_error: 3.7458\n",
      "Epoch 334/500\n",
      "506/506 [==============================] - 0s 83us/step - loss: 3.7206 - mean_squared_error: 3.7206\n",
      "Epoch 335/500\n",
      "506/506 [==============================] - 0s 80us/step - loss: 3.6425 - mean_squared_error: 3.6425\n",
      "Epoch 336/500\n",
      "506/506 [==============================] - 0s 168us/step - loss: 3.5593 - mean_squared_error: 3.5593\n",
      "Epoch 337/500\n",
      "506/506 [==============================] - 0s 191us/step - loss: 3.7763 - mean_squared_error: 3.7763\n",
      "Epoch 338/500\n",
      "506/506 [==============================] - 0s 146us/step - loss: 3.8894 - mean_squared_error: 3.8894\n",
      "Epoch 339/500\n",
      "506/506 [==============================] - 0s 118us/step - loss: 3.7093 - mean_squared_error: 3.7093\n",
      "Epoch 340/500\n",
      "506/506 [==============================] - 0s 110us/step - loss: 3.9059 - mean_squared_error: 3.9059\n",
      "Epoch 341/500\n",
      "506/506 [==============================] - 0s 107us/step - loss: 3.6316 - mean_squared_error: 3.6316\n",
      "Epoch 342/500\n",
      "506/506 [==============================] - 0s 102us/step - loss: 3.6554 - mean_squared_error: 3.6554\n",
      "Epoch 343/500\n",
      "506/506 [==============================] - 0s 116us/step - loss: 3.7369 - mean_squared_error: 3.7369\n",
      "Epoch 344/500\n",
      "506/506 [==============================] - 0s 177us/step - loss: 3.8891 - mean_squared_error: 3.8891\n",
      "Epoch 345/500\n",
      "506/506 [==============================] - 0s 181us/step - loss: 3.4826 - mean_squared_error: 3.4826\n",
      "Epoch 346/500\n",
      "506/506 [==============================] - 0s 189us/step - loss: 3.7538 - mean_squared_error: 3.7538\n",
      "Epoch 347/500\n",
      "506/506 [==============================] - 0s 112us/step - loss: 3.6924 - mean_squared_error: 3.6924\n",
      "Epoch 348/500\n",
      "506/506 [==============================] - 0s 83us/step - loss: 3.5252 - mean_squared_error: 3.5252\n",
      "Epoch 349/500\n",
      "506/506 [==============================] - 0s 76us/step - loss: 3.5234 - mean_squared_error: 3.5234\n",
      "Epoch 350/500\n",
      "506/506 [==============================] - 0s 170us/step - loss: 3.4746 - mean_squared_error: 3.4746\n",
      "Epoch 351/500\n",
      "506/506 [==============================] - 0s 148us/step - loss: 3.4458 - mean_squared_error: 3.4458\n",
      "Epoch 352/500\n",
      "506/506 [==============================] - 0s 143us/step - loss: 3.5178 - mean_squared_error: 3.5178\n",
      "Epoch 353/500\n",
      "506/506 [==============================] - 0s 124us/step - loss: 3.4823 - mean_squared_error: 3.4823\n",
      "Epoch 354/500\n",
      "506/506 [==============================] - 0s 91us/step - loss: 3.4517 - mean_squared_error: 3.4517\n",
      "Epoch 355/500\n",
      "506/506 [==============================] - 0s 81us/step - loss: 3.7214 - mean_squared_error: 3.7214\n",
      "Epoch 356/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "506/506 [==============================] - 0s 79us/step - loss: 3.5181 - mean_squared_error: 3.5181\n",
      "Epoch 357/500\n",
      "506/506 [==============================] - 0s 85us/step - loss: 3.5744 - mean_squared_error: 3.5744\n",
      "Epoch 358/500\n",
      "506/506 [==============================] - 0s 89us/step - loss: 3.6162 - mean_squared_error: 3.6162\n",
      "Epoch 359/500\n",
      "506/506 [==============================] - 0s 95us/step - loss: 3.5114 - mean_squared_error: 3.5114\n",
      "Epoch 360/500\n",
      "506/506 [==============================] - 0s 93us/step - loss: 3.4297 - mean_squared_error: 3.4297\n",
      "Epoch 361/500\n",
      "506/506 [==============================] - 0s 87us/step - loss: 3.4897 - mean_squared_error: 3.4897\n",
      "Epoch 362/500\n",
      "506/506 [==============================] - 0s 85us/step - loss: 3.5273 - mean_squared_error: 3.5273\n",
      "Epoch 363/500\n",
      "506/506 [==============================] - 0s 171us/step - loss: 3.5205 - mean_squared_error: 3.5205\n",
      "Epoch 364/500\n",
      "506/506 [==============================] - 0s 184us/step - loss: 3.3410 - mean_squared_error: 3.3410\n",
      "Epoch 365/500\n",
      "506/506 [==============================] - 0s 98us/step - loss: 3.4235 - mean_squared_error: 3.4235\n",
      "Epoch 366/500\n",
      "506/506 [==============================] - 0s 97us/step - loss: 3.8235 - mean_squared_error: 3.8235\n",
      "Epoch 367/500\n",
      "506/506 [==============================] - 0s 99us/step - loss: 3.3721 - mean_squared_error: 3.3721\n",
      "Epoch 368/500\n",
      "506/506 [==============================] - 0s 88us/step - loss: 3.3185 - mean_squared_error: 3.3185\n",
      "Epoch 369/500\n",
      "506/506 [==============================] - 0s 83us/step - loss: 3.3590 - mean_squared_error: 3.3590\n",
      "Epoch 370/500\n",
      "506/506 [==============================] - 0s 174us/step - loss: 3.4819 - mean_squared_error: 3.4819\n",
      "Epoch 371/500\n",
      "506/506 [==============================] - 0s 155us/step - loss: 3.3904 - mean_squared_error: 3.3904\n",
      "Epoch 372/500\n",
      "506/506 [==============================] - 0s 160us/step - loss: 3.5124 - mean_squared_error: 3.5124\n",
      "Epoch 373/500\n",
      "506/506 [==============================] - 0s 144us/step - loss: 3.2842 - mean_squared_error: 3.2842\n",
      "Epoch 374/500\n",
      "506/506 [==============================] - 0s 152us/step - loss: 3.6406 - mean_squared_error: 3.6406\n",
      "Epoch 375/500\n",
      "506/506 [==============================] - 0s 97us/step - loss: 3.3043 - mean_squared_error: 3.3043\n",
      "Epoch 376/500\n",
      "506/506 [==============================] - 0s 144us/step - loss: 3.3696 - mean_squared_error: 3.3696\n",
      "Epoch 377/500\n",
      "506/506 [==============================] - 0s 160us/step - loss: 3.3632 - mean_squared_error: 3.3632\n",
      "Epoch 378/500\n",
      "506/506 [==============================] - 0s 126us/step - loss: 3.5136 - mean_squared_error: 3.5136\n",
      "Epoch 379/500\n",
      "506/506 [==============================] - 0s 178us/step - loss: 3.6213 - mean_squared_error: 3.6213\n",
      "Epoch 380/500\n",
      "506/506 [==============================] - 0s 134us/step - loss: 3.3050 - mean_squared_error: 3.3050\n",
      "Epoch 381/500\n",
      "506/506 [==============================] - 0s 105us/step - loss: 3.3927 - mean_squared_error: 3.3927\n",
      "Epoch 382/500\n",
      "506/506 [==============================] - 0s 143us/step - loss: 3.4302 - mean_squared_error: 3.4302\n",
      "Epoch 383/500\n",
      "506/506 [==============================] - 0s 83us/step - loss: 3.3867 - mean_squared_error: 3.3867\n",
      "Epoch 384/500\n",
      "506/506 [==============================] - 0s 85us/step - loss: 3.7484 - mean_squared_error: 3.7484\n",
      "Epoch 385/500\n",
      "506/506 [==============================] - 0s 75us/step - loss: 3.1623 - mean_squared_error: 3.1623\n",
      "Epoch 386/500\n",
      "506/506 [==============================] - 0s 81us/step - loss: 3.2158 - mean_squared_error: 3.2158\n",
      "Epoch 387/500\n",
      "506/506 [==============================] - 0s 81us/step - loss: 3.1805 - mean_squared_error: 3.1805\n",
      "Epoch 388/500\n",
      "506/506 [==============================] - 0s 79us/step - loss: 3.1685 - mean_squared_error: 3.1685\n",
      "Epoch 389/500\n",
      "506/506 [==============================] - 0s 89us/step - loss: 3.1781 - mean_squared_error: 3.1781\n",
      "Epoch 390/500\n",
      "506/506 [==============================] - 0s 83us/step - loss: 3.5343 - mean_squared_error: 3.5343\n",
      "Epoch 391/500\n",
      "506/506 [==============================] - 0s 87us/step - loss: 3.4159 - mean_squared_error: 3.4159\n",
      "Epoch 392/500\n",
      "506/506 [==============================] - 0s 89us/step - loss: 3.2555 - mean_squared_error: 3.2555\n",
      "Epoch 393/500\n",
      "506/506 [==============================] - 0s 101us/step - loss: 3.1886 - mean_squared_error: 3.1886\n",
      "Epoch 394/500\n",
      "506/506 [==============================] - 0s 184us/step - loss: 3.1723 - mean_squared_error: 3.1723\n",
      "Epoch 395/500\n",
      "506/506 [==============================] - 0s 184us/step - loss: 3.3160 - mean_squared_error: 3.3160\n",
      "Epoch 396/500\n",
      "506/506 [==============================] - 0s 189us/step - loss: 3.2133 - mean_squared_error: 3.2133\n",
      "Epoch 397/500\n",
      "506/506 [==============================] - 0s 168us/step - loss: 3.1858 - mean_squared_error: 3.1858\n",
      "Epoch 398/500\n",
      "506/506 [==============================] - 0s 79us/step - loss: 3.3533 - mean_squared_error: 3.3533\n",
      "Epoch 399/500\n",
      "506/506 [==============================] - 0s 81us/step - loss: 3.2886 - mean_squared_error: 3.2886\n",
      "Epoch 400/500\n",
      "506/506 [==============================] - 0s 88us/step - loss: 3.1866 - mean_squared_error: 3.1866\n",
      "Epoch 401/500\n",
      "506/506 [==============================] - 0s 89us/step - loss: 3.1669 - mean_squared_error: 3.1669\n",
      "Epoch 402/500\n",
      "506/506 [==============================] - 0s 84us/step - loss: 3.2505 - mean_squared_error: 3.2505\n",
      "Epoch 403/500\n",
      "506/506 [==============================] - 0s 87us/step - loss: 3.1743 - mean_squared_error: 3.1743\n",
      "Epoch 404/500\n",
      "506/506 [==============================] - 0s 76us/step - loss: 3.2553 - mean_squared_error: 3.2553\n",
      "Epoch 405/500\n",
      "506/506 [==============================] - 0s 75us/step - loss: 3.1273 - mean_squared_error: 3.1273\n",
      "Epoch 406/500\n",
      "506/506 [==============================] - 0s 85us/step - loss: 3.0922 - mean_squared_error: 3.0922\n",
      "Epoch 407/500\n",
      "506/506 [==============================] - 0s 85us/step - loss: 3.0672 - mean_squared_error: 3.0672\n",
      "Epoch 408/500\n",
      "506/506 [==============================] - 0s 83us/step - loss: 3.1260 - mean_squared_error: 3.1260\n",
      "Epoch 409/500\n",
      "506/506 [==============================] - 0s 89us/step - loss: 3.1767 - mean_squared_error: 3.1767\n",
      "Epoch 410/500\n",
      "506/506 [==============================] - 0s 162us/step - loss: 3.2471 - mean_squared_error: 3.2471\n",
      "Epoch 411/500\n",
      "506/506 [==============================] - 0s 184us/step - loss: 3.1605 - mean_squared_error: 3.1605\n",
      "Epoch 412/500\n",
      "506/506 [==============================] - 0s 182us/step - loss: 3.4331 - mean_squared_error: 3.4331\n",
      "Epoch 413/500\n",
      "506/506 [==============================] - 0s 142us/step - loss: 3.0346 - mean_squared_error: 3.0346\n",
      "Epoch 414/500\n",
      "506/506 [==============================] - 0s 143us/step - loss: 3.0699 - mean_squared_error: 3.0699\n",
      "Epoch 415/500\n",
      "506/506 [==============================] - 0s 93us/step - loss: 3.0200 - mean_squared_error: 3.0200\n",
      "Epoch 416/500\n",
      "506/506 [==============================] - 0s 89us/step - loss: 3.0890 - mean_squared_error: 3.0890\n",
      "Epoch 417/500\n",
      "506/506 [==============================] - 0s 77us/step - loss: 3.0392 - mean_squared_error: 3.0392\n",
      "Epoch 418/500\n",
      "506/506 [==============================] - 0s 77us/step - loss: 3.0318 - mean_squared_error: 3.0318\n",
      "Epoch 419/500\n",
      "506/506 [==============================] - 0s 136us/step - loss: 3.0672 - mean_squared_error: 3.0672\n",
      "Epoch 420/500\n",
      "506/506 [==============================] - 0s 85us/step - loss: 3.1567 - mean_squared_error: 3.1567\n",
      "Epoch 421/500\n",
      "506/506 [==============================] - 0s 97us/step - loss: 3.0846 - mean_squared_error: 3.0846\n",
      "Epoch 422/500\n",
      "506/506 [==============================] - 0s 95us/step - loss: 3.0631 - mean_squared_error: 3.0631\n",
      "Epoch 423/500\n",
      "506/506 [==============================] - 0s 93us/step - loss: 3.0808 - mean_squared_error: 3.0808\n",
      "Epoch 424/500\n",
      "506/506 [==============================] - 0s 87us/step - loss: 2.9908 - mean_squared_error: 2.9908\n",
      "Epoch 425/500\n",
      "506/506 [==============================] - 0s 93us/step - loss: 3.0640 - mean_squared_error: 3.0640\n",
      "Epoch 426/500\n",
      "506/506 [==============================] - 0s 83us/step - loss: 3.1235 - mean_squared_error: 3.1235\n",
      "Epoch 427/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "506/506 [==============================] - 0s 95us/step - loss: 3.0419 - mean_squared_error: 3.0419\n",
      "Epoch 428/500\n",
      "506/506 [==============================] - 0s 97us/step - loss: 2.9446 - mean_squared_error: 2.9446\n",
      "Epoch 429/500\n",
      "506/506 [==============================] - 0s 87us/step - loss: 2.9729 - mean_squared_error: 2.9729\n",
      "Epoch 430/500\n",
      "506/506 [==============================] - 0s 73us/step - loss: 3.0044 - mean_squared_error: 3.0044\n",
      "Epoch 431/500\n",
      "506/506 [==============================] - 0s 83us/step - loss: 2.9969 - mean_squared_error: 2.9969\n",
      "Epoch 432/500\n",
      "506/506 [==============================] - 0s 86us/step - loss: 3.0738 - mean_squared_error: 3.0738\n",
      "Epoch 433/500\n",
      "506/506 [==============================] - 0s 82us/step - loss: 3.1559 - mean_squared_error: 3.1559\n",
      "Epoch 434/500\n",
      "506/506 [==============================] - 0s 91us/step - loss: 3.0636 - mean_squared_error: 3.0636\n",
      "Epoch 435/500\n",
      "506/506 [==============================] - 0s 75us/step - loss: 2.8896 - mean_squared_error: 2.8896\n",
      "Epoch 436/500\n",
      "506/506 [==============================] - 0s 89us/step - loss: 3.0582 - mean_squared_error: 3.0582\n",
      "Epoch 437/500\n",
      "506/506 [==============================] - 0s 93us/step - loss: 3.2709 - mean_squared_error: 3.2709\n",
      "Epoch 438/500\n",
      "506/506 [==============================] - 0s 182us/step - loss: 2.9654 - mean_squared_error: 2.9654\n",
      "Epoch 439/500\n",
      "506/506 [==============================] - 0s 148us/step - loss: 2.8996 - mean_squared_error: 2.8996\n",
      "Epoch 440/500\n",
      "506/506 [==============================] - 0s 97us/step - loss: 2.8925 - mean_squared_error: 2.8925\n",
      "Epoch 441/500\n",
      "506/506 [==============================] - 0s 90us/step - loss: 3.0757 - mean_squared_error: 3.0757\n",
      "Epoch 442/500\n",
      "506/506 [==============================] - 0s 90us/step - loss: 2.9787 - mean_squared_error: 2.9787\n",
      "Epoch 443/500\n",
      "506/506 [==============================] - 0s 85us/step - loss: 2.9776 - mean_squared_error: 2.9776\n",
      "Epoch 444/500\n",
      "506/506 [==============================] - 0s 83us/step - loss: 2.9956 - mean_squared_error: 2.9956\n",
      "Epoch 445/500\n",
      "506/506 [==============================] - 0s 87us/step - loss: 2.9024 - mean_squared_error: 2.9024\n",
      "Epoch 446/500\n",
      "506/506 [==============================] - 0s 80us/step - loss: 2.7746 - mean_squared_error: 2.7746\n",
      "Epoch 447/500\n",
      "506/506 [==============================] - 0s 88us/step - loss: 2.8172 - mean_squared_error: 2.8172\n",
      "Epoch 448/500\n",
      "506/506 [==============================] - 0s 81us/step - loss: 2.9808 - mean_squared_error: 2.9808\n",
      "Epoch 449/500\n",
      "506/506 [==============================] - 0s 90us/step - loss: 2.8032 - mean_squared_error: 2.8032\n",
      "Epoch 450/500\n",
      "506/506 [==============================] - 0s 89us/step - loss: 2.8995 - mean_squared_error: 2.8995\n",
      "Epoch 451/500\n",
      "506/506 [==============================] - 0s 81us/step - loss: 2.8205 - mean_squared_error: 2.8205\n",
      "Epoch 452/500\n",
      "506/506 [==============================] - 0s 188us/step - loss: 2.7611 - mean_squared_error: 2.7611\n",
      "Epoch 453/500\n",
      "506/506 [==============================] - 0s 122us/step - loss: 2.9637 - mean_squared_error: 2.9637\n",
      "Epoch 454/500\n",
      "506/506 [==============================] - 0s 73us/step - loss: 2.8634 - mean_squared_error: 2.8634\n",
      "Epoch 455/500\n",
      "506/506 [==============================] - 0s 83us/step - loss: 2.8756 - mean_squared_error: 2.8756\n",
      "Epoch 456/500\n",
      "506/506 [==============================] - 0s 77us/step - loss: 3.1840 - mean_squared_error: 3.1840\n",
      "Epoch 457/500\n",
      "506/506 [==============================] - 0s 82us/step - loss: 2.8537 - mean_squared_error: 2.8537\n",
      "Epoch 458/500\n",
      "506/506 [==============================] - 0s 185us/step - loss: 3.0257 - mean_squared_error: 3.0257\n",
      "Epoch 459/500\n",
      "506/506 [==============================] - 0s 154us/step - loss: 2.8214 - mean_squared_error: 2.8214\n",
      "Epoch 460/500\n",
      "506/506 [==============================] - 0s 136us/step - loss: 2.7852 - mean_squared_error: 2.7852\n",
      "Epoch 461/500\n",
      "506/506 [==============================] - 0s 85us/step - loss: 2.9439 - mean_squared_error: 2.9439\n",
      "Epoch 462/500\n",
      "506/506 [==============================] - 0s 87us/step - loss: 2.7491 - mean_squared_error: 2.7491\n",
      "Epoch 463/500\n",
      "506/506 [==============================] - 0s 95us/step - loss: 2.7658 - mean_squared_error: 2.7658\n",
      "Epoch 464/500\n",
      "506/506 [==============================] - 0s 176us/step - loss: 2.6887 - mean_squared_error: 2.6887\n",
      "Epoch 465/500\n",
      "506/506 [==============================] - 0s 132us/step - loss: 2.8583 - mean_squared_error: 2.8583\n",
      "Epoch 466/500\n",
      "506/506 [==============================] - 0s 179us/step - loss: 2.8316 - mean_squared_error: 2.8316\n",
      "Epoch 467/500\n",
      "506/506 [==============================] - 0s 100us/step - loss: 2.6755 - mean_squared_error: 2.6755\n",
      "Epoch 468/500\n",
      "506/506 [==============================] - 0s 93us/step - loss: 2.7665 - mean_squared_error: 2.7665\n",
      "Epoch 469/500\n",
      "506/506 [==============================] - 0s 94us/step - loss: 2.8897 - mean_squared_error: 2.8897\n",
      "Epoch 470/500\n",
      "506/506 [==============================] - 0s 90us/step - loss: 2.8131 - mean_squared_error: 2.8131\n",
      "Epoch 471/500\n",
      "506/506 [==============================] - 0s 89us/step - loss: 2.7847 - mean_squared_error: 2.7847\n",
      "Epoch 472/500\n",
      "506/506 [==============================] - 0s 91us/step - loss: 2.7789 - mean_squared_error: 2.7789\n",
      "Epoch 473/500\n",
      "506/506 [==============================] - 0s 90us/step - loss: 2.8701 - mean_squared_error: 2.8701\n",
      "Epoch 474/500\n",
      "506/506 [==============================] - 0s 89us/step - loss: 2.6970 - mean_squared_error: 2.6970\n",
      "Epoch 475/500\n",
      "506/506 [==============================] - 0s 85us/step - loss: 2.8311 - mean_squared_error: 2.8311\n",
      "Epoch 476/500\n",
      "506/506 [==============================] - 0s 92us/step - loss: 2.8524 - mean_squared_error: 2.8524\n",
      "Epoch 477/500\n",
      "506/506 [==============================] - 0s 97us/step - loss: 2.7703 - mean_squared_error: 2.7703\n",
      "Epoch 478/500\n",
      "506/506 [==============================] - 0s 91us/step - loss: 2.7941 - mean_squared_error: 2.7941\n",
      "Epoch 479/500\n",
      "506/506 [==============================] - 0s 93us/step - loss: 2.6961 - mean_squared_error: 2.6961\n",
      "Epoch 480/500\n",
      "506/506 [==============================] - 0s 95us/step - loss: 2.9188 - mean_squared_error: 2.9188\n",
      "Epoch 481/500\n",
      "506/506 [==============================] - 0s 93us/step - loss: 2.6577 - mean_squared_error: 2.6577\n",
      "Epoch 482/500\n",
      "506/506 [==============================] - 0s 95us/step - loss: 2.6783 - mean_squared_error: 2.6783\n",
      "Epoch 483/500\n",
      "506/506 [==============================] - 0s 94us/step - loss: 2.7805 - mean_squared_error: 2.7805\n",
      "Epoch 484/500\n",
      "506/506 [==============================] - 0s 93us/step - loss: 2.7691 - mean_squared_error: 2.7691\n",
      "Epoch 485/500\n",
      "506/506 [==============================] - 0s 97us/step - loss: 2.6987 - mean_squared_error: 2.6987\n",
      "Epoch 486/500\n",
      "506/506 [==============================] - 0s 93us/step - loss: 2.7238 - mean_squared_error: 2.7238\n",
      "Epoch 487/500\n",
      "506/506 [==============================] - 0s 100us/step - loss: 2.8874 - mean_squared_error: 2.8874\n",
      "Epoch 488/500\n",
      "506/506 [==============================] - 0s 113us/step - loss: 2.7647 - mean_squared_error: 2.7647\n",
      "Epoch 489/500\n",
      "506/506 [==============================] - 0s 104us/step - loss: 2.7037 - mean_squared_error: 2.7037\n",
      "Epoch 490/500\n",
      "506/506 [==============================] - 0s 106us/step - loss: 2.8170 - mean_squared_error: 2.8170\n",
      "Epoch 491/500\n",
      "506/506 [==============================] - 0s 105us/step - loss: 2.6814 - mean_squared_error: 2.6814\n",
      "Epoch 492/500\n",
      "506/506 [==============================] - 0s 108us/step - loss: 2.7567 - mean_squared_error: 2.7567\n",
      "Epoch 493/500\n",
      "506/506 [==============================] - 0s 105us/step - loss: 2.8842 - mean_squared_error: 2.8842\n",
      "Epoch 494/500\n",
      "506/506 [==============================] - 0s 104us/step - loss: 2.8804 - mean_squared_error: 2.8804\n",
      "Epoch 495/500\n",
      "506/506 [==============================] - 0s 104us/step - loss: 2.7352 - mean_squared_error: 2.7352\n",
      "Epoch 496/500\n",
      "506/506 [==============================] - 0s 103us/step - loss: 2.6489 - mean_squared_error: 2.6489\n",
      "Epoch 497/500\n",
      "506/506 [==============================] - 0s 109us/step - loss: 2.6258 - mean_squared_error: 2.6258\n",
      "Epoch 498/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "506/506 [==============================] - 0s 106us/step - loss: 2.6202 - mean_squared_error: 2.6202\n",
      "Epoch 499/500\n",
      "506/506 [==============================] - 0s 105us/step - loss: 2.7448 - mean_squared_error: 2.7448\n",
      "Epoch 500/500\n",
      "506/506 [==============================] - 0s 133us/step - loss: 2.5275 - mean_squared_error: 2.5275\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_, y_, epochs=500, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x23d1277c208>]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.history['loss'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 回答一下理论题目"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. What does a neuron compute?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "一个神经元是一个线性函数 y=Wx+b, 后面可能还跟着一个激活函数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  2. Why we use non-linear activation funcitons in neural networks?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.如果没有引入非线性激活函数，全是线性函数，那么无论多少层神经网络，都能化简为一层神经网络。  \n",
    "2.世界是复杂的，不是纯线性函数就可以拟合的，比如引入非线性函数，才可以更好地拟合复杂的世界。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. What is the 'Logistic Loss' ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logitstic Loss是Cross Entropy Loss在二分类场景中的特例。  \n",
    "loss = -(ylog(y^) + (1-y)log(1-y^))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Assume that you are building a binary classifier for detecting if an image containing cats, which activation functions would you recommen using for the output layer ?\n",
    "\n",
    "A. ReLU    \n",
    "B. Leaky ReLU    \n",
    "C. sigmoid    \n",
    "D. tanh  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "推荐使用sigmoid激活函数。sigmoid的取值范围是[0,1]，比较符合二分类场景。  \n",
    "relu范围是[0, +∞],leaky relu也是从0附近到+∞。  \n",
    "tanh范围是[-1,1].  \n",
    "相比之下，sigmoid最适合二分类。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Why we don't use zero initialization for all parameters ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "神经网络具有对称性。如果初始化全0，那么在这样的对称性结构下，反向传播/梯度下降计算后，相同层的多个神经元参数会相同，模型难以收敛。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Can you implement the softmax function using python ? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def softmax(y_input):\n",
    "    # 行为种类，每列为一个样本标签\n",
    "    temp = np.exp(y_input)\n",
    "    return temp/np.sum(temp, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.01798621, 0.04742587, 0.11920292, 0.26894142],\n",
       "       [0.98201379, 0.95257413, 0.88079708, 0.73105858]])"
      ]
     },
     "execution_count": 281,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax([[1,2,3,4], [5,5,5,5]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.实践题"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In this practical part, you will build a simple digits recognizer to check if the digit in the image is larger than 5. This assignmnet will guide you step by step to finish your first small project in this course ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1 - Packages  \n",
    "sklearn is a famous package for machine learning.   \n",
    "matplotlib is a common package for vasualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2 - Overvie of the dataset  \n",
    "    - a training set has m_train images labeled as 0 if the digit < 5 or 1 if the digit >= 5\n",
    "    - a test set contains m_test images labels as if the digit < 5 or 1 if the digit >= 5\n",
    "    - eah image if of shape (num_px, num_px ). Thus, each image is square(height=num_px and  width = num_px)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the data \n",
    "digits = datasets.load_digits()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x720 with 40 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Vilizating the data\n",
    "plt.figure(figsize=(15,10))\n",
    "for i in range(1,41):\n",
    "    plt.subplot(4,10,i)\n",
    "    plt.imshow(digits.data[i-1].reshape([8,8]),cmap=plt.cm.gray_r)\n",
    "    plt.text(3,10,str(digits.target[i-1]))\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training set and test set \n",
    "X_train, X_test, y_train, y_test = train_test_split(digits.data, digits.target, test_size=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reformulate the label. \n",
    "# If the digit is smaller than 5, the label is 0.\n",
    "# If the digit is larger than 5, the label is 1.\n",
    "\n",
    "y_train[y_train < 5 ] = 0\n",
    "y_train[y_train >= 5] = 1\n",
    "y_test[y_test < 5] = 0\n",
    "y_test[y_test >= 5] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1347, 64)\n",
      "(450, 64)\n",
      "(1347,)\n",
      "(450,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3- Architecture of the neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](./networks.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 直接用sklearn内部的逻辑斯蒂回归函数计算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "lr = LogisticRegression()\n",
    "lr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1,\n",
       "       0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1,\n",
       "       0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0,\n",
       "       1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1,\n",
       "       1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1,\n",
       "       0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
       "       0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0,\n",
       "       1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0,\n",
       "       0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0,\n",
       "       1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1,\n",
       "       1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1,\n",
       "       0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1,\n",
       "       0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0,\n",
       "       0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0,\n",
       "       0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0,\n",
       "       0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1,\n",
       "       1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1,\n",
       "       0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1,\n",
       "       0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1,\n",
       "       1, 0, 1, 1, 0, 1, 0, 0, 1, 1])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pre = lr.predict(X_test)\n",
    "y_pre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8711111111111111"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 计算预测的正确率\n",
    "np.mean(y_pre == y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8711111111111111"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 用自带的函数计算预测正确率\n",
    "lr.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用sklearn的神经网络模型计算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='relu', alpha=1e-05, batch_size='auto', beta_1=0.9,\n",
       "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "       hidden_layer_sizes=(10, 2), learning_rate='constant',\n",
       "       learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
       "       nesterovs_momentum=True, power_t=0.5, random_state=1, shuffle=True,\n",
       "       solver='lbfgs', tol=0.0001, validation_fraction=0.1, verbose=False,\n",
       "       warm_start=False)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "clf = MLPClassifier(solver='lbfgs', alpha=1e-5,\n",
    "                     hidden_layer_sizes=(10, 2), random_state=1)\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9644444444444444"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 计算正确率\n",
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Mathematical expression of the algorithm:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For one example $x^{(i)}$:   \n",
    " $$ z^{(i)} = w^T * x^{(i)} +b $$   \n",
    " $$ y^{(i)} = a^{(i)} = sigmoid(z^{(i)})$$   \n",
    " $$L(a^{(i)},y^{(i)}) = -y^{(i)} log(a^{(i)})-(1-y^{(i)})log(1-a^{(i)})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The total cost over all training examples:\n",
    "$$ J = \\frac{1}{m}\\sum_{i=1}^{m}L(a^{(i)},y^{(i)}) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4 - Building the algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.1- Activation function    \n",
    "###### Exercise:\n",
    "Finish the sigmoid funciton "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    '''\n",
    "    Compute the sigmoid of z\n",
    "    Arguments: z -- a scalar or numpy array of any size.\n",
    "    \n",
    "    Return:\n",
    "    s -- sigmoid(z)\n",
    "    '''\n",
    "    s = 1 / (1 + np.exp(-z))\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sigmoid([0,2]) = [0.5        0.88079708]\n",
      "sigmoid([[0,2], [0,2]]) = [[0.5        0.88079708]\n",
      " [0.5        0.88079708]]\n"
     ]
    }
   ],
   "source": [
    "# Test your code \n",
    "# The result should be [0.5 0.88079708]\n",
    "print(\"sigmoid([0,2]) = \" + str(sigmoid(np.array([0,2]))))\n",
    "print(\"sigmoid([[0,2], [0,2]]) = \" + str(sigmoid(np.array([[0,2], [0,2]]))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.1-Initializaing parameters\n",
    "###### Exercise:\n",
    "Finishe the initialize_parameters function below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random innitialize the parameters\n",
    "\n",
    "def initialize_parameters(dim):\n",
    "    '''\n",
    "    Argument: dim -- size of the w vector\n",
    "    \n",
    "    Returns:\n",
    "    w -- initialized vector of shape (dim,1)\n",
    "    b -- initializaed scalar\n",
    "    '''\n",
    "    \n",
    "    w = np.random.random((dim,1))\n",
    "    b = 0\n",
    "\n",
    "    assert(w.shape == (dim,1))\n",
    "    assert(isinstance(b,float) or isinstance(b,int))\n",
    "    \n",
    "    return w,b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3-Forward and backward propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Some mathematical expressions\n",
    "Forward Propagation:   \n",
    ". X    \n",
    ". A = $\\sigma(w^T*X+b) = (a^{(1)},a^{(2)},...,a^{(m)}$   \n",
    ". J = $-\\frac{1}{m} \\sum_{i=1}^{m}y^{(i)}log(a^{(i)}+(1-y^{(i)})log(1-a^{(i)})$       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some derivative: \n",
    "$$\\frac{\\partial{J}}{\\partial{w}} = \\frac{1}{m}X*(A-Y)^T$$   \n",
    "$$\\frac{\\partial{J}}{\\partial{b}} = \\frac{1}{m}\\sum_{i=1}^m(a^{(i)}-y^{(i)}) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Exercise:\n",
    "Finish the function below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [],
   "source": [
    "def propagate(w,b,X,Y):\n",
    "    '''\n",
    "    Implement the cost function and its gradient for the propagation\n",
    "    \n",
    "    Arguments:\n",
    "    w - weights\n",
    "    b - bias\n",
    "    X - data\n",
    "    Y - ground truth\n",
    "    '''\n",
    "    m = X.shape[1]\n",
    "    A = sigmoid(np.dot(w.T, X) + b)\n",
    "    temp = Y * np.log(A) + (1 - Y) * np.log(1 - A)\n",
    "\n",
    "    cost = -1 / m * np.sum(Y * np.log(A) + (1 - Y) * np.log(1 - A), axis=1)\n",
    "    \n",
    "    dw = 1 / m * np.dot(X, (A - Y).T)\n",
    "    db = 1 / m * np.sum(A - Y, axis=1)\n",
    "    \n",
    "    assert(dw.shape == w.shape)\n",
    "    assert(db.dtype == float)\n",
    "    cost = np.squeeze(cost)\n",
    "    assert(cost.shape == ())\n",
    "    \n",
    "    grads = {'dw':dw,\n",
    "             'db':db}\n",
    "    return grads, cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.4 -Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Exercise:\n",
    "Minimizing the cost function using gradient descent.   \n",
    "$$\\theta = \\theta - \\alpha*d\\theta$$ where $\\alpha$ is the learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize(w, b, X, Y, num_iterations, learning_rate, print_cost=False):\n",
    "    '''\n",
    "    This function optimize w and b by running a gradient descen algorithm\n",
    "    \n",
    "    Arguments:\n",
    "    w - weights\n",
    "    b - bias\n",
    "    X - data\n",
    "    Y - ground truth\n",
    "    num_iterations -- number of iterations of the optimization loop\n",
    "    learning_rate -- learning rate of the gradient descent update rule\n",
    "    print_cost -- True to print the loss every 100 steps\n",
    "    \n",
    "    Returns:\n",
    "    params - dictionary containing the weights w and bias b\n",
    "    grads -- dictionary containing the gradients of the weights and bias with respect to the cost function\n",
    "    costs -- list of all the costs computed during the optimization, this will be used to plot the learning curve.\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    batch_size = 50\n",
    "    for j in range(num_iterations):\n",
    "        costs = []\n",
    "        for i in range(0, X.shape[1], batch_size):\n",
    "            X_ = X[:,i:i+batch_size]\n",
    "            Y_ = Y[:,i:i+batch_size]\n",
    "\n",
    "            grads, cost = propagate(w,b,X_,Y_)\n",
    "\n",
    "            dw = grads['dw']\n",
    "            db = grads['db']\n",
    "\n",
    "            w -= learning_rate * dw\n",
    "            b -= learning_rate * db\n",
    "\n",
    "            costs.append(cost)\n",
    "        \n",
    "        cost = sum(costs)/len(costs)\n",
    "        if print_cost and j % 100 == 0:\n",
    "            print (\"Cost after iteration %i: %f\" %(j, cost))\n",
    "    \n",
    "    params = {\"w\":w,\n",
    "              \"b\":b}\n",
    "    \n",
    "    grads = {\"dw\":dw,\n",
    "             \"db\":db}\n",
    "     \n",
    "    return params, grads, cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Exercise\n",
    "The previous function will output the learned w and b. We are able to use w and b to predict the labels for a dataset X. Implement the predict() function.    \n",
    "Two steps to finish this task:   \n",
    "1. Calculate $\\hat{Y} = A = \\sigma(w^T*X+b)$   \n",
    "2. Convert the entries of a into 0 (if activation <= 0.5) or 1 (if activation > 0.5), stores the predictions in a vector Y_prediction. If you wish, you can use an if/else statement in a for loop (though there is also a way to vectorize this)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(w, b, X):\n",
    "    '''\n",
    "    Predict whether the label is 0 or 1 using learned logistic regression parameters (w, b)\n",
    "    \n",
    "    Arguments:\n",
    "    w -- weights\n",
    "    b -- bias \n",
    "    X -- data \n",
    "    \n",
    "    Returns:\n",
    "    Y_prediction -- a numpy array (vector) containing all predictions (0/1) for the examples in X\n",
    "    '''\n",
    "    m = X.shape[1]\n",
    "    Y_prediction = np.zeros((1,m))\n",
    "    w = w.reshape(X.shape[0],1)\n",
    "    \n",
    "    A = sigmoid(np.dot(w.T, X) + b)\n",
    "    \n",
    "    for i in range(A.shape[1]):\n",
    "        Y_prediction[0,i] = 0 if A[0,i] <= 0.5 else 1\n",
    "    \n",
    "    assert(Y_prediction.shape == (1,m))\n",
    "    \n",
    "    return Y_prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5- Merge all functions into a model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations !! You have finished all the necessary components for constructing a model. Now, Let's take the challenge to merge all the implemented function into one model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(X_train, Y_train, X_test, Y_test, num_iterations, learning_rate, print_cost):\n",
    "    \"\"\"\n",
    "    Build the logistic regression model by calling all the functions you have implemented.\n",
    "    Arguments:\n",
    "    X_train - training set\n",
    "    Y_train - training label\n",
    "    X_test - test set\n",
    "    Y_test - test label\n",
    "    num_iteration - hyperparameter representing the number of iterations to optimize the parameters\n",
    "    learning_rate -- hyperparameter representing the learning rate used in the update rule of optimize()\n",
    "    print_cost -- Set to true to print the cost every 100 iterations\n",
    "    \n",
    "    Returns:\n",
    "    d - dictionary should contain following information w,b,training_accuracy, test_accuracy,cost\n",
    "    eg: d = {\"w\":w,\n",
    "             \"b\":b,\n",
    "             \"training_accuracy\": traing_accuracy,\n",
    "             \"test_accuracy\":test_accuracy,\n",
    "             \"cost\":cost}\n",
    "    \"\"\"\n",
    "#     # X标准化处理\n",
    "#     X_train /= X_train.max()\n",
    "#     X_test /= X_train.max()\n",
    "#     # XY参数格式调整\n",
    "#     X_train = X_train.T\n",
    "#     Y_train = np.expand_dims(Y_train, axis=0)\n",
    "#     X_test = X_test.T\n",
    "#     Y_test = np.expand_dims(Y_test, axis=0)\n",
    "    \n",
    "    # 参数初始化\n",
    "    w, b = initialize_parameters(X_train.shape[0])\n",
    "    \n",
    "    # 训练模型\n",
    "    params, grads, costs = optimize(w, b, X_train, Y_train, num_iterations, learning_rate, print_cost)\n",
    "    w = params['w']\n",
    "    b = params['b']\n",
    "            \n",
    "    # 训练集精度\n",
    "    Y_train_ = predict(w, b, X_train)\n",
    "    traing_accuracy = np.mean(Y_train_ == Y_train)\n",
    "           \n",
    "    # 测试集\n",
    "    Y_pre = predict(w, b, X_test)\n",
    "    test_accuracy = np.mean(Y_pre == Y_test)\n",
    "    \n",
    "    d = {\"w\":w,\n",
    "         \"b\":b,\n",
    "         \"training_accuracy\": traing_accuracy,\n",
    "         \"test_accuracy\":test_accuracy,\n",
    "         \"cost\":costs}\n",
    "    \n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获取数据\n",
    "X_train, X_test, y_train, y_test = train_test_split(digits.data, digits.target, test_size=0.25)\n",
    "y_train[y_train < 5 ] = 0\n",
    "y_train[y_train >= 5] = 1\n",
    "y_test[y_test < 5] = 0\n",
    "y_test[y_test >= 5] = 1\n",
    "\n",
    "# 数据预处理\n",
    "# X标准化处理\n",
    "X_train /= X_train.max()\n",
    "X_test /= X_train.max()\n",
    "# XY参数格式调整\n",
    "X_train = X_train.T\n",
    "y_train = np.expand_dims(y_train, axis=0)\n",
    "X_test = X_test.T\n",
    "y_test = np.expand_dims(y_test, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: 2.172324\n",
      "Cost after iteration 100: 0.275033\n",
      "Cost after iteration 200: 0.256566\n",
      "Cost after iteration 300: 0.250484\n",
      "Cost after iteration 400: 0.247739\n",
      "Cost after iteration 500: 0.246269\n",
      "Cost after iteration 600: 0.245389\n",
      "Cost after iteration 700: 0.244817\n",
      "Cost after iteration 800: 0.244422\n",
      "Cost after iteration 900: 0.244134\n",
      "Cost after iteration 1000: 0.243916\n",
      "Cost after iteration 1100: 0.243745\n",
      "Cost after iteration 1200: 0.243605\n",
      "Cost after iteration 1300: 0.243490\n",
      "Cost after iteration 1400: 0.243391\n",
      "Cost after iteration 1500: 0.243304\n",
      "Cost after iteration 1600: 0.243228\n",
      "Cost after iteration 1700: 0.243159\n",
      "Cost after iteration 1800: 0.243096\n",
      "Cost after iteration 1900: 0.243039\n",
      "Cost after iteration 2000: 0.242985\n",
      "Cost after iteration 2100: 0.242935\n",
      "Cost after iteration 2200: 0.242887\n",
      "Cost after iteration 2300: 0.242842\n",
      "Cost after iteration 2400: 0.242800\n",
      "Cost after iteration 2500: 0.242759\n",
      "Cost after iteration 2600: 0.242719\n",
      "Cost after iteration 2700: 0.242682\n",
      "Cost after iteration 2800: 0.242645\n",
      "Cost after iteration 2900: 0.242610\n",
      "Cost after iteration 3000: 0.242576\n",
      "Cost after iteration 3100: 0.242544\n",
      "Cost after iteration 3200: 0.242512\n",
      "Cost after iteration 3300: 0.242481\n",
      "Cost after iteration 3400: 0.242451\n",
      "Cost after iteration 3500: 0.242421\n",
      "Cost after iteration 3600: 0.242393\n",
      "Cost after iteration 3700: 0.242365\n",
      "Cost after iteration 3800: 0.242338\n",
      "Cost after iteration 3900: 0.242312\n",
      "Cost after iteration 4000: 0.242286\n",
      "Cost after iteration 4100: 0.242261\n",
      "Cost after iteration 4200: 0.242236\n",
      "Cost after iteration 4300: 0.242212\n",
      "Cost after iteration 4400: 0.242188\n",
      "Cost after iteration 4500: 0.242165\n",
      "Cost after iteration 4600: 0.242142\n",
      "Cost after iteration 4700: 0.242120\n",
      "Cost after iteration 4800: 0.242098\n",
      "Cost after iteration 4900: 0.242077\n",
      "{'w': array([[ 0.57595719],\n",
      "       [ 0.26309153],\n",
      "       [ 0.54008902],\n",
      "       [-1.02191641],\n",
      "       [ 1.20215359],\n",
      "       [ 0.89616639],\n",
      "       [ 3.29523305],\n",
      "       [ 2.57136886],\n",
      "       [ 2.29796625],\n",
      "       [-1.72704373],\n",
      "       [ 2.20124025],\n",
      "       [ 0.86360971],\n",
      "       [-0.6799734 ],\n",
      "       [-0.4112492 ],\n",
      "       [ 0.40378527],\n",
      "       [ 0.1245932 ],\n",
      "       [ 0.8650091 ],\n",
      "       [ 0.83734033],\n",
      "       [ 2.55142674],\n",
      "       [-0.4186212 ],\n",
      "       [-3.51388625],\n",
      "       [-0.65244945],\n",
      "       [-2.46247625],\n",
      "       [-2.49871614],\n",
      "       [ 0.20194609],\n",
      "       [-3.23519758],\n",
      "       [ 1.19812239],\n",
      "       [ 2.71352899],\n",
      "       [ 0.54994802],\n",
      "       [ 2.48497949],\n",
      "       [-2.45855854],\n",
      "       [ 0.54701856],\n",
      "       [ 0.06955039],\n",
      "       [-4.82537981],\n",
      "       [ 0.1999165 ],\n",
      "       [ 3.50153184],\n",
      "       [-2.02042703],\n",
      "       [-0.46928798],\n",
      "       [ 0.93056143],\n",
      "       [ 0.55220164],\n",
      "       [ 0.53891113],\n",
      "       [ 0.63425175],\n",
      "       [ 0.53897039],\n",
      "       [-0.55978091],\n",
      "       [ 2.33071185],\n",
      "       [ 1.10301843],\n",
      "       [ 0.95535169],\n",
      "       [ 1.58915327],\n",
      "       [ 0.53180175],\n",
      "       [-0.02282035],\n",
      "       [ 0.14249475],\n",
      "       [-0.74953428],\n",
      "       [-4.85627452],\n",
      "       [-0.61016713],\n",
      "       [ 1.96523784],\n",
      "       [-0.62461761],\n",
      "       [ 0.40395822],\n",
      "       [-1.9896227 ],\n",
      "       [ 0.33927856],\n",
      "       [-0.5268056 ],\n",
      "       [-2.50489312],\n",
      "       [-0.91243163],\n",
      "       [-1.7076443 ],\n",
      "       [-3.72041427]]), 'b': array([-0.55292797]), 'training_accuracy': 0.9161098737936154, 'test_accuracy': 0.8977777777777778, 'cost': 0.24205615693371565}\n"
     ]
    }
   ],
   "source": [
    "d = model(X_train, y_train, X_test, y_test, 5000, 1e-1, True)\n",
    "print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1., 0., 0., 0.,\n",
       "        1., 0., 0., 0., 1., 1., 0., 0., 0., 0., 1., 1., 1., 1., 0., 1.,\n",
       "        1., 0., 0., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0., 0., 0., 0.,\n",
       "        0., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0., 0., 1., 1., 0., 1.,\n",
       "        1., 0., 1., 0., 0., 0., 1., 0., 1., 0., 0., 0., 1., 1., 1., 0.,\n",
       "        0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 1., 0., 1.,\n",
       "        1., 1., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
       "        1., 1., 1., 0., 0., 1., 0., 1., 0., 1., 0., 1., 1., 1., 0., 1.,\n",
       "        0., 1., 0., 0., 1., 1., 1., 1., 0., 0., 0., 1., 1., 1., 1., 1.,\n",
       "        1., 0., 0., 0., 0., 1., 0., 1., 1., 0., 1., 0., 0., 1., 0., 0.,\n",
       "        1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 0., 0., 0., 1., 0., 1.,\n",
       "        1., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 1., 1., 1., 0., 0.,\n",
       "        0., 1., 0., 0., 1., 0., 1., 1., 1., 0., 1., 0., 0., 0., 1., 1.,\n",
       "        1., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0., 1., 1., 0., 0., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1., 1., 0.,\n",
       "        1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 0.,\n",
       "        1., 0., 1., 0., 0., 0., 0., 1., 0., 1., 1., 1., 0., 0., 0., 0.,\n",
       "        1., 0., 0., 0., 0., 1., 0., 1., 0., 1., 0., 1., 1., 1., 1., 0.,\n",
       "        1., 1., 1., 0., 1., 1., 0., 1., 0., 0., 0., 1., 0., 1., 0., 0.,\n",
       "        0., 0., 1., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 1., 1., 1.,\n",
       "        0., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 0., 1., 0., 0., 0.,\n",
       "        1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1., 1., 0.,\n",
       "        0., 0., 0., 1., 0., 1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 0., 1., 0., 1., 0., 0., 1., 0., 0., 0., 1., 1., 1., 0., 1.,\n",
       "        0., 1., 1., 0., 1., 0., 0., 1., 0., 1., 0., 1., 1., 0., 1., 0.,\n",
       "        0., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 0., 1., 0., 0.,\n",
       "        1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 1.,\n",
       "        0., 1., 1., 1., 1., 0., 0., 1., 0., 0., 1., 1., 0., 0., 0., 1.,\n",
       "        0., 1.]])"
      ]
     },
     "execution_count": 305,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pre = predict(d['w'], d['b'], X_test)\n",
    "y_pre"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.选做题"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations on building your first logistic regression model. It is your time to analyze it further."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.1 Observe the effect of learning rate on the leraning process.   \n",
    "Hits: plot the learning curve with different learning rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "学习率越大，学习进度越快。即同样的训练轮次，学习率大的loss值小。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0005\n",
      "0.001\n",
      "0.0015\n",
      "0.002\n",
      "0.0025\n",
      "0.003\n",
      "0.0035\n",
      "0.004\n",
      "0.0045000000000000005\n",
      "0.005000000000000001\n",
      "0.0055\n",
      "0.006\n",
      "0.006500000000000001\n",
      "0.007000000000000001\n",
      "0.0075\n",
      "0.008\n",
      "0.0085\n",
      "0.009000000000000001\n",
      "0.009500000000000001\n",
      "0.01\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lrs = np.linspace(5e-4, 1e-2, 20, endpoint=True)\n",
    "costs = []\n",
    "for i in lrs:\n",
    "    print(i)\n",
    "    d = model(X_train, y_train, X_test, y_test, 500, i, False)\n",
    "    costs.append(d['cost'])\n",
    "plt.plot(lrs, costs)\n",
    "plt.xlabel('Learning Rate')\n",
    "plt.ylabel('loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.2 Observe the effect of iteration_num on the test accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "训练轮次越多，正确率越高。\n",
    "曲线图中产生的抖动猜测是w参数随机初始化的影响。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "400\n",
      "600\n",
      "800\n",
      "1000\n",
      "1200\n",
      "1400\n",
      "1600\n",
      "1800\n",
      "2000\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "iters = np.linspace(200, 2000, 10, endpoint=True).astype(np.int)\n",
    "accs = []\n",
    "for i in iters:\n",
    "    print(i)\n",
    "    d = model(X_train, y_train, X_test, y_test, i, 1e-3, False)\n",
    "    accs.append(d['test_accuracy'])\n",
    "plt.plot(iters, accs)\n",
    "plt.xlabel('Iterration Num')\n",
    "plt.ylabel('Test Accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge ! ! !\n",
    "\n",
    "The original data have images labeled 0,1,2,3,4,5,6,7,8,9. In our logistic model, we only detect if the digit in the image is larger or smaller than 5. Now, Let's go for a more challenging problem. Try to use softmax function to build a model to recognize which digit (0,1,2,3,4,5,6,7,8,9) is in the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters_2(dim1, dim2):\n",
    "\n",
    "    w = np.random.random((dim1,dim2))\n",
    "    b = np.zeros((dim2, 1))\n",
    "\n",
    "    return w,b\n",
    "\n",
    "def softmax(z):\n",
    "    temp = np.exp(z)\n",
    "    return temp/np.sum(temp, axis=0)\n",
    "\n",
    "# 重写反向传播函数\n",
    "def propagate_2(w,b,X,Y):\n",
    "    '''\n",
    "    Implement the cost function and its gradient for the propagation\n",
    "    \n",
    "    Arguments:\n",
    "    w - weights\n",
    "    b - bias\n",
    "    X - data\n",
    "    Y - ground truth\n",
    "    '''\n",
    "    m = X.shape[1]\n",
    "    A = softmax(np.dot(w.T, X) + b)\n",
    "    temp = Y * np.log(A) + (1 - Y) * np.log(1 - A)\n",
    "\n",
    "    cost = -1 / m * np.sum(Y * np.log(A) + (1 - Y) * np.log(1 - A))\n",
    "    \n",
    "    dw = 1 / m * np.dot(X, (A - Y).T)\n",
    "    db = 1 / m * np.sum(A - Y, axis=(0,1))\n",
    "    \n",
    "    assert(dw.shape == w.shape)\n",
    "    assert(db.dtype == float)\n",
    "    cost = np.squeeze(cost)\n",
    "    assert(cost.shape == ())\n",
    "    \n",
    "    grads = {'dw':dw,\n",
    "             'db':db}\n",
    "    return grads, cost\n",
    "\n",
    "def optimize_2(w, b, X, Y, num_iterations, learning_rate, print_cost=False):\n",
    "    '''\n",
    "    This function optimize w and b by running a gradient descen algorithm\n",
    "    \n",
    "    Arguments:\n",
    "    w - weights\n",
    "    b - bias\n",
    "    X - data\n",
    "    Y - ground truth\n",
    "    num_iterations -- number of iterations of the optimization loop\n",
    "    learning_rate -- learning rate of the gradient descent update rule\n",
    "    print_cost -- True to print the loss every 100 steps\n",
    "    \n",
    "    Returns:\n",
    "    params - dictionary containing the weights w and bias b\n",
    "    grads -- dictionary containing the gradients of the weights and bias with respect to the cost function\n",
    "    costs -- list of all the costs computed during the optimization, this will be used to plot the learning curve.\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    batch_size = 50\n",
    "    for j in range(num_iterations):\n",
    "        costs = []\n",
    "        for i in range(0, X.shape[1], batch_size):\n",
    "            X_ = X[:,i:i+batch_size]\n",
    "            Y_ = Y[:,i:i+batch_size]\n",
    "\n",
    "            grads, cost = propagate_2(w,b,X_,Y_)\n",
    "\n",
    "            dw = grads['dw']\n",
    "            db = grads['db']\n",
    "\n",
    "            w -= learning_rate * dw\n",
    "            b -= learning_rate * db\n",
    "\n",
    "            costs.append(cost)\n",
    "        \n",
    "        cost = sum(costs)/len(costs)\n",
    "        if print_cost and j % 100 == 0:\n",
    "            print (\"Cost after iteration %i: %f\" %(j, cost))\n",
    "    \n",
    "    params = {\"w\":w,\n",
    "              \"b\":b}\n",
    "    \n",
    "    grads = {\"dw\":dw,\n",
    "             \"db\":db}\n",
    "     \n",
    "    return params, grads, cost\n",
    "\n",
    "def predict_2(w, b, X):\n",
    "    '''\n",
    "    Predict whether the label is 0 or 1 using learned logistic regression parameters (w, b)\n",
    "    \n",
    "    Arguments:\n",
    "    w -- weights\n",
    "    b -- bias \n",
    "    X -- data \n",
    "    \n",
    "    Returns:\n",
    "    Y_prediction -- a numpy array (vector) containing all predictions (0/1) for the examples in X\n",
    "    '''\n",
    "    m = X.shape[1]\n",
    "    Y_prediction = np.zeros((1,m))\n",
    "#     w = w.reshape(X.shape[0],1)\n",
    "    \n",
    "    A = softmax(np.dot(w.T, X) + b)\n",
    "    \n",
    "    Y_prediction[0] = A.argmax(axis=0)\n",
    "    \n",
    "    assert(Y_prediction.shape == (1,m))\n",
    "    \n",
    "    return Y_prediction\n",
    "\n",
    "def model_2(X_train, Y_train, X_test, Y_test, num_iterations, learning_rate, print_cost):\n",
    "    \"\"\"\n",
    "    Build the logistic regression model by calling all the functions you have implemented.\n",
    "    Arguments:\n",
    "    X_train - training set\n",
    "    Y_train - training label\n",
    "    X_test - test set\n",
    "    Y_test - test label\n",
    "    num_iteration - hyperparameter representing the number of iterations to optimize the parameters\n",
    "    learning_rate -- hyperparameter representing the learning rate used in the update rule of optimize()\n",
    "    print_cost -- Set to true to print the cost every 100 iterations\n",
    "    \n",
    "    Returns:\n",
    "    d - dictionary should contain following information w,b,training_accuracy, test_accuracy,cost\n",
    "    eg: d = {\"w\":w,\n",
    "             \"b\":b,\n",
    "             \"training_accuracy\": traing_accuracy,\n",
    "             \"test_accuracy\":test_accuracy,\n",
    "             \"cost\":cost}\n",
    "    \"\"\"\n",
    "    \n",
    "    # 参数初始化\n",
    "    w, b = initialize_parameters_2(X_train.shape[0], Y_train.shape[0])\n",
    "    \n",
    "    # 训练模型\n",
    "    params, grads, costs = optimize_2(w, b, X_train, Y_train, num_iterations, learning_rate, print_cost)\n",
    "    w = params['w']\n",
    "    b = params['b']\n",
    "            \n",
    "    # 训练集精度\n",
    "    Y_train_ = predict_2(w, b, X_train)\n",
    "    traing_accuracy = np.mean(Y_train_ == np.expand_dims(Y_train.argmax(axis=0), axis=0))\n",
    "           \n",
    "    # 测试集\n",
    "    Y_pre = predict_2(w, b, X_test)\n",
    "    test_accuracy = np.mean(Y_pre == np.expand_dims(Y_test.argmax(axis=0), axis=0))\n",
    "    \n",
    "    d = {\"w\":w,\n",
    "         \"b\":b,\n",
    "         \"training_accuracy\": traing_accuracy,\n",
    "         \"test_accuracy\":test_accuracy,\n",
    "         \"cost\":costs}\n",
    "    \n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获取数据\n",
    "X_train, X_test, y_train, y_test = train_test_split(digits.data, digits.target, test_size=0.25)\n",
    "\n",
    "# 数据预处理\n",
    "# X标准化处理\n",
    "X_train /= X_train.max()\n",
    "X_test /= X_train.max()\n",
    "# XY参数格式调整\n",
    "X_train = X_train.T\n",
    "y_train = np.eye(10)[y_train].T\n",
    "X_test = X_test.T\n",
    "y_test = np.eye(10)[y_test].T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: 3.729599\n",
      "Cost after iteration 100: 1.093362\n",
      "Cost after iteration 200: 0.713761\n",
      "Cost after iteration 300: 0.562693\n",
      "Cost after iteration 400: 0.478560\n",
      "Cost after iteration 500: 0.423736\n",
      "Cost after iteration 600: 0.384599\n",
      "Cost after iteration 700: 0.354932\n",
      "Cost after iteration 800: 0.331463\n",
      "Cost after iteration 900: 0.312299\n",
      "Cost after iteration 1000: 0.296262\n",
      "Cost after iteration 1100: 0.282577\n",
      "Cost after iteration 1200: 0.270713\n",
      "Cost after iteration 1300: 0.260292\n",
      "Cost after iteration 1400: 0.251038\n",
      "Cost after iteration 1500: 0.242743\n",
      "Cost after iteration 1600: 0.235247\n",
      "Cost after iteration 1700: 0.228426\n",
      "Cost after iteration 1800: 0.222182\n",
      "Cost after iteration 1900: 0.216434\n",
      "Cost after iteration 2000: 0.211117\n",
      "Cost after iteration 2100: 0.206178\n",
      "Cost after iteration 2200: 0.201572\n",
      "Cost after iteration 2300: 0.197262\n",
      "Cost after iteration 2400: 0.193215\n",
      "Cost after iteration 2500: 0.189405\n",
      "Cost after iteration 2600: 0.185808\n",
      "Cost after iteration 2700: 0.182403\n",
      "Cost after iteration 2800: 0.179174\n",
      "Cost after iteration 2900: 0.176105\n",
      "Cost after iteration 3000: 0.173182\n",
      "Cost after iteration 3100: 0.170394\n",
      "Cost after iteration 3200: 0.167729\n",
      "Cost after iteration 3300: 0.165179\n",
      "Cost after iteration 3400: 0.162735\n",
      "Cost after iteration 3500: 0.160389\n",
      "Cost after iteration 3600: 0.158135\n",
      "Cost after iteration 3700: 0.155966\n",
      "Cost after iteration 3800: 0.153877\n",
      "Cost after iteration 3900: 0.151863\n",
      "Cost after iteration 4000: 0.149919\n",
      "Cost after iteration 4100: 0.148042\n",
      "Cost after iteration 4200: 0.146226\n",
      "Cost after iteration 4300: 0.144470\n",
      "Cost after iteration 4400: 0.142769\n",
      "Cost after iteration 4500: 0.141120\n",
      "Cost after iteration 4600: 0.139521\n",
      "Cost after iteration 4700: 0.137970\n",
      "Cost after iteration 4800: 0.136463\n",
      "Cost after iteration 4900: 0.134999\n",
      "{'w': array([[ 8.52358514e-01,  1.48234142e-01,  9.78190246e-01,\n",
      "         2.27346963e-02,  5.53656002e-01,  3.89122829e-01,\n",
      "         4.83807434e-01,  2.54643196e-01,  6.13861425e-01,\n",
      "         2.37392209e-01],\n",
      "       [ 8.86934654e-01,  4.43849772e-01,  4.98263721e-01,\n",
      "         9.76929528e-01,  4.25749052e-01,  4.35078656e-01,\n",
      "         5.38268574e-01,  4.82954001e-01,  5.14589795e-01,\n",
      "         1.72971049e-01],\n",
      "       [ 4.52173153e-01, -1.50092191e-01,  1.31436119e+00,\n",
      "         3.81436059e-01,  2.52653783e-01,  2.61579467e+00,\n",
      "        -4.66946014e-01,  1.07844515e+00,  4.79596926e-02,\n",
      "         1.72455588e-01],\n",
      "       [ 1.18115493e+00,  5.43188807e-01,  5.17520788e-01,\n",
      "         1.58833356e+00, -1.49503143e+00,  5.07236077e-01,\n",
      "         7.45792307e-02,  1.49341597e+00, -4.62839994e-01,\n",
      "         1.58203036e+00],\n",
      "       [ 7.24764895e-01, -2.02466225e+00,  7.07612106e-01,\n",
      "         2.62694754e+00, -3.13415447e-01,  1.27852582e+00,\n",
      "         2.67012193e-01,  1.69523594e+00,  7.37386797e-01,\n",
      "         9.84956817e-01],\n",
      "       [-9.37638383e-01,  1.56870968e+00, -2.50549343e-02,\n",
      "         1.59153404e+00, -1.89773272e+00,  3.01961185e+00,\n",
      "         1.04374827e-02,  1.40955673e+00,  2.97353300e-01,\n",
      "        -2.34440393e-01],\n",
      "       [-2.57980434e-01,  4.92268575e-01, -1.64577236e-01,\n",
      "         8.30040402e-01, -4.14279263e-01,  2.00481750e+00,\n",
      "         2.33911939e-01,  1.60041015e+00, -6.58427175e-01,\n",
      "         5.92530975e-01],\n",
      "       [-2.30901165e-02,  8.37106257e-01,  8.27034766e-02,\n",
      "         8.57628932e-01,  4.22954589e-01,  3.45402292e-01,\n",
      "         3.99604179e-01,  9.91066140e-01,  1.20347872e-01,\n",
      "        -1.63501881e-01],\n",
      "       [ 3.94735546e-01,  2.09671815e-01,  6.50797117e-01,\n",
      "         1.05061558e-01,  5.58311247e-01,  1.93640156e-01,\n",
      "         6.76486034e-01,  7.67355769e-01,  4.06651018e-01,\n",
      "         3.55948515e-01],\n",
      "       [ 6.91261188e-01, -5.23431058e-01,  1.24468808e+00,\n",
      "         9.30515418e-01,  5.64863190e-01,  4.94893600e-01,\n",
      "        -2.33020115e-01,  5.17284595e-01,  3.97416250e-01,\n",
      "         1.45703519e+00],\n",
      "       [ 4.82833618e-01, -1.93444201e+00,  1.12376640e+00,\n",
      "         1.18231574e+00, -3.65087333e-01,  1.82247316e+00,\n",
      "        -7.76700751e-01,  9.59511917e-01,  1.39810498e+00,\n",
      "         1.46318415e+00],\n",
      "       [ 1.27459741e+00, -6.26323284e-02,  6.55452188e-01,\n",
      "         5.23923100e-01, -2.06496919e-01,  7.50321344e-01,\n",
      "         1.04278080e+00,  1.22416456e+00,  8.51890651e-01,\n",
      "         8.66096491e-01],\n",
      "       [ 4.26537439e-01,  1.40257357e+00,  1.99053065e+00,\n",
      "         7.56616794e-01, -7.77945359e-01,  2.97667205e-01,\n",
      "        -1.05238253e+00,  2.11692175e+00, -5.89499577e-01,\n",
      "         2.89254664e-01],\n",
      "       [ 1.63302103e+00,  6.32752967e-01,  3.35661430e-01,\n",
      "         1.56142016e+00, -5.22360825e-01, -4.96925066e-01,\n",
      "        -5.37079698e-01,  4.64436936e-01,  1.65203642e+00,\n",
      "         9.63576802e-01],\n",
      "       [ 1.17572013e-01, -4.63112879e-01,  9.90436349e-01,\n",
      "         8.55276203e-01,  2.46331970e-01, -2.12501123e-01,\n",
      "         1.66593477e-01,  1.08792743e+00,  1.25797554e+00,\n",
      "         1.35609896e+00],\n",
      "       [ 3.50476937e-01,  4.58466063e-01,  2.56565744e-01,\n",
      "         5.58537849e-01,  5.07203917e-01,  2.74742775e-01,\n",
      "         7.74904714e-01,  9.39300920e-01,  8.03601528e-01,\n",
      "         5.66321089e-01],\n",
      "       [ 8.95767599e-01,  6.62085498e-01,  7.21821202e-01,\n",
      "         1.06124934e-01,  4.34353459e-01,  8.49946720e-01,\n",
      "         7.81889601e-01,  6.26608433e-02,  2.99652331e-01,\n",
      "         2.98722145e-02],\n",
      "       [ 7.51453173e-01,  6.90824690e-01,  1.00716286e+00,\n",
      "         6.47015795e-01,  8.53494292e-01,  8.41016418e-01,\n",
      "        -3.37775491e-01, -7.31442212e-01,  1.28809532e+00,\n",
      "         5.41197964e-01],\n",
      "       [ 9.77872213e-01,  5.79102585e-01, -5.20282444e-01,\n",
      "        -2.12285986e+00,  1.46666890e+00,  1.89137071e+00,\n",
      "         1.12419786e+00, -1.31179072e+00,  8.69861064e-01,\n",
      "         1.41503391e+00],\n",
      "       [ 3.28541372e-01,  3.92840485e+00, -4.85612123e-01,\n",
      "        -1.26896071e+00,  1.76503496e+00,  3.70702801e-01,\n",
      "         7.87961453e-01, -5.52819137e-01,  2.33733958e-01,\n",
      "         4.18838784e-01],\n",
      "       [-6.69078940e-01,  3.43810138e+00,  1.03642747e+00,\n",
      "         1.83825448e+00,  1.10664730e+00, -1.61854545e+00,\n",
      "        -1.13453641e+00,  1.06749554e+00,  3.20237501e-01,\n",
      "         9.61361574e-01],\n",
      "       [ 2.00844862e+00,  4.18232288e-01,  2.11768909e-01,\n",
      "        -5.60569256e-01,  5.64475082e-01, -2.16508260e+00,\n",
      "        -1.88736379e+00,  5.94010499e-01,  1.37465028e+00,\n",
      "         3.32204596e+00],\n",
      "       [ 6.67843563e-01, -4.76759552e-01,  7.24663163e-01,\n",
      "         6.07929632e-01,  9.73391748e-01, -1.32350790e+00,\n",
      "         3.64797540e-01,  1.57912057e+00,  9.69416988e-01,\n",
      "         9.20878159e-01],\n",
      "       [ 6.49966227e-01,  2.40109544e-01,  5.92204048e-02,\n",
      "         4.42905029e-01,  2.21061543e-01,  6.48721813e-01,\n",
      "         3.73523848e-01,  4.65250241e-01, -6.64870566e-04,\n",
      "         7.69124226e-01],\n",
      "       [ 3.13322636e-01,  3.13716667e-01,  5.85409956e-01,\n",
      "         4.33666740e-01,  3.75290150e-01,  4.28592379e-02,\n",
      "         5.95454855e-01,  8.75930110e-02,  4.02010508e-01,\n",
      "         5.83382960e-01],\n",
      "       [ 1.03140566e+00,  4.98276795e-01, -5.47734310e-01,\n",
      "        -2.67103760e-01,  1.29708261e+00,  9.24840268e-01,\n",
      "         9.05337440e-01, -4.34822478e-01, -7.95238804e-01,\n",
      "         3.94031824e-01],\n",
      "       [ 1.57710633e+00,  1.08787869e+00, -2.20614016e+00,\n",
      "        -1.91264763e+00,  2.48735508e+00,  1.82584481e+00,\n",
      "         8.38019601e-01, -1.14812704e-01, -1.22905248e-01,\n",
      "         1.46661662e+00],\n",
      "       [-7.90449442e-01,  1.79141236e+00, -2.21974278e+00,\n",
      "        -8.54979171e-02,  4.17904934e-01,  1.21590456e+00,\n",
      "         9.30423165e-01, -6.75428926e-01,  2.58299099e+00,\n",
      "         2.63142788e+00],\n",
      "       [-2.83105503e+00,  6.43383827e-01, -1.89971139e-01,\n",
      "         1.80043394e+00,  9.95787581e-01,  6.08366544e-01,\n",
      "        -4.57714744e-01,  3.33067301e-01,  6.55664689e-01,\n",
      "         1.77333547e+00],\n",
      "       [ 6.38170045e-01,  5.20852259e-01,  2.69425298e-01,\n",
      "        -1.87762280e+00,  5.54664303e-01, -2.03597680e-01,\n",
      "        -5.60409096e-02,  1.47731640e+00,  6.27026434e-02,\n",
      "         2.29529051e+00],\n",
      "       [ 1.39360439e+00,  4.50192549e-01,  7.08461902e-01,\n",
      "        -8.32475627e-01,  2.50049040e+00, -1.09490048e+00,\n",
      "        -4.09188338e-01,  1.50722801e+00, -1.62828662e-01,\n",
      "         8.86308319e-01],\n",
      "       [ 5.53097140e-01,  1.96554222e-02,  1.18928579e-01,\n",
      "         6.94915820e-02,  8.53346345e-01,  9.16743189e-01,\n",
      "         7.03951185e-01,  5.70985572e-02,  1.97226758e-01,\n",
      "         7.17804079e-01],\n",
      "       [ 1.51208487e-01,  4.16080397e-01,  6.69488829e-01,\n",
      "         2.39303939e-01,  1.66080743e-01,  7.29161696e-01,\n",
      "         2.65655150e-01,  7.48950170e-01,  2.15030010e-01,\n",
      "         3.31370999e-01],\n",
      "       [ 1.11585210e+00,  2.09758453e-01, -3.67408511e-01,\n",
      "         2.74826487e-01,  2.21689349e+00,  4.33825992e-01,\n",
      "         1.41047764e+00,  1.20208191e+00, -3.22903178e-01,\n",
      "        -1.12794915e+00],\n",
      "       [ 1.70784678e+00,  7.24661196e-01, -1.55168947e+00,\n",
      "        -9.16767111e-01,  1.95746544e+00,  1.08226467e+00,\n",
      "         1.73883287e+00,  6.50362189e-01, -1.66756443e-01,\n",
      "         8.87428434e-02],\n",
      "       [-1.09175391e+00,  2.78047817e-01, -4.23875623e-01,\n",
      "         1.98429831e-01,  2.60104446e-01, -3.82353711e-01,\n",
      "         1.77447436e+00,  7.55319790e-01,  2.19123673e+00,\n",
      "         9.17718757e-01],\n",
      "       [-1.95583005e+00,  1.08077540e+00,  8.02837401e-01,\n",
      "         2.41134488e+00,  1.76900914e+00, -5.80339936e-01,\n",
      "         7.48540364e-01,  1.79756544e+00,  1.15178074e+00,\n",
      "        -1.08309588e+00],\n",
      "       [ 7.84894902e-02,  6.59827488e-02, -9.83614007e-01,\n",
      "         4.24356179e-01,  1.63950617e+00,  8.68955422e-01,\n",
      "         2.44602660e-01,  2.26894576e+00, -1.02217881e+00,\n",
      "         2.70925841e-02],\n",
      "       [ 1.02047146e+00, -2.12730937e-01, -5.29500000e-01,\n",
      "         3.03710551e-01,  2.06477191e+00,  1.19958694e+00,\n",
      "         3.48528647e-01,  1.57301453e+00, -1.04622561e+00,\n",
      "         4.60239205e-01],\n",
      "       [ 1.75615251e-02,  2.26304375e-01,  3.00840785e-01,\n",
      "         1.03622640e-01,  9.50471656e-01,  2.16810549e-01,\n",
      "         2.56912640e-01,  7.14192258e-01,  4.05992640e-01,\n",
      "         1.97640782e-01],\n",
      "       [ 9.53740594e-01,  4.94979486e-02,  3.42523861e-01,\n",
      "         5.88476916e-01,  2.26783512e-01,  1.73149100e-01,\n",
      "         2.67268147e-01,  1.47818560e-01,  8.97015928e-01,\n",
      "         4.44007684e-01],\n",
      "       [ 5.25558602e-01, -5.94299035e-01,  8.73847435e-01,\n",
      "         7.36215376e-02,  2.94875105e+00, -1.64558660e-01,\n",
      "         4.08713062e-01,  7.50574840e-01,  7.27533371e-03,\n",
      "         4.04744395e-01],\n",
      "       [ 1.99947230e+00, -8.25157822e-03,  8.88624896e-01,\n",
      "        -1.21019962e+00,  3.96310715e-01, -1.31626254e+00,\n",
      "         2.77937495e+00,  5.89142541e-01,  2.34804141e+00,\n",
      "        -2.02811951e+00],\n",
      "       [-4.04566000e-01,  1.10820606e+00,  2.68784195e+00,\n",
      "        -1.96447446e+00,  2.85891237e+00, -9.40878176e-01,\n",
      "         1.62372425e+00,  1.82192733e+00,  1.75379364e+00,\n",
      "        -2.73908701e+00],\n",
      "       [-4.77869285e-01,  1.47177014e+00, -2.68387282e-01,\n",
      "         7.64081862e-01,  2.76513943e+00, -3.85375061e-02,\n",
      "         8.21776683e-01,  7.02573948e-01,  1.08103276e+00,\n",
      "        -9.34469382e-01],\n",
      "       [ 8.92130664e-01, -8.24562032e-01, -1.87962794e+00,\n",
      "         2.14112590e+00,  1.28126074e+00,  1.04405540e+00,\n",
      "         1.03100687e+00,  8.46049224e-01,  1.60812635e+00,\n",
      "        -4.93740117e-02],\n",
      "       [ 9.07940191e-01, -1.52593908e+00, -6.67168961e-01,\n",
      "         2.26291194e+00,  3.22471211e-01,  8.65828131e-01,\n",
      "         2.05692569e+00, -4.97914473e-02,  1.11019321e-01,\n",
      "         3.38029793e-01],\n",
      "       [ 4.34181116e-01,  8.80976447e-01,  6.09312954e-01,\n",
      "         4.42883491e-01,  1.91560486e-01,  9.49078650e-01,\n",
      "         9.36413121e-01,  9.53582131e-01,  5.63275046e-01,\n",
      "         3.52743938e-01],\n",
      "       [ 6.32723735e-01,  6.51179228e-01,  5.96572947e-01,\n",
      "         4.30148512e-01,  4.43935046e-01,  2.34997589e-01,\n",
      "         2.61057501e-03,  3.04012001e-01,  9.85183171e-01,\n",
      "         9.38914615e-02],\n",
      "       [ 2.62713265e-01, -7.06585589e-02,  8.15122947e-01,\n",
      "         5.80273747e-01,  1.05349447e+00,  1.47137344e-01,\n",
      "         2.25207452e-01,  4.68862689e-01,  4.33452496e-02,\n",
      "         8.93867925e-01],\n",
      "       [ 1.23775515e+00, -3.25482162e-01,  1.46008738e+00,\n",
      "         4.89250131e-01, -4.17713308e-01,  1.96176137e-03,\n",
      "         1.36220909e+00,  8.01916505e-02,  1.42890879e+00,\n",
      "        -5.53900239e-01],\n",
      "       [ 6.91210520e-01,  8.83380416e-01,  2.93243727e+00,\n",
      "        -3.65184433e-01,  1.25733357e+00, -3.81825853e-01,\n",
      "         1.53632076e+00,  1.01977361e+00, -1.27224038e+00,\n",
      "        -8.10315127e-01],\n",
      "       [ 1.08732616e+00,  1.89049346e+00,  2.15065785e+00,\n",
      "         4.89250243e-01,  8.75405705e-01,  1.11821902e+00,\n",
      "        -3.70389694e-01, -8.58515459e-01, -1.00814579e+00,\n",
      "        -4.75566058e-01],\n",
      "       [ 1.53269287e+00,  1.46830908e-01,  2.02061784e+00,\n",
      "         1.06957100e+00, -6.81719062e-01,  8.64431685e-01,\n",
      "         1.92756540e+00, -1.79504586e+00,  1.60881417e+00,\n",
      "        -2.63756849e-01],\n",
      "       [-8.81666301e-02, -1.45246114e-01,  1.31869659e+00,\n",
      "         2.19106426e+00, -5.23767261e-01, -5.28214844e-02,\n",
      "         1.82038716e+00, -1.84628589e-01,  4.98113615e-01,\n",
      "         6.04116458e-01],\n",
      "       [ 7.88279484e-01,  1.14224880e+00,  3.14518620e-01,\n",
      "         1.03174022e-01,  5.69578459e-01,  3.59430292e-01,\n",
      "        -2.75682760e-02,  8.93319121e-01,  6.15081041e-01,\n",
      "         1.72517848e-01],\n",
      "       [ 1.44462440e-01,  8.53687962e-01,  4.75805365e-01,\n",
      "         7.12813917e-01,  8.41892483e-01,  2.07776754e-01,\n",
      "         7.92975682e-01,  3.00895983e-02,  1.29307754e-01,\n",
      "         9.52944415e-01],\n",
      "       [ 9.02815863e-02,  6.09483933e-01,  5.62457009e-01,\n",
      "         6.65355143e-01,  9.22584809e-01,  8.69542534e-01,\n",
      "        -1.08175955e-01,  4.06682568e-01, -2.30094052e-01,\n",
      "         2.14179621e-01],\n",
      "       [-8.27556757e-02, -6.23278326e-01,  1.42010093e+00,\n",
      "         1.68365453e+00,  9.76746244e-02,  2.34525100e+00,\n",
      "        -6.25017894e-01,  8.46135209e-01, -8.64389825e-01,\n",
      "         4.25224911e-01],\n",
      "       [ 1.50798416e+00, -1.31511809e-01,  9.26153003e-01,\n",
      "         1.19520389e+00, -8.66637599e-01,  1.34169145e+00,\n",
      "         1.04534132e-01, -2.48792979e-01,  9.41849348e-01,\n",
      "         5.37054073e-01],\n",
      "       [ 2.50810614e-01,  1.11098646e+00,  8.47241562e-01,\n",
      "         7.76060448e-01, -2.12827699e-01,  7.04934259e-01,\n",
      "         1.91075986e-01, -2.45123682e+00,  1.59417664e+00,\n",
      "         6.53099694e-01],\n",
      "       [ 9.72280476e-02,  1.56878488e+00,  2.19613395e+00,\n",
      "         7.03602800e-01, -8.93239674e-01, -4.18476952e-01,\n",
      "         2.01118491e+00, -1.09908542e+00,  2.88701247e-01,\n",
      "         1.08503239e-01],\n",
      "       [-1.66765446e-01,  8.43904738e-01,  2.18716477e+00,\n",
      "        -1.02326875e-01, -1.67300890e-03, -2.39533502e-01,\n",
      "         7.45421064e-02, -1.41900689e-02, -4.79242807e-01,\n",
      "         1.89289117e-01],\n",
      "       [ 4.13955549e-01,  1.75045808e+00,  7.68942807e-01,\n",
      "        -1.82283928e-01,  2.49737929e-01, -2.05435250e-01,\n",
      "        -6.37096439e-02,  2.63541737e-01,  9.61245949e-02,\n",
      "         3.63552217e-01]]), 'b': array([[9.69856287e-17],\n",
      "       [9.69856287e-17],\n",
      "       [9.69856287e-17],\n",
      "       [9.69856287e-17],\n",
      "       [9.69856287e-17],\n",
      "       [9.69856287e-17],\n",
      "       [9.69856287e-17],\n",
      "       [9.69856287e-17],\n",
      "       [9.69856287e-17],\n",
      "       [9.69856287e-17]]), 'training_accuracy': 0.9866369710467706, 'test_accuracy': 0.9733333333333334, 'cost': 0.1335900380746724}\n"
     ]
    }
   ],
   "source": [
    "d = model_2(X_train, y_train, X_test, y_test, 5000, 1e-2, True)\n",
    "print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 1., 0.],\n",
       "       [1., 0., 0.]])"
      ]
     },
     "execution_count": 275,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# one-hot编码 \n",
    "a = np.asarray([0,1,2,1,0])\n",
    "b = np.eye(3)[a]\n",
    "b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Congratulations ! You have completed assigment 4. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
