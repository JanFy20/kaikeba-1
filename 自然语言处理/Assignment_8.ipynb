{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.复习课上内容， 阅读相应论文。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 回答以下理论题目"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.  What is autoencoder?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Autoencoder 是一种利用神经网络自动训练 encoder 和 decoder 的算法，用于数据压缩和特征抽取。input参数进入encoder神经网络，输出较小维度的数据code，然后这个code作为输入，进入decoder神经网络，得到输出output。训练时，loss算法设计要满足input与output越相似，loss越小。训练完成后，input经过encoder后得到的code就是input数据的特征或者说是压缩值，这个code经过decoder后就可以解压缩为原数据。   \n",
    "在NLP领域，上面的encoder和decoder神经网络就用RNN。如果input和output用相同含义的两种不同语言，那训练出来的模型，从input到outpu就是一个翻译模型。如果input是问题，output是回答，那训练出来就是问答系统。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. What are the differences between greedy search and beam search?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在Sequence to Sequence的RNN模型做推理时，greedy search算法在每个RNN节点直接取最大的output；而beam search为了避免由于偶然的误差错过最优解，对于每个RNN节点都保留k个最优解，然后计算句子的概率后，选择前k个最大概率的句子对应的当前output继续向后计算，这样就有更大可能得到最优解。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. What is the intuition of attention mechanism?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在神经网络中，对于输入参数到输出，不是所有的输入参数都是有相同价值的，网络会选择它关心的那部分来生成对应的输出，就是说对于输入部分，神经网络在工作时会包含输入参数所对应的权重，或者叫注意力。对于分类、回归之类的单个输出的网络，注意力已经隐含在网络权重之中了。对于NLP的sequence类型输出的网络，由于网络权重对每个输出都是相同的，没有体现出每个输出对于相同输入该有不同的注意力，所以另行计算出相应的注意力权重。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. What is the disadvantage of word embeding introduced in previous lectures ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "词嵌入模型无法解决 一词多义的问题。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. What is the architecture of ELMo model. (A brief description is enough)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ELMo是双层双向的LSTM 模型。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Compared to RNN,  what is the advantage of Transformer ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformer在encoder阶段是可以一句话所有单词同时输入，并行处理的；RNN只能一个单词一个单词串行处理。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7. Why we use layer normalizaiton instead of batch normalization in Transformer ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Batch nromalizaton是同一批次样本对于相同位置参数的归一化，一则这个归一化受样本的影响，二则也没有什么实际的意义。Layer normalization是在一个样本中对所有单词的相同维度进行的归一化。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8. Why we need position embedding in Transformer ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "句子中的词语，如果顺序不同，则表达不同的含义。RNN在encoding时是按顺序输入的，已经包含了词语顺序信息；但是Transfo是整句输入并行处理的，单词的顺序体现地不明显，所以需要设计position embedding在参数中包含词语的位置信息，能更好地表达意思。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9. Briefly describe what is self-attention and what is multi-head attention?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "self-attention计算了某个单词与这句话中其它词的相关关系。一组权重参数（WQ、WK、WV），与输入的单词A计算得到Q、K、V。 某个词与这个句子中的其它词的相关关系Z=softmax(QK)V。multi-head attention就是多组这样的self-attention。每一组self-attention可以从不同的角度去计算词之间的相关关系。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 10. What is the basic unit of GPT model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GPT的基本单元就是Transformer的decoder的一层。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 11. Briefly descibe how to use GPT in other NLP tasks?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GPT可以用在多种任务。  \n",
    "分类：输入文本，产生分类结果。    \n",
    "判断推理正误：输入前提与结论，判断正误（即前提是否能推理得到结论）   \n",
    "相似性：输入两个句子，判断句子是否相似。   \n",
    "多选题：输入多个问题和答案组，得出概率最大的那个。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 12. What is masked language model in BERT ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Masked language model类似完形填空，所有的位置信息都在，但是某些需要预测的词已经用mask替代。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 13. What are the inputs of BERT ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BERT的输入参数是多个句子的编码向量。编码向量是token embedding、position embedding、segment embedding三部分的和。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 14. Briely descibe how to use BERT in other NLP tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1、用作完形填空，预测空缺的词。  \n",
    "2、判断两个句子是否连续。  \n",
    "3、用于文本分类，比如情感分类。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 15. What are the differences between these three models: GPT, BERT, GPT2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GPT使用了Transformer的decoder部分。  \n",
    "BERT是使用了Transformer的encoder部分，并做了改造。  \n",
    "GPT2是在GPT的基础上加大了规模。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
