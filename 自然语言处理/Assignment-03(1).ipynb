{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment-03 First Step of Machine Learning: Model and Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "同学们，今天我们的学习了基本的机器学习概念，相比你已经对机器学习的这些方法有一个基本的认识了。值得说明的是，机器学习不仅仅是一系列方法，更重要的是一种思维体系，即：依据以往的、现有的数据，构建某种方法来解决未见过的问题。而且决策树，贝叶斯只是实现这个目标的一个方法，包括之后的神经网络。很有可能有一天，神经网络也会被淘汰，但是重要的是我们要理解机器学习的目标，就是尽可能的自动化解决未知的问题。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://timgsa.baidu.com/timg?image&quality=80&size=b9999_10000&sec=1571556399207&di=4a97dc15ad08dd49d3748d1edf6109b3&imgtype=0&src=http%3A%2F%2Fc.hiphotos.baidu.com%2Fzhidao%2Fwh%3D450%2C600%2Fsign%3Dae742c6aedcd7b89e93932873a146e91%2F5d6034a85edf8db1b16050c40223dd54574e74c7.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part-1 Programming Review 编程回顾"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Re-code the Linear-Regression Model using scikit-learning(10 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<评阅点>： \n",
    "> + 是否完成线性回归模型 (4')\n",
    "+ 能够进行预测新数据(3')\n",
    "+ 能够进行可视化操作(3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19] [ 8.67362789 15.53446991 31.79948374 43.58219088 38.07454943 50.18448821\n",
      " 53.43912045 57.85501574 61.99059886 47.0551468  56.45313544 66.18748588\n",
      " 65.18161833 60.03006339 74.41858714 58.37752359 71.36636617 90.13924183\n",
      " 73.07065559 78.07259955]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x1e8a750a160>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# you code here\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "X = np.asarray(range(20))\n",
    "y = 3.4 * X + 5.6 + np.random.random(20) * 30\n",
    "print(X,y)\n",
    "plt.scatter(X,y,color='r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "reg = LinearRegression().fit(X.reshape(-1,1),y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8008978080285423"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg.score(X.reshape(-1, 1), y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3.06705574])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25.93726888552938"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x): \n",
    "    return reg.coef_ * x + reg.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1e8a8756748>]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "xx = np.asarray([21,22,23])\n",
    "yy = reg.predict(xx.reshape(-1,1))\n",
    "plt.scatter(X, y, color='b')\n",
    "plt.scatter(xx,yy,color='y')\n",
    "plt.plot(X, f(X), color='red')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Complete the unfinished KNN Model using pure python to solve the previous Line-Regression problem. (8 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<评阅点>:\n",
    "> + 是否完成了KNN模型 (4')\n",
    "+ 是否能够预测新的数据 (4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you code here\n",
    "from scipy.spatial.distance import cosine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(X, y):\n",
    "    # 直接存储 X,y 即可\n",
    "    return [(Xi, yi) for Xi, yi in zip(X, y)]\n",
    "\n",
    "def distance(x1, x2):\n",
    "    return cosine(x1, x2)\n",
    "\n",
    "def predict(x, k=5):\n",
    "    # 在predicate的时候，需要做大量的计算\n",
    "#     most_similars = sorted(model(X, y), key=lambda xi: distance(xi[0], x))[:k]\n",
    "    most_similars = sorted(model(X, y), key=lambda xi: abs(xi[0] - x))[:k]\n",
    "    \n",
    "    y_hats = [_y for x, _y in most_similars]\n",
    "    \n",
    "    print(most_similars)\n",
    "    \n",
    "    return np.mean(y_hats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "myself_knn = model(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(3, 43.5821908849128), (4, 38.074549425765596), (2, 31.79948374015893), (5, 50.184488207113844), (1, 15.534469905730287)]\n",
      "[(12, 65.18161832985098), (13, 60.03006338956298), (11, 66.1874858788953), (14, 74.41858713622187), (10, 56.453135439466976)]\n",
      "[(18, 73.07065559165645), (17, 90.13924182774194), (19, 78.07259955137184), (16, 71.36636617232826), (15, 58.37752358782443)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x1e8a8771940>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "xx = np.asarray([3.5,  12.5, 17.9])\n",
    "yy = [predict(x) for x in xx]\n",
    "plt.scatter(X, y, color='b')\n",
    "plt.scatter(xx, yy,color='y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine(1, 0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Re-code the Decision Tree, which could sort the features by salience. (12 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<评阅点>\n",
    "> + 是否实现了信息熵 (1' )\n",
    "+ 是否实现了最优先特征点的选择(5')\n",
    "+ 是否实现了持续的特征选则(6')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you code here\n",
    "mock_data = {\n",
    "    'gender':['F', 'F', 'F', 'F', 'M', 'M', 'M'],\n",
    "    'income': ['+10', '-10', '+10', '+10', '+10', '+10', '-10'],\n",
    "    'family_number': [1, 1, 2, 1, 1, 1, 2],\n",
    "   # 'pet': [1, 1, 1, 0, 0, 0, 1],\n",
    "    'bought': [1, 1, 1, 0, 0, 0, 1],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gender</th>\n",
       "      <th>income</th>\n",
       "      <th>family_number</th>\n",
       "      <th>bought</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>F</td>\n",
       "      <td>+10</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>F</td>\n",
       "      <td>-10</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>F</td>\n",
       "      <td>+10</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>F</td>\n",
       "      <td>+10</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>M</td>\n",
       "      <td>+10</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>M</td>\n",
       "      <td>+10</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>M</td>\n",
       "      <td>-10</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  gender income  family_number  bought\n",
       "0      F    +10              1       1\n",
       "1      F    -10              1       1\n",
       "2      F    +10              2       1\n",
       "3      F    +10              1       0\n",
       "4      M    +10              1       0\n",
       "5      M    +10              1       0\n",
       "6      M    -10              2       1"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "dataset = pd.DataFrame.from_dict(mock_data)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from icecream import ic\n",
    "def entropy(elements):\n",
    "    '''群体的混乱程度'''\n",
    "    counter = Counter(elements)\n",
    "    probs = [counter[c] / len(elements) for c in set(elements)]\n",
    "#     ic(probs)\n",
    "    return - sum(p * np.log(p) for p in probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| probs: [1.0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-0.0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# entropy([1,1,0])\n",
    "entropy([0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_the_optimal_spliter(training_data: pd.DataFrame, target: str) -> str:\n",
    "    # 用于分类的列名\n",
    "    x_fields = set(training_data.columns.tolist()) - {target}\n",
    "    \n",
    "    spliter = None\n",
    "    # 初始化为最大值\n",
    "    min_entropy = float('inf')\n",
    "    \n",
    "    for f in x_fields:\n",
    "#         ic(f)\n",
    "        values = set(training_data[f])\n",
    "#         ic(values)\n",
    "        for v in values:\n",
    "            # f列值为v的标签值\n",
    "            sub_spliter_1 = training_data[training_data[f] == v][target].tolist()\n",
    "#             ic(sub_spliter_1)\n",
    "            # split by the current feature and one value\n",
    "            \n",
    "            entropy_1 = entropy(sub_spliter_1)\n",
    "#             ic(entropy_1)\n",
    "            # f列值不为v的标签值\n",
    "            sub_spliter_2 = training_data[training_data[f] != v][target].tolist()\n",
    "#             ic(sub_spliter_2)\n",
    "            \n",
    "            entropy_2 = entropy(sub_spliter_2)\n",
    "#             ic(entropy_2)\n",
    "            # 计算以f列的v值分类后的熵\n",
    "            entropy_v = entropy_1 + entropy_2\n",
    "#             ic(entropy_v)\n",
    "            # 保留最小熵以及对应的列名和值\n",
    "            if entropy_v <= min_entropy:\n",
    "                min_entropy = entropy_v\n",
    "                spliter = (f, v)\n",
    "    \n",
    "#     print('spliter is: {}'.format(spliter))\n",
    "#     print('the min entropy is: {}'.format(min_entropy))\n",
    "    \n",
    "    return spliter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_optimal_spliter(training_data: pd.DataFrame, target: str):\n",
    "    if entropy(training_data[target]) == 0 or len(training_data.columns.tolist()) == 1:\n",
    "        # 熵为0无需再分割，或列为1无法再分割\n",
    "        print('  ' * (4-len(training_data.columns)) + str(training_data[target].tolist()))\n",
    "        return\n",
    "    f,v = find_the_optimal_spliter(training_data, target)\n",
    "    dataset1 = training_data[training_data[f] == v]\n",
    "    print('  ' * (4-len(training_data.columns)) + '%s=%s'%(f,v))\n",
    "    get_optimal_spliter(dataset1.drop([f], axis=1), target)\n",
    "    dataset2 = training_data[training_data[f] != v]\n",
    "    print('  ' * (4-len(training_data.columns)) + '%s!=%s'%(f,v))\n",
    "    get_optimal_spliter(dataset2.drop([f], axis=1), target)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "family_number=2\n",
      "  [1, 1]\n",
      "family_number!=2\n",
      "  income=-10\n",
      "    [1]\n",
      "  income!=-10\n",
      "    gender=M\n",
      "      [0, 0]\n",
      "    gender!=M\n",
      "      [1, 0]\n"
     ]
    }
   ],
   "source": [
    "get_optimal_spliter(dataset, 'bought')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Finish the K-Means using 2-D matplotlib (8 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<评阅点>\n",
    "> + 是否完成了KMeans模型，基于scikit-learning (3')\n",
    "+ 是否完成了可视化任务（5'）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x1e8aa97ea58>"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD8CAYAAAB5Pm/hAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAAGqFJREFUeJzt3X+MZlV9x/H3191VB40ZkFVhYN01Jaip1aUTi25r6CIBwcrGmlRj6zax2T9qW1CDjukf1j9axmj8lRgM9Rc2BqhAFiJGSnYhxk2lznZR0ZUulQI7rLIWFo1u6rJ++8dzh50dnmeeH/fH+fV5JZuZ586dec695z7fPefc7znX3B0REcnXs0IXQERE2qVALyKSOQV6EZHMKdCLiGROgV5EJHMK9CIimVOgFxHJnAK9iEjmFOhFRDK3NnQBAE4//XTfuHFj6GKIiCRl7969P3f39cP2iyLQb9y4kYWFhdDFEBFJipk9NMp+GroREcmcAr2ISOYU6EVEMjc00JvZF83sMTO7b9m208zsTjM7UH09tdpuZvYZM3vAzL5vZue1WXgRERlulBb9l4FLVmybA3a5+znAruo1wJuAc6p/O4BrmimmiIhMamjWjbt/y8w2rth8OXBB9f11wN3AB6vtX/He00y+Y2bTZnaGux9qqsASr537FvnYHffz6JGjnDk9xVUXn8u2zTMDt4cqj3RL9RDepOmVL14K3u5+yMxeVG2fAR5Ztt/BatszAr2Z7aDX6mfDhg0TFkNisXPfIh+65QccPXYcgMUjR/nQLT9g4aHHuXnv4jO2A61+2AeVp+33lZOpHuLQ9M1Y67Ot77MK3f1ad59199n164fm+0vkPnbH/U9/mJccPXac6+95pO/2j91xf5DytP2+cjLVQxwmDfQ/M7MzAKqvj1XbDwJnL9vvLODRyYsnqXj0yNG+248PeCbxoP2bMujvt/2+cjLVQxwmDfS3Adur77cDty7b/q4q++Z84MmUx+d37ltky/xuNs3dzpb53ezctxi6SNE6c3qq7/Y11q+TN3j/pgz6+22/r5xM9RCHUdIrrwf+HTjXzA6a2buBeeAiMzsAXFS9BvgG8BPgAeCfgb9updQdWBpbXDxyFOfE2KKCfX9XXXwuU+vWnLRtat0a3vEHZ/fdftXF5wYpT9vvKydTPcRhlKybdwz40YV99nXgPXULFYPVxhbr3ETKNQNh6Rj6HdvsS0/r/JhXK490R/UQB/MBY6hdmp2d9dgWNds0d3vfu8gGPDh/2UR/c2UGAvRaN1e/9VW68EVkbGa2191nh+2nJRAGaGNsURkIIhKCAv0AbYwtlpSBoBvZIvGIYj36GLUxtnjm9BSLfYJ6bhkImiQjEhcF+lVs2zzTaGC66uJz+47R55aB0NaNbBGZjAJ9h0rJQChpiEokBQr0HWu6lxCjroeock1ZFWmKbsZK47qcJKOJbSLDKdBL47ZtnuHqt76KmekpDJiZnmptroBSVkWG09CNtKKrISrdDxAZTi16SZoWzRIZToFekqZFs0SG09CNtKKrTJhSUlZF6lCgl8Z1PTO2hJRVkToU6KVxpc6MXd6LmT5lHe7w5NFj6mVIcAr00rgSM2FW9mKe+PWxp3+mtX4kNN2MlcaVmAnTrxeznHL7JSQFemlciZkwo/RWcu7RSNwU6KVxXc6MjcUovZWcezQSN43RSytKy4TptwT1crn3aCRuCvQN0iqK5VqZz6+sG4mJAn1D9FQlKa0XI+nQGH1DtIqiiMRKgb4hJeaOi0gaNHTTkFIe/C0izejynp5a9A0pMXdcRCbT9ZPR1KJvSMhVFHPL9snteEZR4jHHrO366Ho9KAX6BoXIusgt2ye34xlFicccsy7qo+t7ehq6SVxu2T6rHc/OfYtsmd/Nprnb2TK/e2g3d9z9Q8mtDlPXRX10vR6UAn3icsv2GVTupVbVqGOaXY+B1pFbHaaui/ro+p6eAn3iclspclC515iN1cpKqZW8Wh2m0ivJSRefqa7Xg1KgT1xu2T6Djue4e9/9x219xdhKHnTMf/zy9cn0SnLS1Wdq2+YZ9sxt5cH5y9gzt7XV+zG1Ar2ZvdfMfmhm95nZ9Wb2XDPbZGb3mNkBM7vRzJ7dVGFz0HQLLbeVIgcdz8yYrayUejqDjvmuHx9OpleSk9w+UwDmA1pKQ3/RbAb4NvBKdz9qZv8KfAO4FLjF3W8ws88B33P3a1b7W7Ozs76wsDBROVKy8m4+9FoKqV9EXRj33OVwrjfN3U6/T6cBD85f1nVxJEJmttfdZ4ftV3foZi0wZWZrgVOAQ8BW4Kbq59cB22q+RzZSGjeOzbitrBxaZSn1SiRuE+fRu/uimX0ceBg4CvwbsBc44u5PVbsdBPp+ssxsB7ADYMOGDZMWIykpjRvHaNx5CqmvJtlvjfuU779IOBO36M3sVOByYBNwJvA84E19du07NuTu17r7rLvPrl+/ftJiJEUtNBlHDr0SiUOdmbFvBB5098MAZnYL8Hpg2szWVq36s4BH6xczD3VbaJom364Yz29KvZIYz5/01An0DwPnm9kp9IZuLgQWgLuAtwE3ANuBW+sWMhd11sPRNPl26fzWo/MXt4mzbgDM7CPAnwFPAfuAv6I3Jn8DcFq17c/d/f9W+zulZN3UsWV+d99lkGemp9gztzVAidIyrLU56flVK7ZH12cYo2bd1FrUzN0/DHx4xeafAK+t83flmXQjd3KjtDYnOb9qxZ6g6zNumhmbiNA3clOeij9KWusk51fpsieEvj5ldQr0iQi51EFKC4T1M0prc5Lzq1bsCbktxZEbrUefiJAPNun6IQnLNTEGPspjHic5v3p85Akhr08ZToE+IaFS7UK1XJsaAx81rXXc86sJTSdLKRW0NBq6kaFCjb82NQbe1sQjTWiSVKhFL0OFark22ZNoq7WpVqykQC16GSpUy1WZHCLNUIteRhKi5aoxcJFmKNAHpFmVq1Mmh0gzFOgD0azK0WgMXKQ+BfqalrfKp09Zhzs8efTY0NZnyNx0ESmLAn0NK1vlT/z62NM/G9ZC16xKEemKsm5q6NcqX+7oseNceeO9fdeGUUZJHlJeA0jKoUBfw6it735rw2htkPSlvgaQlENDNzUMWuukn5Xj78ooSZ/us4TLHFPG2ngU6Gvol+e9mpU9AGWUpK30+yyhMseUsTY+Dd3UsHLG6KmnrGN6at3A/TX+npfS77OEWo9fzwEYn1r0NfVrla9scYDG33NU+szdUD2a0ntSk1CLvgVa1bAMpddzqB5N6T2pSahF3xKNv5eh5HoO1aMpvSc1iWQDfYl33Us8ZolXqMwxZayNz9w9dBmYnZ31hYWFkfcfNAaec7e5xGMWkdWZ2V53nx22X5It+hLzl0s85pXUo5FJlX7tJBnoS7zrXuIxL6fcaZmUrp1Es25KvOte4jEvp9xpmZSunUQDfYnrxJR4zMuV3qORyenaSTTQl5i/XOIxL1d6j0Ymp2sn0TF6KDN/ucRjXqLcaZmUrp2EA72kb5xMCOVOxy/WlSx17SSaRy/p07yAvISqz9Kvo1Hz6JMco5f0TZIJoac5xUsrWcZNQzcSxLiZEMqFjptWsoxbrRa9mU2b2U1m9mMz229mrzOz08zsTjM7UH09tanCSj7GzYRQyy1uWskybnWHbj4NfNPdXw68GtgPzAG73P0cYFf1WuQk484LUMstbqHmeZQ+v2RUEw/dmNkLgDcAfwng7r8BfmNmlwMXVLtdB9wNfLBOISU/42ZCDHo+r1pucdBKlnGrM0b/MuAw8CUzezWwF7gCeLG7HwJw90Nm9qL6xZTS1c2FLn1Rqy6EmudR8vySUdUZulkLnAdc4+6bgV8xxjCNme0wswUzWzh8+HCNYkiKlm6uLh45inPi5uqgTJo6M4PHfS+R3EycR29mLwG+4+4bq9d/RC/Q/w5wQdWaPwO4291XbXYpj748W+Z39x2KmZmeYs/c1mTfS9qjXtkztZ5H7+4/BR4xs6UgfiHwI+A2YHu1bTtw66TvIfnq8uaqbuSmT72yeupm3fwt8FUz+z7wGuCfgHngIjM7AFxUvRY5SZdpcUrBS5/Sa+upFejd/V53n3X333P3be7+hLv/r7tf6O7nVF8fb6qwko8u0+KUgpc+9crq0cxYCaLLtLhtm2dYeOhxrr/nEY67s8aMP/19ZWpMItQ4eSzptaneJ1Cgl2C6SovbuW+Rm/cucrxKPDjuzs17F5l96WlJfEhjEXIZihiWGk55GQ4taibZ0/huM0KexxgevJPydaQWvWRP47vNCH0eQ0+MCn38dahFL9lT1k0zSj+PKR+/Ar1kT1k3zSj9PKZ8/Bq6kexp4atmlH4eUz5+PUpQRCRRepSgiIgACvQiItlToBcRyZwCvYhI5hToRUQyp0AvIpI55dG3JNVV7kQkPwr0LUh5lTsRyY8CfQtWW+VueaBXq788qnMJQYG+BaOscqdWf3lU5xKKbsa2YJRV7lJe21omozqXUBToWzDKKncpr20tk1GdSygaumnBKKvcxfIMTOmO6lxC3aNRoG/JsKfhxPAMTOmW6rxsIe/RKNAHkvLa1jIZ1Xk9qWcsjZqN1wYF+oBCPwNTuqc6n0wOGUsh79HoZqyIRC+HjKWQz5xVoBeR6OWQsRTymbMK9CISvZCt4aZs2zzD1W99FTPTUxgwMz3F1W99lbJu6kr95k2pVG+y0lUXn8tVX/sex3574hnX655lyWUshbpHk22gz+HmTYlUbzKQDXktA2U7dJPDzZsSqd4G27lvkS3zu9k0dztb5nezc99i6CJ15mN33M+x437StmPHXdfFiLJt0edw86ZEqrf+Su/p6LqoJ9sWfQ43b0qkeuuv9J6Orot6agd6M1tjZvvM7OvV601mdo+ZHTCzG83s2fWLOb6QqUwyOdVbf6W3aHVd1NPE0M0VwH7gBdXrjwKfdPcbzOxzwLuBaxp4n7GkPN089qyTNsuXcr21qfQF0XRd1GPuPnyvQb9sdhZwHfCPwPuAPwEOAy9x96fM7HXAP7j7xav9ndnZWV9YWJi4HDlZORYLvZZLV/m2w8RevlzpvEs/ZrbX3WeH7Vd36OZTwAeA31avXwgccfenqtcHAV2FY4h9LDb28uWq68k2JWf45GjioRszezPwmLvvNbMLljb32bVvl8HMdgA7ADZs2DBpMbIT+1hs7OXLWVeTbUrP8MlRnTH6LcBbzOxS4Ln0xug/BUyb2dqqVX8W8Gi/X3b3a4FroTd0U6McSRk2vt31WOy44+1Nli/2exGlCrmcbuxSvWYnHrpx9w+5+1nuvhF4O7Db3d8J3AW8rdptO3Br7VJmYqmltHjkKM6JltLybnGX2QWjlGelpso3yXtLN9Rr6y/la7aNPPoPAu8zswfojdl/oYX3SMrSeOeVN947dHy7y7HYScbbmypfLmP9OY5lK2e9vyau2VDXSyMzY939buDu6vufAK9t4u/moF+2xEorW0pdjcVO2nJronw5tBpzHcvWIw/7q3vNhrxesp0ZG4t+rYCVQrWUQrbccmg15tIrWSnkcroxq3vNhrxeFOhbNux/+5AtpZCzDXOY6ZhDr2SQbZtn2DO3lQfnL2PP3NbigzzUv2ZDXi/ZLmq2Uqi75YOyVKDXUgp51z7kbMMcZjqmOlu1i89Cqtkpq6l7zYa8XmrNjG1K2zNjQ84q1IzGfKVYt12UOcXz0oU2zsuoM2OLaNGHzAvOoeUq/aVYt6N+FkZtkffbT3n4/YW8XooI9KHHUkM9Pkzal1rdjvJZGDU7ZNB+g5IPcrh3UVeo66WIm7E5ZHiINGGUz8Ko2SGD9ltj/Z/xp89bOEUE+hwyPESaMMpnYdQe8KD9jrvr8xaZIgK98oJluRxns45qlM/CqD3gQfst/U193uJRRNaNyBJlhAw36jnSuQyvq/XoRZKS62zWJo3aA1ZPOR1FZN2ILAmdgZWKUbNDUss6KlUxgT7HmXoyvlRns4rUUcTQTcrrSEuzlIElJSqiRR/TTD31LMJKcTarSF1FBPpYxmVzXb88NRpXltIUMXQTy8xYZXyISAhFBPpYxmVj6VmISFmKCPSx5PvG0rMQkbIUMUYPcYzL6lmcIhJCMYE+Bsr4EJEQFOg7FkPPQkTKUsQYvYhIydSil5GEmuilCWYi9SnQy1ChJnppgplIMzR0I0OFmuilCWYizVCgl6FCTfTSBDORZijQy0BLj9wb9Ayytid6aYKZSDMU6KWv5Us799PFRK9Ylq4QSZ1uxnYgxcyRfuPjS2Y6OgZNMItfitd2iRToW5Zq5sigcXAD9sxt7awcmmAWr1Sv7RJp6KZlTWaOLI2Zb5q7nS3zu1t9QpbGx2UYZUWlY+JAb2Znm9ldZrbfzH5oZldU208zszvN7ED19dTmipuepjJHun4cosbHZRhlRaWjTov+KeD97v4K4HzgPWb2SmAO2OXu5wC7qtfFaqpl3HXrKZalnSVeKfX6uuwNx/TeSyYeo3f3Q8Ch6vtfmtl+YAa4HLig2u064G7gg7VKmbCmliYO0XrS+LisJpVlt0PeS4jlPkYjY/RmthHYDNwDvLj6T2DpP4MXNfEeqWqqZZxC6ymGlot0J5VeX8h7CbHcx6iddWNmzwduBq5091+Y2ai/twPYAbBhw4a6xYhaEy3j2FtPsbRcpFsp9PpC3kuI5T5GrRa9ma2jF+S/6u63VJt/ZmZnVD8/A3is3++6+7XuPuvus+vXr69TjCLE3nqKpeUyLvVChkv9HIXsDcfSE5+4RW+9pvsXgP3u/ollP7oN2A7MV19vrVVCeVrMradYWi7jUC9kuBzOUcjecCw98Tot+i3AXwBbzeze6t+l9AL8RWZ2ALioei2Zi6XlMo5UeyFdyuEchewNx9ITr5N18216EyX7uXDSvytpiqXlMo4UeyFdy+UchewNx9AT1xIIkUh9zZAU16U5c3qq76JtMfdCuqZzlAcF+gjkMA4KcbRcxpFiL6RrOkd5UKCPwGrjoCkFztSk2AvpWlfnKMUebUplVqCPQC7joClKrRcSQtvnKMUebWpl1uqVEUgxY0WkKSlm9qRWZgX6CGilSClZij3a1MqsQB+BWHJtRUJIsUebWpk1Rh8JjRVLqVLM7EmtzAr0IhJUitlPqZXZ3D10GZidnfWFhYXQxRARSYqZ7XX32WH7qUVfiJRyfkWkWQr0BUgt51dEmqVAX4BYZ96ql5EG1dP4YjtnCvQFiDHnV72MNKiexhfjOVMefQFizPlNbWZhqVRP44vxnCnQTyC1R6vFOPM2xl6GPJPqaXwxnjMF+jEtdcsWjxzFOdEtiznYxzjzNsZehjyT6ml8MZ4zBfoxxdgtG8W2zTPsmdvKg/OXsWdua/Dx1ZC9jNR6ZCHF2BuMXYznTDdjxxRjtyxFoWYWxnijLGapzQCNQYznTDNjx7RlfnffR6vNTE+xZ25rgBLJOJquv9jS6KQso86M1dDNmGLslsnomuyRpXi/RsqkQD+mGG9syuiavFGW6v0aKY/G6CegJYXT1eTysrpfI6lQoJeiNHmj7Mzpqb7j/av1DjSmLyEo0EtxmuqRjds7UMaPhKJALwOp9bm6cXsHsS4uJ/lToJe+1PoczTi9A43pSyjKuplACTMrlVHSvBinxksZFOjHVErutFqfzdMcDAlFgX5MpbR01fpsnuZgSCgaox9TKS3dJvPN5QTNwZAQ1KIfUyktXbU+RfLRSovezC4BPg2sAT7v7vNtvE8IJbV01foUyUPjgd7M1gCfBS4CDgLfNbPb3P1HTb9XCDEuQSoispo2WvSvBR5w958AmNkNwOVAFoEe1NIVkbS0MUY/Azyy7PXBapuIiATQRqC3Ptue8XQTM9thZgtmtnD48OEWiiEiItBOoD8InL3s9VnAoyt3cvdr3X3W3WfXr1/fQjFERATaCfTfBc4xs01m9mzg7cBtLbyPiIiMoPGbse7+lJn9DXAHvfTKL7r7D5t+HxERGU0UDwc3s8PAQxP++unAzxssTgp0zGXQMZehzjG/1N2Hjn1HEejrMLOFUZ6CnhMdcxl0zGXo4pi1BIKISOYU6EVEMpdDoL82dAEC0DGXQcdchtaPOfkxehERWV0OLXoREVlF0oHezC4xs/vN7AEzmwtdnjaY2dlmdpeZ7TezH5rZFdX208zsTjM7UH09NXRZm2Rma8xsn5l9vXq9yczuqY73xmoyXjbMbNrMbjKzH1d1/boC6vi91TV9n5ldb2bPza2ezeyLZvaYmd23bFvferWez1Tx7Ptmdl5T5Ug20C9bDvlNwCuBd5jZK8OWqhVPAe9391cA5wPvqY5zDtjl7ucAu6rXObkC2L/s9UeBT1bH+wTw7iClas+ngW+6+8uBV9M79mzr2MxmgL8DZt39d+lNrnw7+dXzl4FLVmwbVK9vAs6p/u0ArmmqEMkGepYth+zuvwGWlkPOirsfcvf/rL7/Jb0AMEPvWK+rdrsO2BamhM0zs7OAy4DPV68N2ArcVO2S2/G+AHgD8AUAd/+Nux8h4zqurAWmzGwtcApwiMzq2d2/BTy+YvOger0c+Ir3fAeYNrMzmihHyoG+uOWQzWwjsBm4B3ixux+C3n8GwIvClaxxnwI+APy2ev1C4Ii7P1W9zq2uXwYcBr5UDVd93syeR8Z17O6LwMeBh+kF+CeBveRdz0sG1WtrMS3lQD/Scsi5MLPnAzcDV7r7L0KXpy1m9mbgMXffu3xzn11zquu1wHnANe6+GfgVGQ3T9FONS18ObALOBJ5Hb+hipZzqeZjWrvOUA/1IyyHnwMzW0QvyX3X3W6rNP1vq1lVfHwtVvoZtAd5iZv9DbzhuK70W/nTVxYf86vogcNDd76le30Qv8OdaxwBvBB5098Pufgy4BXg9edfzkkH12lpMSznQF7EccjU+/QVgv7t/YtmPbgO2V99vB27tumxtcPcPuftZ7r6RXp3udvd3AncBb6t2y+Z4Adz9p8AjZrb0hPkL6T16M8s6rjwMnG9mp1TX+NIxZ1vPywyq19uAd1XZN+cDTy4N8dTm7sn+Ay4F/gv4b+DvQ5enpWP8Q3rdt+8D91b/LqU3br0LOFB9PS10WVs49guAr1ffvwz4D+AB4GvAc0KXr+FjfQ2wUNXzTuDU3OsY+AjwY+A+4F+A5+RWz8D19O5BHKPXYn/3oHqlN3Tz2Sqe/YBeRlIj5dDMWBGRzKU8dCMiIiNQoBcRyZwCvYhI5hToRUQyp0AvIpI5BXoRkcwp0IuIZE6BXkQkc/8P8KEMzrCeQbcAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X1 = [random.randint(0, 100) for _ in range(100)]\n",
    "X2 = [random.randint(0, 100) for _ in range(100)]\n",
    "plt.scatter(X1, X2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=500,\n",
       "    n_clusters=6, n_init=10, n_jobs=1, precompute_distances='auto',\n",
       "    random_state=None, tol=0.0001, verbose=0)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tranning_data = [[x1, x2] for x1, x2 in zip(X1, X2)]\n",
    "cluster = KMeans(n_clusters=6, max_iter=500)\n",
    "cluster.fit(tranning_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "centers = defaultdict(list)\n",
    "for label, location in zip(cluster.labels_, tranning_data):\n",
    "    centers[label].append(location)\n",
    "color = ['red', 'green', 'grey', 'black', 'yellow', 'orange']\n",
    "\n",
    "for i, c in enumerate(centers):\n",
    "    for location in centers[c]:\n",
    "        plt.scatter(*location, c=color[i])\n",
    "        \n",
    "for center in cluster.cluster_centers_:\n",
    "    plt.scatter(*center, s=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part-2 Question and Answer 问答"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. What's the *model*? why  all the models are wrong, but some are useful? (5 points) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans:模型是对真实世界的抽象。“所有的模型都是错误的”是指模型只是对真实世界的外在表现进行的函数拟合，并不代表模型描述了真实世界的内在运行逻辑。“有些模型是有用的”是指模型在对真实世界进行预测时的精度达到了人们使用场景的要求，那就是有用的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<评阅点>\n",
    "> + 对模型的理解是否正确,对模型的抽象性是否正确(5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. What's the underfitting and overfitting? List the reasons that could make model overfitting or underfitting. (10 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans:欠拟合是训练集和验证集的误差都较大。过拟合是训练集误差较小，但验证集误差较大。  \n",
    "欠拟合的原因可能有：模型选择不合理；模型太简单（比如特征量少）；训练数据量太小；超参数设置不合理；训练轮次不足；等等。  \n",
    "过拟合的原因可能有：特征量太多；模型太复杂；数据噪声太大；等等。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<评阅点>\n",
    "> + 对过拟合和欠拟合的理解是否正确 (3')\n",
    "+ 对欠拟合产生的原因是否理解正确(2')\n",
    "+ 对过拟合产生的原因是否理解正确(5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. What's the precision, recall, AUC, F1, F2score. What are they mainly target on? (12')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans:  \n",
    "precision：所有预测结果为“是”且预测正确的数量 / 预测结果为“是”的总数。用于判断预测精度，主要是用在正负样本分布比较均匀的场合。  \n",
    "recall: 所有预测结果为“是”且预测正确的数量 / 所有真正标签为“是”的总数。使用场合是，我们关注正样本是否被检测出，对负样本被检测为正样本的这种错误有一定的容忍度。  \n",
    "AUC: AUC是ROC曲线之下与x坐标轴之间围成的面积。ROC曲线是（正阳率，伪阳率）的坐标点连成的。主要用于正负样本非常不均匀的场合。比如负样本比例过低，只要全部预测为正就可以有较高的精度及召回率。AUC就可以识别这种看似高效实则没意义的算法。指标希望能尽可能识别出真正的正样本，也希望尽可能少地将负样本识别为正样本。  \n",
    "F Score：precision和recall通常是难以兼得的，调参导致其中一个指标上升很可能导致另一个指标下降。F Score是将这两个指标在评价时权衡。  \n",
    "$$ F Score = (1+\\beta ^{2})\\frac{precision*recall}{\\beta ^{2}precision+recall} $$\n",
    "F1: beta值为1，precision和recall权重相同，都很重要。  \n",
    "F2：beta值为2，precision比recall权重高。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<评阅点>\n",
    "> + 对precision, recall, AUC, F1, F2 理解是否正确(6‘)\n",
    "+ 对precision, recall, AUC, F1, F2的使用侧重点是否理解正确 (6’)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Based on our course and yourself mind, what's the machine learning?  (8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans: 传统分析式编程是需要人工分析数据，获得关键参数，写成代码，比如分类规则、数据的预测函数等等。机器学习是由人来设计计算模型，给定数据后，由机器来计算出分类规则、预测函数等等。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<评阅点> 开放式问题，是否能说出来机器学习这种思维方式和传统的分析式编程的区别（8'）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. \"正确定义了机器学习模型的评价标准(evaluation)， 问题基本上就已经解决一半\". 这句话是否正确？你是怎么看待的？ (8‘)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans:正确。模型的选择对于机器学习的结果是非常重要的。判断模型的好坏，取决于评价标准。评价标准的选择，又是基于对需求场景的了解。从真实的应用场景出发，判断各类标准中哪个标准最贴近需求，然后选择这个标准来选择模型和调优，就能得到相对比较满意的结果。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<评阅点> 开放式问题，主要看能理解评价指标对机器学习模型的重要性."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part-03 Programming Practice 编程练习"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. In our course and previous practice, we complete some importance components of Decision Tree. In this problem, you need to build a **completed** Decision Tree Model. You show finish a `predicate()` function, which accepts three parameters **<gender, income, family_number>**, and outputs the predicated 'bought': 1 or 0.  (20 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from icecream import ic\n",
    "class my_decision_tree(object):    \n",
    "    def __init__(self):\n",
    "        # 初始化模型数据\n",
    "        self.data = []\n",
    "    \n",
    "    def entropy_2(self, elements):\n",
    "        '''群体的混乱程度'''\n",
    "        counter = Counter(elements)\n",
    "        probs = [counter[c] / len(elements) for c in set(elements)]\n",
    "        return - sum(p * np.log(p) for p in probs)\n",
    "    \n",
    "    def find_the_optimal_spliter_2(self, training_data: pd.DataFrame, target: str) -> str:\n",
    "        # 用于分类的列名\n",
    "        x_fields = set(training_data.columns.tolist()) - {target}\n",
    "\n",
    "        spliter = None\n",
    "        # 初始化为最大值\n",
    "        min_entropy = float('inf')\n",
    "\n",
    "        for f in x_fields:\n",
    "            values = set(training_data[f])\n",
    "            for v in values:\n",
    "                # f列值为v的标签值\n",
    "                sub_spliter_1 = training_data[training_data[f] == v][target].tolist()\n",
    "                entropy_1 = entropy(sub_spliter_1)\n",
    "                # f列值不为v的标签值\n",
    "                sub_spliter_2 = training_data[training_data[f] != v][target].tolist()\n",
    "                entropy_2 = entropy(sub_spliter_2)\n",
    "                # 计算以f列的v值分类后的熵\n",
    "                entropy_v = entropy_1 + entropy_2\n",
    "                # 保留最小熵以及对应的列名和值\n",
    "                if entropy_v <= min_entropy:\n",
    "                    min_entropy = entropy_v\n",
    "                    spliter = (f, v)\n",
    "        return spliter\n",
    "    \n",
    "    def get_optimal_spliter_2(self, training_data: pd.DataFrame, target: str):\n",
    "        if entropy(training_data[target]) == 0 or len(training_data.columns.tolist()) == 1:\n",
    "            # 熵为0无需再分割，或列为1无法再分割\n",
    "            return training_data.reset_index().loc[0,target]\n",
    "        f,v = self.find_the_optimal_spliter_2(training_data, target)\n",
    "        \n",
    "        dataset1 = training_data[training_data[f] == v]\n",
    "        result1 = self.get_optimal_spliter_2(dataset1.drop([f], axis=1), target)\n",
    "        \n",
    "        dataset2 = training_data[training_data[f] != v]\n",
    "        result2 = self.get_optimal_spliter_2(dataset2.drop([f], axis=1), target)\n",
    "        \n",
    "        return [f, v, result1, result2]\n",
    "    \n",
    "    def fit(self, training_data: pd.DataFrame, target: str):\n",
    "        self.data = self.get_optimal_spliter_2(training_data, target)\n",
    "        return\n",
    "    \n",
    "    def predict(self, input_dict):\n",
    "        # 数据结构为：list，下标0表示列名，下标1表示值，下标2表示等于这个值时的标签（或嵌套下一个list）\n",
    "        # 下标3表示不等于这个值时的标签（或嵌套下一个list）\n",
    "        data = self.data\n",
    "        while(isinstance(data, list)):\n",
    "            f = data[0]\n",
    "            v = data[1]\n",
    "            if v == input_dict[f]:\n",
    "                data = data[2]\n",
    "            else:\n",
    "                data = data[3]\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gender</th>\n",
       "      <th>income</th>\n",
       "      <th>family_number</th>\n",
       "      <th>bought</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>F</td>\n",
       "      <td>+10</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>F</td>\n",
       "      <td>-10</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>F</td>\n",
       "      <td>+10</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>F</td>\n",
       "      <td>+10</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>M</td>\n",
       "      <td>+10</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>M</td>\n",
       "      <td>+10</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>M</td>\n",
       "      <td>-10</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  gender income  family_number  bought\n",
       "0      F    +10              1       1\n",
       "1      F    -10              1       1\n",
       "2      F    +10              2       1\n",
       "3      F    +10              1       0\n",
       "4      M    +10              1       0\n",
       "5      M    +10              1       0\n",
       "6      M    -10              2       1"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# you code here\n",
    "mock_data = {\n",
    "    'gender':['F', 'F', 'F', 'F', 'M', 'M', 'M'],\n",
    "    'income': ['+10', '-10', '+10', '+10', '+10', '+10', '-10'],\n",
    "    'family_number': [1, 1, 2, 1, 1, 1, 2],\n",
    "    'bought': [1, 1, 1, 0, 0, 0, 1],\n",
    "}\n",
    "dataset = pd.DataFrame.from_dict(mock_data)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label is: 0\n",
      "label is: 1\n"
     ]
    }
   ],
   "source": [
    "dt = my_decision_tree()\n",
    "dt.fit(dataset, 'bought')\n",
    "result = dt.predict({'gender':'M', 'income':'+10', 'family_number':1})\n",
    "print('label is:', result)\n",
    "result = dt.predict({'gender':'M', 'income':'-10', 'family_number':1})\n",
    "print('label is:', result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['family_number', 2, 1, ['income', '-10', 1, ['gender', 'M', 0, 1]]]"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<评阅点>\n",
    "> + 是否将之前的决策树模型的部分进行合并组装， predicate函数能够顺利运行(8')\n",
    "+ 是够能够输入未曾见过的X变量，例如gender, income, family_number 分别是： <M, -10, 1>, 模型能够预测出结果 (12')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. 将上一节课(第二节课)的线性回归问题中的Loss函数改成\"绝对值\"，并且改变其偏导的求值方式，观察其结果的变化。(19 point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you code here\n",
    "from sklearn.datasets import load_boston\n",
    "import random\n",
    "\n",
    "dataset = load_boston()\n",
    "x,y=dataset['data'],dataset['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data': array([[6.3200e-03, 1.8000e+01, 2.3100e+00, ..., 1.5300e+01, 3.9690e+02,\n",
       "         4.9800e+00],\n",
       "        [2.7310e-02, 0.0000e+00, 7.0700e+00, ..., 1.7800e+01, 3.9690e+02,\n",
       "         9.1400e+00],\n",
       "        [2.7290e-02, 0.0000e+00, 7.0700e+00, ..., 1.7800e+01, 3.9283e+02,\n",
       "         4.0300e+00],\n",
       "        ...,\n",
       "        [6.0760e-02, 0.0000e+00, 1.1930e+01, ..., 2.1000e+01, 3.9690e+02,\n",
       "         5.6400e+00],\n",
       "        [1.0959e-01, 0.0000e+00, 1.1930e+01, ..., 2.1000e+01, 3.9345e+02,\n",
       "         6.4800e+00],\n",
       "        [4.7410e-02, 0.0000e+00, 1.1930e+01, ..., 2.1000e+01, 3.9690e+02,\n",
       "         7.8800e+00]]),\n",
       " 'target': array([24. , 21.6, 34.7, 33.4, 36.2, 28.7, 22.9, 27.1, 16.5, 18.9, 15. ,\n",
       "        18.9, 21.7, 20.4, 18.2, 19.9, 23.1, 17.5, 20.2, 18.2, 13.6, 19.6,\n",
       "        15.2, 14.5, 15.6, 13.9, 16.6, 14.8, 18.4, 21. , 12.7, 14.5, 13.2,\n",
       "        13.1, 13.5, 18.9, 20. , 21. , 24.7, 30.8, 34.9, 26.6, 25.3, 24.7,\n",
       "        21.2, 19.3, 20. , 16.6, 14.4, 19.4, 19.7, 20.5, 25. , 23.4, 18.9,\n",
       "        35.4, 24.7, 31.6, 23.3, 19.6, 18.7, 16. , 22.2, 25. , 33. , 23.5,\n",
       "        19.4, 22. , 17.4, 20.9, 24.2, 21.7, 22.8, 23.4, 24.1, 21.4, 20. ,\n",
       "        20.8, 21.2, 20.3, 28. , 23.9, 24.8, 22.9, 23.9, 26.6, 22.5, 22.2,\n",
       "        23.6, 28.7, 22.6, 22. , 22.9, 25. , 20.6, 28.4, 21.4, 38.7, 43.8,\n",
       "        33.2, 27.5, 26.5, 18.6, 19.3, 20.1, 19.5, 19.5, 20.4, 19.8, 19.4,\n",
       "        21.7, 22.8, 18.8, 18.7, 18.5, 18.3, 21.2, 19.2, 20.4, 19.3, 22. ,\n",
       "        20.3, 20.5, 17.3, 18.8, 21.4, 15.7, 16.2, 18. , 14.3, 19.2, 19.6,\n",
       "        23. , 18.4, 15.6, 18.1, 17.4, 17.1, 13.3, 17.8, 14. , 14.4, 13.4,\n",
       "        15.6, 11.8, 13.8, 15.6, 14.6, 17.8, 15.4, 21.5, 19.6, 15.3, 19.4,\n",
       "        17. , 15.6, 13.1, 41.3, 24.3, 23.3, 27. , 50. , 50. , 50. , 22.7,\n",
       "        25. , 50. , 23.8, 23.8, 22.3, 17.4, 19.1, 23.1, 23.6, 22.6, 29.4,\n",
       "        23.2, 24.6, 29.9, 37.2, 39.8, 36.2, 37.9, 32.5, 26.4, 29.6, 50. ,\n",
       "        32. , 29.8, 34.9, 37. , 30.5, 36.4, 31.1, 29.1, 50. , 33.3, 30.3,\n",
       "        34.6, 34.9, 32.9, 24.1, 42.3, 48.5, 50. , 22.6, 24.4, 22.5, 24.4,\n",
       "        20. , 21.7, 19.3, 22.4, 28.1, 23.7, 25. , 23.3, 28.7, 21.5, 23. ,\n",
       "        26.7, 21.7, 27.5, 30.1, 44.8, 50. , 37.6, 31.6, 46.7, 31.5, 24.3,\n",
       "        31.7, 41.7, 48.3, 29. , 24. , 25.1, 31.5, 23.7, 23.3, 22. , 20.1,\n",
       "        22.2, 23.7, 17.6, 18.5, 24.3, 20.5, 24.5, 26.2, 24.4, 24.8, 29.6,\n",
       "        42.8, 21.9, 20.9, 44. , 50. , 36. , 30.1, 33.8, 43.1, 48.8, 31. ,\n",
       "        36.5, 22.8, 30.7, 50. , 43.5, 20.7, 21.1, 25.2, 24.4, 35.2, 32.4,\n",
       "        32. , 33.2, 33.1, 29.1, 35.1, 45.4, 35.4, 46. , 50. , 32.2, 22. ,\n",
       "        20.1, 23.2, 22.3, 24.8, 28.5, 37.3, 27.9, 23.9, 21.7, 28.6, 27.1,\n",
       "        20.3, 22.5, 29. , 24.8, 22. , 26.4, 33.1, 36.1, 28.4, 33.4, 28.2,\n",
       "        22.8, 20.3, 16.1, 22.1, 19.4, 21.6, 23.8, 16.2, 17.8, 19.8, 23.1,\n",
       "        21. , 23.8, 23.1, 20.4, 18.5, 25. , 24.6, 23. , 22.2, 19.3, 22.6,\n",
       "        19.8, 17.1, 19.4, 22.2, 20.7, 21.1, 19.5, 18.5, 20.6, 19. , 18.7,\n",
       "        32.7, 16.5, 23.9, 31.2, 17.5, 17.2, 23.1, 24.5, 26.6, 22.9, 24.1,\n",
       "        18.6, 30.1, 18.2, 20.6, 17.8, 21.7, 22.7, 22.6, 25. , 19.9, 20.8,\n",
       "        16.8, 21.9, 27.5, 21.9, 23.1, 50. , 50. , 50. , 50. , 50. , 13.8,\n",
       "        13.8, 15. , 13.9, 13.3, 13.1, 10.2, 10.4, 10.9, 11.3, 12.3,  8.8,\n",
       "         7.2, 10.5,  7.4, 10.2, 11.5, 15.1, 23.2,  9.7, 13.8, 12.7, 13.1,\n",
       "        12.5,  8.5,  5. ,  6.3,  5.6,  7.2, 12.1,  8.3,  8.5,  5. , 11.9,\n",
       "        27.9, 17.2, 27.5, 15. , 17.2, 17.9, 16.3,  7. ,  7.2,  7.5, 10.4,\n",
       "         8.8,  8.4, 16.7, 14.2, 20.8, 13.4, 11.7,  8.3, 10.2, 10.9, 11. ,\n",
       "         9.5, 14.5, 14.1, 16.1, 14.3, 11.7, 13.4,  9.6,  8.7,  8.4, 12.8,\n",
       "        10.5, 17.1, 18.4, 15.4, 10.8, 11.8, 14.9, 12.6, 14.1, 13. , 13.4,\n",
       "        15.2, 16.1, 17.8, 14.9, 14.1, 12.7, 13.5, 14.9, 20. , 16.4, 17.7,\n",
       "        19.5, 20.2, 21.4, 19.9, 19. , 19.1, 19.1, 20.1, 19.9, 19.6, 23.2,\n",
       "        29.8, 13.8, 13.3, 16.7, 12. , 14.6, 21.4, 23. , 23.7, 25. , 21.8,\n",
       "        20.6, 21.2, 19.1, 20.6, 15.2,  7. ,  8.1, 13.6, 20.1, 21.8, 24.5,\n",
       "        23.1, 19.7, 18.3, 21.2, 17.5, 16.8, 22.4, 20.6, 23.9, 22. , 11.9]),\n",
       " 'feature_names': array(['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD',\n",
       "        'TAX', 'PTRATIO', 'B', 'LSTAT'], dtype='<U7'),\n",
       " 'DESCR': \"Boston House Prices dataset\\n===========================\\n\\nNotes\\n------\\nData Set Characteristics:  \\n\\n    :Number of Instances: 506 \\n\\n    :Number of Attributes: 13 numeric/categorical predictive\\n    \\n    :Median Value (attribute 14) is usually the target\\n\\n    :Attribute Information (in order):\\n        - CRIM     per capita crime rate by town\\n        - ZN       proportion of residential land zoned for lots over 25,000 sq.ft.\\n        - INDUS    proportion of non-retail business acres per town\\n        - CHAS     Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)\\n        - NOX      nitric oxides concentration (parts per 10 million)\\n        - RM       average number of rooms per dwelling\\n        - AGE      proportion of owner-occupied units built prior to 1940\\n        - DIS      weighted distances to five Boston employment centres\\n        - RAD      index of accessibility to radial highways\\n        - TAX      full-value property-tax rate per $10,000\\n        - PTRATIO  pupil-teacher ratio by town\\n        - B        1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town\\n        - LSTAT    % lower status of the population\\n        - MEDV     Median value of owner-occupied homes in $1000's\\n\\n    :Missing Attribute Values: None\\n\\n    :Creator: Harrison, D. and Rubinfeld, D.L.\\n\\nThis is a copy of UCI ML housing dataset.\\nhttp://archive.ics.uci.edu/ml/datasets/Housing\\n\\n\\nThis dataset was taken from the StatLib library which is maintained at Carnegie Mellon University.\\n\\nThe Boston house-price data of Harrison, D. and Rubinfeld, D.L. 'Hedonic\\nprices and the demand for clean air', J. Environ. Economics & Management,\\nvol.5, 81-102, 1978.   Used in Belsley, Kuh & Welsch, 'Regression diagnostics\\n...', Wiley, 1980.   N.B. Various transformations are used in the table on\\npages 244-261 of the latter.\\n\\nThe Boston house-price data has been used in many machine learning papers that address regression\\nproblems.   \\n     \\n**References**\\n\\n   - Belsley, Kuh & Welsch, 'Regression diagnostics: Identifying Influential Data and Sources of Collinearity', Wiley, 1980. 244-261.\\n   - Quinlan,R. (1993). Combining Instance-Based and Model-Based Learning. In Proceedings on the Tenth International Conference of Machine Learning, 236-243, University of Massachusetts, Amherst. Morgan Kaufmann.\\n   - many more! (see http://archive.ics.uci.edu/ml/datasets/Housing)\\n\"}"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6.575, 6.421, 7.185, 6.998, 7.147, 6.43 , 6.012, 6.172, 5.631,\n",
       "       6.004, 6.377, 6.009, 5.889, 5.949, 6.096, 5.834, 5.935, 5.99 ,\n",
       "       5.456, 5.727, 5.57 , 5.965, 6.142, 5.813, 5.924, 5.599, 5.813,\n",
       "       6.047, 6.495, 6.674, 5.713, 6.072, 5.95 , 5.701, 6.096, 5.933,\n",
       "       5.841, 5.85 , 5.966, 6.595, 7.024, 6.77 , 6.169, 6.211, 6.069,\n",
       "       5.682, 5.786, 6.03 , 5.399, 5.602, 5.963, 6.115, 6.511, 5.998,\n",
       "       5.888, 7.249, 6.383, 6.816, 6.145, 5.927, 5.741, 5.966, 6.456,\n",
       "       6.762, 7.104, 6.29 , 5.787, 5.878, 5.594, 5.885, 6.417, 5.961,\n",
       "       6.065, 6.245, 6.273, 6.286, 6.279, 6.14 , 6.232, 5.874, 6.727,\n",
       "       6.619, 6.302, 6.167, 6.389, 6.63 , 6.015, 6.121, 7.007, 7.079,\n",
       "       6.417, 6.405, 6.442, 6.211, 6.249, 6.625, 6.163, 8.069, 7.82 ,\n",
       "       7.416, 6.727, 6.781, 6.405, 6.137, 6.167, 5.851, 5.836, 6.127,\n",
       "       6.474, 6.229, 6.195, 6.715, 5.913, 6.092, 6.254, 5.928, 6.176,\n",
       "       6.021, 5.872, 5.731, 5.87 , 6.004, 5.961, 5.856, 5.879, 5.986,\n",
       "       5.613, 5.693, 6.431, 5.637, 6.458, 6.326, 6.372, 5.822, 5.757,\n",
       "       6.335, 5.942, 6.454, 5.857, 6.151, 6.174, 5.019, 5.403, 5.468,\n",
       "       4.903, 6.13 , 5.628, 4.926, 5.186, 5.597, 6.122, 5.404, 5.012,\n",
       "       5.709, 6.129, 6.152, 5.272, 6.943, 6.066, 6.51 , 6.25 , 7.489,\n",
       "       7.802, 8.375, 5.854, 6.101, 7.929, 5.877, 6.319, 6.402, 5.875,\n",
       "       5.88 , 5.572, 6.416, 5.859, 6.546, 6.02 , 6.315, 6.86 , 6.98 ,\n",
       "       7.765, 6.144, 7.155, 6.563, 5.604, 6.153, 7.831, 6.782, 6.556,\n",
       "       7.185, 6.951, 6.739, 7.178, 6.8  , 6.604, 7.875, 7.287, 7.107,\n",
       "       7.274, 6.975, 7.135, 6.162, 7.61 , 7.853, 8.034, 5.891, 6.326,\n",
       "       5.783, 6.064, 5.344, 5.96 , 5.404, 5.807, 6.375, 5.412, 6.182,\n",
       "       5.888, 6.642, 5.951, 6.373, 6.951, 6.164, 6.879, 6.618, 8.266,\n",
       "       8.725, 8.04 , 7.163, 7.686, 6.552, 5.981, 7.412, 8.337, 8.247,\n",
       "       6.726, 6.086, 6.631, 7.358, 6.481, 6.606, 6.897, 6.095, 6.358,\n",
       "       6.393, 5.593, 5.605, 6.108, 6.226, 6.433, 6.718, 6.487, 6.438,\n",
       "       6.957, 8.259, 6.108, 5.876, 7.454, 8.704, 7.333, 6.842, 7.203,\n",
       "       7.52 , 8.398, 7.327, 7.206, 5.56 , 7.014, 8.297, 7.47 , 5.92 ,\n",
       "       5.856, 6.24 , 6.538, 7.691, 6.758, 6.854, 7.267, 6.826, 6.482,\n",
       "       6.812, 7.82 , 6.968, 7.645, 7.923, 7.088, 6.453, 6.23 , 6.209,\n",
       "       6.315, 6.565, 6.861, 7.148, 6.63 , 6.127, 6.009, 6.678, 6.549,\n",
       "       5.79 , 6.345, 7.041, 6.871, 6.59 , 6.495, 6.982, 7.236, 6.616,\n",
       "       7.42 , 6.849, 6.635, 5.972, 4.973, 6.122, 6.023, 6.266, 6.567,\n",
       "       5.705, 5.914, 5.782, 6.382, 6.113, 6.426, 6.376, 6.041, 5.708,\n",
       "       6.415, 6.431, 6.312, 6.083, 5.868, 6.333, 6.144, 5.706, 6.031,\n",
       "       6.316, 6.31 , 6.037, 5.869, 5.895, 6.059, 5.985, 5.968, 7.241,\n",
       "       6.54 , 6.696, 6.874, 6.014, 5.898, 6.516, 6.635, 6.939, 6.49 ,\n",
       "       6.579, 5.884, 6.728, 5.663, 5.936, 6.212, 6.395, 6.127, 6.112,\n",
       "       6.398, 6.251, 5.362, 5.803, 8.78 , 3.561, 4.963, 3.863, 4.97 ,\n",
       "       6.683, 7.016, 6.216, 5.875, 4.906, 4.138, 7.313, 6.649, 6.794,\n",
       "       6.38 , 6.223, 6.968, 6.545, 5.536, 5.52 , 4.368, 5.277, 4.652,\n",
       "       5.   , 4.88 , 5.39 , 5.713, 6.051, 5.036, 6.193, 5.887, 6.471,\n",
       "       6.405, 5.747, 5.453, 5.852, 5.987, 6.343, 6.404, 5.349, 5.531,\n",
       "       5.683, 4.138, 5.608, 5.617, 6.852, 5.757, 6.657, 4.628, 5.155,\n",
       "       4.519, 6.434, 6.782, 5.304, 5.957, 6.824, 6.411, 6.006, 5.648,\n",
       "       6.103, 5.565, 5.896, 5.837, 6.202, 6.193, 6.38 , 6.348, 6.833,\n",
       "       6.425, 6.436, 6.208, 6.629, 6.461, 6.152, 5.935, 5.627, 5.818,\n",
       "       6.406, 6.219, 6.485, 5.854, 6.459, 6.341, 6.251, 6.185, 6.417,\n",
       "       6.749, 6.655, 6.297, 7.393, 6.728, 6.525, 5.976, 5.936, 6.301,\n",
       "       6.081, 6.701, 6.376, 6.317, 6.513, 6.209, 5.759, 5.952, 6.003,\n",
       "       5.926, 5.713, 6.167, 6.229, 6.437, 6.98 , 5.427, 6.162, 6.484,\n",
       "       5.304, 6.185, 6.229, 6.242, 6.75 , 7.061, 5.762, 5.871, 6.312,\n",
       "       6.114, 5.905, 5.454, 5.414, 5.093, 5.983, 5.983, 5.707, 5.926,\n",
       "       5.67 , 5.39 , 5.794, 6.019, 5.569, 6.027, 6.593, 6.12 , 6.976,\n",
       "       6.794, 6.03 ])"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_rm = x[:,5]\n",
    "X_rm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x1e8ab279518>"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot the RM with respect to y\n",
    "plt.scatter(X_rm,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define target function\n",
    "def price(rm, k, b):\n",
    "    return k * rm + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define loss function \n",
    "def loss(y,y_hat):\n",
    "    return sum(abs(y_i - y_hat_i) for y_i, y_hat_i in zip(list(y),list(y_hat)))/len(list(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define partial derivative \n",
    "def partial_derivative_k(x, y, y_hat): \n",
    "    n = len(y)\n",
    "    gradient = 0\n",
    "    for x_i, y_i, y_hat_i in zip(list(x),list(y),list(y_hat)):\n",
    "        if y_i > y_hat_i:\n",
    "            gradient += - x_i\n",
    "        else:\n",
    "            gradient += x_i\n",
    "    return 1/n * gradient\n",
    "\n",
    "def partial_derivative_b(y, y_hat):\n",
    "    n = len(y)\n",
    "    gradient = 0\n",
    "    for y_i, y_hat_i in zip(list(y),list(y_hat)):\n",
    "        if y_i > y_hat_i:\n",
    "            gradient += - 1\n",
    "        else:\n",
    "            gradient += 1\n",
    "    return 1/n * gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, the loss is 322.0661170938671, parameters k is -38.55061529216446 and b is -57.256788251050224\n",
      "Iteration 1, the loss is 321.6611508000401, parameters k is -38.48776894829094 and b is -57.246788251050226\n",
      "Iteration 2, the loss is 321.25618450621323, parameters k is -38.424922604417425 and b is -57.23678825105023\n",
      "Iteration 3, the loss is 320.8512182123867, parameters k is -38.36207626054391 and b is -57.22678825105023\n",
      "Iteration 4, the loss is 320.44625191855954, parameters k is -38.29922991667039 and b is -57.21678825105023\n",
      "Iteration 5, the loss is 320.04128562473306, parameters k is -38.236383572796875 and b is -57.206788251050234\n",
      "Iteration 6, the loss is 319.63631933090596, parameters k is -38.17353722892336 and b is -57.196788251050236\n",
      "Iteration 7, the loss is 319.231353037079, parameters k is -38.11069088504984 and b is -57.18678825105024\n",
      "Iteration 8, the loss is 318.8263867432523, parameters k is -38.047844541176325 and b is -57.17678825105024\n",
      "Iteration 9, the loss is 318.4214204494255, parameters k is -37.98499819730281 and b is -57.16678825105024\n",
      "Iteration 10, the loss is 318.01645415559835, parameters k is -37.92215185342929 and b is -57.156788251050244\n",
      "Iteration 11, the loss is 317.6114878617717, parameters k is -37.859305509555774 and b is -57.146788251050246\n",
      "Iteration 12, the loss is 317.2065215679451, parameters k is -37.79645916568226 and b is -57.13678825105025\n",
      "Iteration 13, the loss is 316.80155527411813, parameters k is -37.73361282180874 and b is -57.12678825105025\n",
      "Iteration 14, the loss is 316.3965889802911, parameters k is -37.670766477935224 and b is -57.11678825105025\n",
      "Iteration 15, the loss is 315.99162268646444, parameters k is -37.60792013406171 and b is -57.106788251050254\n",
      "Iteration 16, the loss is 315.5866563926377, parameters k is -37.54507379018819 and b is -57.096788251050256\n",
      "Iteration 17, the loss is 315.1816900988105, parameters k is -37.482227446314674 and b is -57.08678825105026\n",
      "Iteration 18, the loss is 314.77672380498404, parameters k is -37.41938110244116 and b is -57.07678825105026\n",
      "Iteration 19, the loss is 314.3717575111572, parameters k is -37.35653475856764 and b is -57.06678825105026\n",
      "Iteration 20, the loss is 313.96679121733047, parameters k is -37.293688414694124 and b is -57.056788251050264\n",
      "Iteration 21, the loss is 313.56182492350376, parameters k is -37.23084207082061 and b is -57.046788251050266\n",
      "Iteration 22, the loss is 313.1568586296768, parameters k is -37.16799572694709 and b is -57.03678825105027\n",
      "Iteration 23, the loss is 312.7518923358501, parameters k is -37.105149383073574 and b is -57.02678825105027\n",
      "Iteration 24, the loss is 312.34692604202274, parameters k is -37.04230303920006 and b is -57.01678825105027\n",
      "Iteration 25, the loss is 311.9419597481963, parameters k is -36.97945669532654 and b is -57.006788251050274\n",
      "Iteration 26, the loss is 311.53699345436905, parameters k is -36.91661035145302 and b is -56.996788251050276\n",
      "Iteration 27, the loss is 311.13202716054246, parameters k is -36.85376400757951 and b is -56.98678825105028\n",
      "Iteration 28, the loss is 310.72706086671536, parameters k is -36.79091766370599 and b is -56.97678825105028\n",
      "Iteration 29, the loss is 310.32209457288866, parameters k is -36.72807131983247 and b is -56.96678825105028\n",
      "Iteration 30, the loss is 309.91712827906207, parameters k is -36.66522497595896 and b is -56.956788251050284\n",
      "Iteration 31, the loss is 309.51216198523554, parameters k is -36.60237863208544 and b is -56.946788251050286\n",
      "Iteration 32, the loss is 309.1071956914084, parameters k is -36.53953228821192 and b is -56.93678825105029\n",
      "Iteration 33, the loss is 308.7022293975816, parameters k is -36.476685944338406 and b is -56.92678825105029\n",
      "Iteration 34, the loss is 308.2972631037546, parameters k is -36.41383960046489 and b is -56.91678825105029\n",
      "Iteration 35, the loss is 307.8922968099276, parameters k is -36.35099325659137 and b is -56.906788251050294\n",
      "Iteration 36, the loss is 307.4873305161007, parameters k is -36.288146912717856 and b is -56.896788251050296\n",
      "Iteration 37, the loss is 307.08236422227424, parameters k is -36.22530056884434 and b is -56.8867882510503\n",
      "Iteration 38, the loss is 306.67739792844725, parameters k is -36.16245422497082 and b is -56.8767882510503\n",
      "Iteration 39, the loss is 306.27243163462055, parameters k is -36.099607881097306 and b is -56.8667882510503\n",
      "Iteration 40, the loss is 305.86746534079356, parameters k is -36.03676153722379 and b is -56.856788251050304\n",
      "Iteration 41, the loss is 305.46249904696657, parameters k is -35.97391519335027 and b is -56.846788251050306\n",
      "Iteration 42, the loss is 305.05753275313987, parameters k is -35.911068849476756 and b is -56.83678825105031\n",
      "Iteration 43, the loss is 304.65256645931316, parameters k is -35.84822250560324 and b is -56.82678825105031\n",
      "Iteration 44, the loss is 304.2476001654863, parameters k is -35.78537616172972 and b is -56.81678825105031\n",
      "Iteration 45, the loss is 303.84263387165913, parameters k is -35.722529817856206 and b is -56.806788251050314\n",
      "Iteration 46, the loss is 303.43766757783254, parameters k is -35.65968347398269 and b is -56.796788251050316\n",
      "Iteration 47, the loss is 303.032701284006, parameters k is -35.59683713010917 and b is -56.78678825105032\n",
      "Iteration 48, the loss is 302.6277349901792, parameters k is -35.533990786235655 and b is -56.77678825105032\n",
      "Iteration 49, the loss is 302.2227686963524, parameters k is -35.47114444236214 and b is -56.76678825105032\n",
      "Iteration 50, the loss is 301.81780240252533, parameters k is -35.40829809848862 and b is -56.756788251050324\n",
      "Iteration 51, the loss is 301.41283610869834, parameters k is -35.345451754615105 and b is -56.746788251050326\n",
      "Iteration 52, the loss is 301.0078698148713, parameters k is -35.28260541074159 and b is -56.73678825105033\n",
      "Iteration 53, the loss is 300.602903521045, parameters k is -35.21975906686807 and b is -56.72678825105033\n",
      "Iteration 54, the loss is 300.1979372272177, parameters k is -35.156912722994555 and b is -56.71678825105033\n",
      "Iteration 55, the loss is 299.7929709333911, parameters k is -35.09406637912104 and b is -56.706788251050334\n",
      "Iteration 56, the loss is 299.38800463956414, parameters k is -35.03122003524752 and b is -56.696788251050336\n",
      "Iteration 57, the loss is 298.9830383457375, parameters k is -34.968373691374005 and b is -56.68678825105034\n",
      "Iteration 58, the loss is 298.57807205191045, parameters k is -34.90552734750049 and b is -56.67678825105034\n",
      "Iteration 59, the loss is 298.173105758084, parameters k is -34.84268100362697 and b is -56.66678825105034\n",
      "Iteration 60, the loss is 297.76813946425693, parameters k is -34.779834659753455 and b is -56.656788251050344\n",
      "Iteration 61, the loss is 297.36317317043034, parameters k is -34.71698831587994 and b is -56.646788251050346\n",
      "Iteration 62, the loss is 296.9582068766037, parameters k is -34.65414197200642 and b is -56.63678825105035\n",
      "Iteration 63, the loss is 296.5532405827764, parameters k is -34.591295628132904 and b is -56.62678825105035\n",
      "Iteration 64, the loss is 296.1482742889496, parameters k is -34.52844928425939 and b is -56.61678825105035\n",
      "Iteration 65, the loss is 295.7433079951224, parameters k is -34.46560294038587 and b is -56.606788251050354\n",
      "Iteration 66, the loss is 295.3383417012955, parameters k is -34.402756596512354 and b is -56.596788251050356\n",
      "Iteration 67, the loss is 294.93337540746893, parameters k is -34.33991025263884 and b is -56.58678825105036\n",
      "Iteration 68, the loss is 294.52840911364194, parameters k is -34.27706390876532 and b is -56.57678825105036\n",
      "Iteration 69, the loss is 294.12344281981547, parameters k is -34.214217564891804 and b is -56.56678825105036\n",
      "Iteration 70, the loss is 293.7184765259885, parameters k is -34.15137122101829 and b is -56.556788251050364\n",
      "Iteration 71, the loss is 293.31351023216155, parameters k is -34.08852487714477 and b is -56.546788251050366\n",
      "Iteration 72, the loss is 292.9085439383353, parameters k is -34.025678533271254 and b is -56.53678825105037\n",
      "Iteration 73, the loss is 292.5035776445082, parameters k is -33.96283218939774 and b is -56.52678825105037\n",
      "Iteration 74, the loss is 292.09861135068104, parameters k is -33.89998584552422 and b is -56.51678825105037\n",
      "Iteration 75, the loss is 291.6936450568545, parameters k is -33.837139501650704 and b is -56.506788251050374\n",
      "Iteration 76, the loss is 291.2886787630275, parameters k is -33.77429315777719 and b is -56.496788251050376\n",
      "Iteration 77, the loss is 290.8837124692004, parameters k is -33.71144681390367 and b is -56.48678825105038\n",
      "Iteration 78, the loss is 290.4787461753736, parameters k is -33.64860047003015 and b is -56.47678825105038\n",
      "Iteration 79, the loss is 290.07377988154656, parameters k is -33.58575412615664 and b is -56.46678825105038\n",
      "Iteration 80, the loss is 289.66881358771997, parameters k is -33.52290778228312 and b is -56.456788251050384\n",
      "Iteration 81, the loss is 289.2638472938933, parameters k is -33.4600614384096 and b is -56.446788251050386\n",
      "Iteration 82, the loss is 288.85888100006645, parameters k is -33.39721509453609 and b is -56.43678825105039\n",
      "Iteration 83, the loss is 288.45391470623986, parameters k is -33.33436875066257 and b is -56.42678825105039\n",
      "Iteration 84, the loss is 288.04894841241315, parameters k is -33.27152240678905 and b is -56.41678825105039\n",
      "Iteration 85, the loss is 287.64398211858645, parameters k is -33.208676062915536 and b is -56.40678825105039\n",
      "Iteration 86, the loss is 287.2390158247587, parameters k is -33.14582971904202 and b is -56.396788251050396\n",
      "Iteration 87, the loss is 286.834049530932, parameters k is -33.0829833751685 and b is -56.3867882510504\n",
      "Iteration 88, the loss is 286.4290832371053, parameters k is -33.020137031294986 and b is -56.3767882510504\n",
      "Iteration 89, the loss is 286.02411694327844, parameters k is -32.95729068742147 and b is -56.3667882510504\n",
      "Iteration 90, the loss is 285.6191506494517, parameters k is -32.89444434354795 and b is -56.3567882510504\n",
      "Iteration 91, the loss is 285.2141843556246, parameters k is -32.831597999674436 and b is -56.346788251050405\n",
      "Iteration 92, the loss is 284.8092180617982, parameters k is -32.76875165580092 and b is -56.33678825105041\n",
      "Iteration 93, the loss is 284.4042517679713, parameters k is -32.7059053119274 and b is -56.32678825105041\n",
      "Iteration 94, the loss is 283.99928547414436, parameters k is -32.643058968053886 and b is -56.31678825105041\n",
      "Iteration 95, the loss is 283.59431918031754, parameters k is -32.58021262418037 and b is -56.30678825105041\n",
      "Iteration 96, the loss is 283.18935288649055, parameters k is -32.51736628030685 and b is -56.296788251050415\n",
      "Iteration 97, the loss is 282.78438659266396, parameters k is -32.454519936433336 and b is -56.28678825105042\n",
      "Iteration 98, the loss is 282.379420298837, parameters k is -32.39167359255982 and b is -56.27678825105042\n",
      "Iteration 99, the loss is 281.9744540050098, parameters k is -32.3288272486863 and b is -56.26678825105042\n",
      "Iteration 100, the loss is 281.5694877111835, parameters k is -32.265980904812785 and b is -56.25678825105042\n",
      "Iteration 101, the loss is 281.16452141735664, parameters k is -32.20313456093927 and b is -56.246788251050425\n",
      "Iteration 102, the loss is 280.75955512352965, parameters k is -32.14028821706575 and b is -56.23678825105043\n",
      "Iteration 103, the loss is 280.35458882970295, parameters k is -32.077441873192235 and b is -56.22678825105043\n",
      "Iteration 104, the loss is 279.94962253587585, parameters k is -32.01459552931872 and b is -56.21678825105043\n",
      "Iteration 105, the loss is 279.5446562420489, parameters k is -31.951749185445202 and b is -56.20678825105043\n",
      "Iteration 106, the loss is 279.1396899482222, parameters k is -31.888902841571685 and b is -56.196788251050435\n",
      "Iteration 107, the loss is 278.7347236543956, parameters k is -31.82605649769817 and b is -56.18678825105044\n",
      "Iteration 108, the loss is 278.32975736056886, parameters k is -31.76321015382465 and b is -56.17678825105044\n",
      "Iteration 109, the loss is 277.9247910667418, parameters k is -31.700363809951135 and b is -56.16678825105044\n",
      "Iteration 110, the loss is 277.5198247729149, parameters k is -31.637517466077618 and b is -56.15678825105044\n",
      "Iteration 111, the loss is 277.1148584790883, parameters k is -31.5746711222041 and b is -56.146788251050445\n",
      "Iteration 112, the loss is 276.7098921852613, parameters k is -31.511824778330585 and b is -56.13678825105045\n",
      "Iteration 113, the loss is 276.30492589143444, parameters k is -31.448978434457068 and b is -56.12678825105045\n",
      "Iteration 114, the loss is 275.89995959760773, parameters k is -31.38613209058355 and b is -56.11678825105045\n",
      "Iteration 115, the loss is 275.49499330378086, parameters k is -31.323285746710035 and b is -56.10678825105045\n",
      "Iteration 116, the loss is 275.0900270099546, parameters k is -31.260439402836518 and b is -56.096788251050455\n",
      "Iteration 117, the loss is 274.6850607161273, parameters k is -31.197593058963 and b is -56.08678825105046\n",
      "Iteration 118, the loss is 274.2800944223002, parameters k is -31.134746715089484 and b is -56.07678825105046\n",
      "Iteration 119, the loss is 273.87512812847353, parameters k is -31.071900371215968 and b is -56.06678825105046\n",
      "Iteration 120, the loss is 273.4701618346464, parameters k is -31.00905402734245 and b is -56.05678825105046\n",
      "Iteration 121, the loss is 273.06519554082007, parameters k is -30.946207683468934 and b is -56.046788251050465\n",
      "Iteration 122, the loss is 272.660229246993, parameters k is -30.883361339595417 and b is -56.03678825105047\n",
      "Iteration 123, the loss is 272.25526295316627, parameters k is -30.8205149957219 and b is -56.02678825105047\n",
      "Iteration 124, the loss is 271.85029665933905, parameters k is -30.757668651848384 and b is -56.01678825105047\n",
      "Iteration 125, the loss is 271.44533036551235, parameters k is -30.694822307974867 and b is -56.00678825105047\n",
      "Iteration 126, the loss is 271.04036407168553, parameters k is -30.63197596410135 and b is -55.996788251050475\n",
      "Iteration 127, the loss is 270.6353977778589, parameters k is -30.569129620227834 and b is -55.98678825105048\n",
      "Iteration 128, the loss is 270.2304314840318, parameters k is -30.506283276354317 and b is -55.97678825105048\n",
      "Iteration 129, the loss is 269.82546519020536, parameters k is -30.4434369324808 and b is -55.96678825105048\n",
      "Iteration 130, the loss is 269.42049889637843, parameters k is -30.380590588607284 and b is -55.95678825105048\n",
      "Iteration 131, the loss is 269.0155326025512, parameters k is -30.317744244733767 and b is -55.946788251050485\n",
      "Iteration 132, the loss is 268.61056630872463, parameters k is -30.25489790086025 and b is -55.93678825105049\n",
      "Iteration 133, the loss is 268.20560001489775, parameters k is -30.192051556986733 and b is -55.92678825105049\n",
      "Iteration 134, the loss is 267.80063372107094, parameters k is -30.129205213113217 and b is -55.91678825105049\n",
      "Iteration 135, the loss is 267.39566742724423, parameters k is -30.0663588692397 and b is -55.90678825105049\n",
      "Iteration 136, the loss is 266.9907011334172, parameters k is -30.003512525366183 and b is -55.896788251050495\n",
      "Iteration 137, the loss is 266.58573483959054, parameters k is -29.940666181492666 and b is -55.8867882510505\n",
      "Iteration 138, the loss is 266.1807685457635, parameters k is -29.87781983761915 and b is -55.8767882510505\n",
      "Iteration 139, the loss is 265.77580225193685, parameters k is -29.814973493745633 and b is -55.8667882510505\n",
      "Iteration 140, the loss is 265.37083595810986, parameters k is -29.752127149872116 and b is -55.8567882510505\n",
      "Iteration 141, the loss is 264.96586966428316, parameters k is -29.6892808059986 and b is -55.846788251050505\n",
      "Iteration 142, the loss is 264.5609033704563, parameters k is -29.626434462125083 and b is -55.83678825105051\n",
      "Iteration 143, the loss is 264.1559370766296, parameters k is -29.563588118251566 and b is -55.82678825105051\n",
      "Iteration 144, the loss is 263.7509707828027, parameters k is -29.50074177437805 and b is -55.81678825105051\n",
      "Iteration 145, the loss is 263.34600448897567, parameters k is -29.437895430504533 and b is -55.80678825105051\n",
      "Iteration 146, the loss is 262.9410381951492, parameters k is -29.375049086631016 and b is -55.796788251050515\n",
      "Iteration 147, the loss is 262.5360719013221, parameters k is -29.3122027427575 and b is -55.78678825105052\n",
      "Iteration 148, the loss is 262.13110560749533, parameters k is -29.249356398883982 and b is -55.77678825105052\n",
      "Iteration 149, the loss is 261.7261393136684, parameters k is -29.186510055010466 and b is -55.76678825105052\n",
      "Iteration 150, the loss is 261.32117301984175, parameters k is -29.12366371113695 and b is -55.75678825105052\n",
      "Iteration 151, the loss is 260.91620672601476, parameters k is -29.060817367263432 and b is -55.746788251050525\n",
      "Iteration 152, the loss is 260.5112404321881, parameters k is -28.997971023389916 and b is -55.73678825105053\n",
      "Iteration 153, the loss is 260.10627413836113, parameters k is -28.9351246795164 and b is -55.72678825105053\n",
      "Iteration 154, the loss is 259.70130784453437, parameters k is -28.872278335642882 and b is -55.71678825105053\n",
      "Iteration 155, the loss is 259.2963415507073, parameters k is -28.809431991769365 and b is -55.70678825105053\n",
      "Iteration 156, the loss is 258.8913752568805, parameters k is -28.74658564789585 and b is -55.696788251050535\n",
      "Iteration 157, the loss is 258.48640896305363, parameters k is -28.683739304022332 and b is -55.68678825105054\n",
      "Iteration 158, the loss is 258.08144266922676, parameters k is -28.620892960148815 and b is -55.67678825105054\n",
      "Iteration 159, the loss is 257.67647637539994, parameters k is -28.5580466162753 and b is -55.66678825105054\n",
      "Iteration 160, the loss is 257.2715100815731, parameters k is -28.49520027240178 and b is -55.65678825105054\n",
      "Iteration 161, the loss is 256.8665437877464, parameters k is -28.432353928528265 and b is -55.646788251050545\n",
      "Iteration 162, the loss is 256.4615774939195, parameters k is -28.369507584654748 and b is -55.63678825105055\n",
      "Iteration 163, the loss is 256.05661120009256, parameters k is -28.30666124078123 and b is -55.62678825105055\n",
      "Iteration 164, the loss is 255.65164490626597, parameters k is -28.243814896907715 and b is -55.61678825105055\n",
      "Iteration 165, the loss is 255.24667861243935, parameters k is -28.180968553034198 and b is -55.60678825105055\n",
      "Iteration 166, the loss is 254.8417123186123, parameters k is -28.11812220916068 and b is -55.596788251050555\n",
      "Iteration 167, the loss is 254.4367460247856, parameters k is -28.055275865287165 and b is -55.58678825105056\n",
      "Iteration 168, the loss is 254.03177973095862, parameters k is -27.992429521413648 and b is -55.57678825105056\n",
      "Iteration 169, the loss is 253.62681343713174, parameters k is -27.92958317754013 and b is -55.56678825105056\n",
      "Iteration 170, the loss is 253.22184714330467, parameters k is -27.866736833666614 and b is -55.55678825105056\n",
      "Iteration 171, the loss is 252.8168808494781, parameters k is -27.803890489793098 and b is -55.546788251050565\n",
      "Iteration 172, the loss is 252.41191455565118, parameters k is -27.74104414591958 and b is -55.53678825105057\n",
      "Iteration 173, the loss is 252.0069482618244, parameters k is -27.678197802046064 and b is -55.52678825105057\n",
      "Iteration 174, the loss is 251.60198196799746, parameters k is -27.615351458172547 and b is -55.51678825105057\n",
      "Iteration 175, the loss is 251.1970156741707, parameters k is -27.55250511429903 and b is -55.50678825105057\n",
      "Iteration 176, the loss is 250.79204938034377, parameters k is -27.489658770425514 and b is -55.496788251050575\n",
      "Iteration 177, the loss is 250.38708308651675, parameters k is -27.426812426551997 and b is -55.48678825105058\n",
      "Iteration 178, the loss is 249.98211679269036, parameters k is -27.36396608267848 and b is -55.47678825105058\n",
      "Iteration 179, the loss is 249.5771504988636, parameters k is -27.301119738804964 and b is -55.46678825105058\n",
      "Iteration 180, the loss is 249.17218420503661, parameters k is -27.238273394931447 and b is -55.45678825105058\n",
      "Iteration 181, the loss is 248.76721791120977, parameters k is -27.17542705105793 and b is -55.446788251050585\n",
      "Iteration 182, the loss is 248.36225161738278, parameters k is -27.112580707184414 and b is -55.43678825105059\n",
      "Iteration 183, the loss is 247.95728532355588, parameters k is -27.049734363310897 and b is -55.42678825105059\n",
      "Iteration 184, the loss is 247.55231902972886, parameters k is -26.98688801943738 and b is -55.41678825105059\n",
      "Iteration 185, the loss is 247.1473527359024, parameters k is -26.924041675563863 and b is -55.40678825105059\n",
      "Iteration 186, the loss is 246.74238644207531, parameters k is -26.861195331690347 and b is -55.396788251050594\n",
      "Iteration 187, the loss is 246.3374201482485, parameters k is -26.79834898781683 and b is -55.386788251050596\n",
      "Iteration 188, the loss is 245.93245385442188, parameters k is -26.735502643943313 and b is -55.3767882510506\n",
      "Iteration 189, the loss is 245.52748756059492, parameters k is -26.672656300069796 and b is -55.3667882510506\n",
      "Iteration 190, the loss is 245.1225212667682, parameters k is -26.60980995619628 and b is -55.3567882510506\n",
      "Iteration 191, the loss is 244.7175549729413, parameters k is -26.546963612322763 and b is -55.346788251050604\n",
      "Iteration 192, the loss is 244.3125886791144, parameters k is -26.484117268449246 and b is -55.336788251050606\n",
      "Iteration 193, the loss is 243.90762238528757, parameters k is -26.42127092457573 and b is -55.32678825105061\n",
      "Iteration 194, the loss is 243.50265609146098, parameters k is -26.358424580702213 and b is -55.31678825105061\n",
      "Iteration 195, the loss is 243.09768979763402, parameters k is -26.295578236828696 and b is -55.30678825105061\n",
      "Iteration 196, the loss is 242.69272350380712, parameters k is -26.23273189295518 and b is -55.296788251050614\n",
      "Iteration 197, the loss is 242.28775720998019, parameters k is -26.169885549081663 and b is -55.286788251050616\n",
      "Iteration 198, the loss is 241.88279091615354, parameters k is -26.107039205208146 and b is -55.27678825105062\n",
      "Iteration 199, the loss is 241.47782462232678, parameters k is -26.04419286133463 and b is -55.26678825105062\n",
      "Iteration 200, the loss is 241.07285832849985, parameters k is -25.981346517461112 and b is -55.25678825105062\n",
      "Iteration 201, the loss is 240.66789203467286, parameters k is -25.918500173587596 and b is -55.246788251050624\n",
      "Iteration 202, the loss is 240.26292574084647, parameters k is -25.85565382971408 and b is -55.236788251050626\n",
      "Iteration 203, the loss is 239.8579594470192, parameters k is -25.792807485840562 and b is -55.22678825105063\n",
      "Iteration 204, the loss is 239.45299315319224, parameters k is -25.729961141967046 and b is -55.21678825105063\n",
      "Iteration 205, the loss is 239.04802685936568, parameters k is -25.66711479809353 and b is -55.20678825105063\n",
      "Iteration 206, the loss is 238.64306056553906, parameters k is -25.604268454220012 and b is -55.196788251050634\n",
      "Iteration 207, the loss is 238.23809427171187, parameters k is -25.541422110346495 and b is -55.186788251050636\n",
      "Iteration 208, the loss is 237.83312797788537, parameters k is -25.47857576647298 and b is -55.17678825105064\n",
      "Iteration 209, the loss is 237.4281616840582, parameters k is -25.415729422599462 and b is -55.16678825105064\n",
      "Iteration 210, the loss is 237.02319539023122, parameters k is -25.352883078725945 and b is -55.15678825105064\n",
      "Iteration 211, the loss is 236.61822909640435, parameters k is -25.29003673485243 and b is -55.146788251050644\n",
      "Iteration 212, the loss is 236.21326280257787, parameters k is -25.22719039097891 and b is -55.136788251050646\n",
      "Iteration 213, the loss is 235.80829650875086, parameters k is -25.164344047105395 and b is -55.12678825105065\n",
      "Iteration 214, the loss is 235.40333021492395, parameters k is -25.10149770323188 and b is -55.11678825105065\n",
      "Iteration 215, the loss is 234.99836392109717, parameters k is -25.03865135935836 and b is -55.10678825105065\n",
      "Iteration 216, the loss is 234.5933976272705, parameters k is -24.975805015484845 and b is -55.096788251050654\n",
      "Iteration 217, the loss is 234.1884313334436, parameters k is -24.912958671611328 and b is -55.086788251050656\n",
      "Iteration 218, the loss is 233.7834650396166, parameters k is -24.85011232773781 and b is -55.07678825105066\n",
      "Iteration 219, the loss is 233.3784987457898, parameters k is -24.787265983864295 and b is -55.06678825105066\n",
      "Iteration 220, the loss is 232.97353245196317, parameters k is -24.724419639990778 and b is -55.05678825105066\n",
      "Iteration 221, the loss is 232.5685661581362, parameters k is -24.66157329611726 and b is -55.046788251050664\n",
      "Iteration 222, the loss is 232.16359986430948, parameters k is -24.598726952243744 and b is -55.036788251050666\n",
      "Iteration 223, the loss is 231.75863357048246, parameters k is -24.535880608370228 and b is -55.02678825105067\n",
      "Iteration 224, the loss is 231.3536672766556, parameters k is -24.47303426449671 and b is -55.01678825105067\n",
      "Iteration 225, the loss is 230.94870098282902, parameters k is -24.410187920623194 and b is -55.00678825105067\n",
      "Iteration 226, the loss is 230.54373468900206, parameters k is -24.347341576749677 and b is -54.996788251050674\n",
      "Iteration 227, the loss is 230.13876839517516, parameters k is -24.28449523287616 and b is -54.986788251050676\n",
      "Iteration 228, the loss is 229.7338021013482, parameters k is -24.221648889002644 and b is -54.97678825105068\n",
      "Iteration 229, the loss is 229.3288358075215, parameters k is -24.158802545129127 and b is -54.96678825105068\n",
      "Iteration 230, the loss is 228.92386951369477, parameters k is -24.09595620125561 and b is -54.95678825105068\n",
      "Iteration 231, the loss is 228.5189032198676, parameters k is -24.033109857382094 and b is -54.946788251050684\n",
      "Iteration 232, the loss is 228.1139369260408, parameters k is -23.970263513508577 and b is -54.936788251050686\n",
      "Iteration 233, the loss is 227.7089706322141, parameters k is -23.90741716963506 and b is -54.92678825105069\n",
      "Iteration 234, the loss is 227.30400433838747, parameters k is -23.844570825761544 and b is -54.91678825105069\n",
      "Iteration 235, the loss is 226.89903804456048, parameters k is -23.781724481888027 and b is -54.90678825105069\n",
      "Iteration 236, the loss is 226.49407175073367, parameters k is -23.71887813801451 and b is -54.896788251050694\n",
      "Iteration 237, the loss is 226.08910545690688, parameters k is -23.656031794140993 and b is -54.886788251050696\n",
      "Iteration 238, the loss is 225.68413916307998, parameters k is -23.593185450267477 and b is -54.8767882510507\n",
      "Iteration 239, the loss is 225.27917286925307, parameters k is -23.53033910639396 and b is -54.8667882510507\n",
      "Iteration 240, the loss is 224.8742065754264, parameters k is -23.467492762520443 and b is -54.8567882510507\n",
      "Iteration 241, the loss is 224.46924028159953, parameters k is -23.404646418646927 and b is -54.846788251050704\n",
      "Iteration 242, the loss is 224.06427398777282, parameters k is -23.34180007477341 and b is -54.836788251050706\n",
      "Iteration 243, the loss is 223.65930769394575, parameters k is -23.278953730899893 and b is -54.82678825105071\n",
      "Iteration 244, the loss is 223.25434140011907, parameters k is -23.216107387026376 and b is -54.81678825105071\n",
      "Iteration 245, the loss is 222.8493751062923, parameters k is -23.15326104315286 and b is -54.80678825105071\n",
      "Iteration 246, the loss is 222.44440881246547, parameters k is -23.090414699279343 and b is -54.796788251050714\n",
      "Iteration 247, the loss is 222.0394425186385, parameters k is -23.027568355405826 and b is -54.786788251050716\n",
      "Iteration 248, the loss is 221.63447622481155, parameters k is -22.96472201153231 and b is -54.77678825105072\n",
      "Iteration 249, the loss is 221.22950993098448, parameters k is -22.901875667658793 and b is -54.76678825105072\n",
      "Iteration 250, the loss is 220.82454363715797, parameters k is -22.839029323785276 and b is -54.75678825105072\n",
      "Iteration 251, the loss is 220.41957734333133, parameters k is -22.77618297991176 and b is -54.746788251050724\n",
      "Iteration 252, the loss is 220.0146110495044, parameters k is -22.713336636038242 and b is -54.736788251050726\n",
      "Iteration 253, the loss is 219.60964475567746, parameters k is -22.650490292164726 and b is -54.72678825105073\n",
      "Iteration 254, the loss is 219.20467846185036, parameters k is -22.58764394829121 and b is -54.71678825105073\n",
      "Iteration 255, the loss is 218.7997121680236, parameters k is -22.524797604417692 and b is -54.70678825105073\n",
      "Iteration 256, the loss is 218.39474587419707, parameters k is -22.461951260544176 and b is -54.696788251050734\n",
      "Iteration 257, the loss is 217.98977958037014, parameters k is -22.39910491667066 and b is -54.686788251050736\n",
      "Iteration 258, the loss is 217.5848132865434, parameters k is -22.336258572797142 and b is -54.67678825105074\n",
      "Iteration 259, the loss is 217.17984699271616, parameters k is -22.273412228923625 and b is -54.66678825105074\n",
      "Iteration 260, the loss is 216.77488069888975, parameters k is -22.21056588505011 and b is -54.65678825105074\n",
      "Iteration 261, the loss is 216.36991440506299, parameters k is -22.147719541176592 and b is -54.646788251050744\n",
      "Iteration 262, the loss is 215.9649481112362, parameters k is -22.084873197303075 and b is -54.636788251050746\n",
      "Iteration 263, the loss is 215.55998181740927, parameters k is -22.02202685342956 and b is -54.62678825105075\n",
      "Iteration 264, the loss is 215.1550155235823, parameters k is -21.95918050955604 and b is -54.61678825105075\n",
      "Iteration 265, the loss is 214.75004922975515, parameters k is -21.896334165682525 and b is -54.60678825105075\n",
      "Iteration 266, the loss is 214.34508293592873, parameters k is -21.83348782180901 and b is -54.596788251050754\n",
      "Iteration 267, the loss is 213.94011664210194, parameters k is -21.77064147793549 and b is -54.586788251050756\n",
      "Iteration 268, the loss is 213.53515034827475, parameters k is -21.707795134061975 and b is -54.57678825105076\n",
      "Iteration 269, the loss is 213.13018405444828, parameters k is -21.644948790188458 and b is -54.56678825105076\n",
      "Iteration 270, the loss is 212.72521776062158, parameters k is -21.58210244631494 and b is -54.55678825105076\n",
      "Iteration 271, the loss is 212.32025146679456, parameters k is -21.519256102441425 and b is -54.546788251050764\n",
      "Iteration 272, the loss is 211.91528517296777, parameters k is -21.456409758567908 and b is -54.536788251050766\n",
      "Iteration 273, the loss is 211.51031887914064, parameters k is -21.39356341469439 and b is -54.52678825105077\n",
      "Iteration 274, the loss is 211.10535258531394, parameters k is -21.330717070820874 and b is -54.51678825105077\n",
      "Iteration 275, the loss is 210.70038629148723, parameters k is -21.267870726947358 and b is -54.50678825105077\n",
      "Iteration 276, the loss is 210.29541999766022, parameters k is -21.20502438307384 and b is -54.49678825105077\n",
      "Iteration 277, the loss is 209.8904537038334, parameters k is -21.142178039200324 and b is -54.486788251050776\n",
      "Iteration 278, the loss is 209.48548741000653, parameters k is -21.079331695326808 and b is -54.47678825105078\n",
      "Iteration 279, the loss is 209.08052111617974, parameters k is -21.01648535145329 and b is -54.46678825105078\n",
      "Iteration 280, the loss is 208.675554822353, parameters k is -20.953639007579774 and b is -54.45678825105078\n",
      "Iteration 281, the loss is 208.27058852852616, parameters k is -20.890792663706257 and b is -54.44678825105078\n",
      "Iteration 282, the loss is 207.8656222346993, parameters k is -20.82794631983274 and b is -54.436788251050785\n",
      "Iteration 283, the loss is 207.4606559408722, parameters k is -20.765099975959224 and b is -54.42678825105079\n",
      "Iteration 284, the loss is 207.05568964704548, parameters k is -20.702253632085707 and b is -54.41678825105079\n",
      "Iteration 285, the loss is 206.65072335321867, parameters k is -20.63940728821219 and b is -54.40678825105079\n",
      "Iteration 286, the loss is 206.24575705939174, parameters k is -20.576560944338674 and b is -54.39678825105079\n",
      "Iteration 287, the loss is 205.8407907655651, parameters k is -20.513714600465157 and b is -54.386788251050795\n",
      "Iteration 288, the loss is 205.4358244717381, parameters k is -20.45086825659164 and b is -54.3767882510508\n",
      "Iteration 289, the loss is 205.03085817791154, parameters k is -20.388021912718123 and b is -54.3667882510508\n",
      "Iteration 290, the loss is 204.62589188408467, parameters k is -20.325175568844607 and b is -54.3567882510508\n",
      "Iteration 291, the loss is 204.2209255902577, parameters k is -20.26232922497109 and b is -54.3467882510508\n",
      "Iteration 292, the loss is 203.81595929643092, parameters k is -20.199482881097573 and b is -54.336788251050805\n",
      "Iteration 293, the loss is 203.4109930026042, parameters k is -20.136636537224057 and b is -54.32678825105081\n",
      "Iteration 294, the loss is 203.00602670877723, parameters k is -20.07379019335054 and b is -54.31678825105081\n",
      "Iteration 295, the loss is 202.60106041495018, parameters k is -20.010943849477023 and b is -54.30678825105081\n",
      "Iteration 296, the loss is 202.19609412112354, parameters k is -19.948097505603506 and b is -54.29678825105081\n",
      "Iteration 297, the loss is 201.79112782729695, parameters k is -19.88525116172999 and b is -54.286788251050815\n",
      "Iteration 298, the loss is 201.3861615334699, parameters k is -19.822404817856473 and b is -54.27678825105082\n",
      "Iteration 299, the loss is 200.98119523964286, parameters k is -19.759558473982956 and b is -54.26678825105082\n",
      "Iteration 300, the loss is 200.57622894581627, parameters k is -19.69671213010944 and b is -54.25678825105082\n",
      "Iteration 301, the loss is 200.17126265198945, parameters k is -19.633865786235923 and b is -54.24678825105082\n",
      "Iteration 302, the loss is 199.76629635816255, parameters k is -19.571019442362406 and b is -54.236788251050825\n",
      "Iteration 303, the loss is 199.3613300643358, parameters k is -19.50817309848889 and b is -54.22678825105083\n",
      "Iteration 304, the loss is 198.9563637705086, parameters k is -19.445326754615373 and b is -54.21678825105083\n",
      "Iteration 305, the loss is 198.5513974766819, parameters k is -19.382480410741856 and b is -54.20678825105083\n",
      "Iteration 306, the loss is 198.14643118285525, parameters k is -19.31963406686834 and b is -54.19678825105083\n",
      "Iteration 307, the loss is 197.74146488902835, parameters k is -19.256787722994822 and b is -54.186788251050835\n",
      "Iteration 308, the loss is 197.3364985952013, parameters k is -19.193941379121306 and b is -54.17678825105084\n",
      "Iteration 309, the loss is 196.93153230137457, parameters k is -19.13109503524779 and b is -54.16678825105084\n",
      "Iteration 310, the loss is 196.526566007548, parameters k is -19.068248691374272 and b is -54.15678825105084\n",
      "Iteration 311, the loss is 196.12159971372117, parameters k is -19.005402347500755 and b is -54.14678825105084\n",
      "Iteration 312, the loss is 195.71663341989412, parameters k is -18.94255600362724 and b is -54.136788251050845\n",
      "Iteration 313, the loss is 195.31166712606756, parameters k is -18.879709659753722 and b is -54.12678825105085\n",
      "Iteration 314, the loss is 194.90670083224035, parameters k is -18.816863315880205 and b is -54.11678825105085\n",
      "Iteration 315, the loss is 194.50173453841361, parameters k is -18.75401697200669 and b is -54.10678825105085\n",
      "Iteration 316, the loss is 194.09676824458688, parameters k is -18.69117062813317 and b is -54.09678825105085\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 317, the loss is 193.69180195075995, parameters k is -18.628324284259655 and b is -54.086788251050855\n",
      "Iteration 318, the loss is 193.2868356569329, parameters k is -18.56547794038614 and b is -54.07678825105086\n",
      "Iteration 319, the loss is 192.8818693631062, parameters k is -18.50263159651262 and b is -54.06678825105086\n",
      "Iteration 320, the loss is 192.47690306927933, parameters k is -18.439785252639105 and b is -54.05678825105086\n",
      "Iteration 321, the loss is 192.0719367754524, parameters k is -18.376938908765588 and b is -54.04678825105086\n",
      "Iteration 322, the loss is 191.6669704816258, parameters k is -18.31409256489207 and b is -54.036788251050865\n",
      "Iteration 323, the loss is 191.2620041877989, parameters k is -18.251246221018555 and b is -54.02678825105087\n",
      "Iteration 324, the loss is 190.85703789397226, parameters k is -18.188399877145038 and b is -54.01678825105087\n",
      "Iteration 325, the loss is 190.45207160014513, parameters k is -18.12555353327152 and b is -54.00678825105087\n",
      "Iteration 326, the loss is 190.04710530631854, parameters k is -18.062707189398004 and b is -53.99678825105087\n",
      "Iteration 327, the loss is 189.6421390124919, parameters k is -17.999860845524488 and b is -53.986788251050875\n",
      "Iteration 328, the loss is 189.23717271866465, parameters k is -17.93701450165097 and b is -53.97678825105088\n",
      "Iteration 329, the loss is 188.8322064248379, parameters k is -17.874168157777454 and b is -53.96678825105088\n",
      "Iteration 330, the loss is 188.42724013101108, parameters k is -17.811321813903938 and b is -53.95678825105088\n",
      "Iteration 331, the loss is 188.0222738371842, parameters k is -17.74847547003042 and b is -53.94678825105088\n",
      "Iteration 332, the loss is 187.6173075433573, parameters k is -17.685629126156904 and b is -53.936788251050885\n",
      "Iteration 333, the loss is 187.21234124953088, parameters k is -17.622782782283387 and b is -53.92678825105089\n",
      "Iteration 334, the loss is 186.8073749557038, parameters k is -17.55993643840987 and b is -53.91678825105089\n",
      "Iteration 335, the loss is 186.4024086618768, parameters k is -17.497090094536354 and b is -53.90678825105089\n",
      "Iteration 336, the loss is 185.99744236804997, parameters k is -17.434243750662837 and b is -53.89678825105089\n",
      "Iteration 337, the loss is 185.59247607422333, parameters k is -17.37139740678932 and b is -53.886788251050895\n",
      "Iteration 338, the loss is 185.18750978039637, parameters k is -17.308551062915804 and b is -53.8767882510509\n",
      "Iteration 339, the loss is 184.78254348656975, parameters k is -17.245704719042287 and b is -53.8667882510509\n",
      "Iteration 340, the loss is 184.37757719274248, parameters k is -17.18285837516877 and b is -53.8567882510509\n",
      "Iteration 341, the loss is 183.97261089891572, parameters k is -17.120012031295254 and b is -53.8467882510509\n",
      "Iteration 342, the loss is 183.56764460508938, parameters k is -17.057165687421737 and b is -53.836788251050905\n",
      "Iteration 343, the loss is 183.16267831126203, parameters k is -16.99431934354822 and b is -53.82678825105091\n",
      "Iteration 344, the loss is 182.75771201743532, parameters k is -16.931472999674703 and b is -53.81678825105091\n",
      "Iteration 345, the loss is 182.3527457236083, parameters k is -16.868626655801187 and b is -53.80678825105091\n",
      "Iteration 346, the loss is 181.94777942978175, parameters k is -16.80578031192767 and b is -53.79678825105091\n",
      "Iteration 347, the loss is 181.54281313595484, parameters k is -16.742933968054153 and b is -53.786788251050915\n",
      "Iteration 348, the loss is 181.137846842128, parameters k is -16.680087624180636 and b is -53.77678825105092\n",
      "Iteration 349, the loss is 180.7328805483013, parameters k is -16.61724128030712 and b is -53.76678825105092\n",
      "Iteration 350, the loss is 180.3279142544744, parameters k is -16.554394936433603 and b is -53.75678825105092\n",
      "Iteration 351, the loss is 179.92294796064746, parameters k is -16.491548592560086 and b is -53.74678825105092\n",
      "Iteration 352, the loss is 179.5179816668207, parameters k is -16.42870224868657 and b is -53.736788251050925\n",
      "Iteration 353, the loss is 179.11301537299386, parameters k is -16.365855904813053 and b is -53.72678825105093\n",
      "Iteration 354, the loss is 178.7080490791671, parameters k is -16.303009560939536 and b is -53.71678825105093\n",
      "Iteration 355, the loss is 178.30308278534008, parameters k is -16.24016321706602 and b is -53.70678825105093\n",
      "Iteration 356, the loss is 177.8981164915132, parameters k is -16.177316873192503 and b is -53.69678825105093\n",
      "Iteration 357, the loss is 177.4931501976867, parameters k is -16.114470529318986 and b is -53.686788251050935\n",
      "Iteration 358, the loss is 177.08818390385994, parameters k is -16.05162418544547 and b is -53.67678825105094\n",
      "Iteration 359, the loss is 176.68321761003284, parameters k is -15.98877784157195 and b is -53.66678825105094\n",
      "Iteration 360, the loss is 176.2782513162058, parameters k is -15.925931497698432 and b is -53.65678825105094\n",
      "Iteration 361, the loss is 175.87328502237915, parameters k is -15.863085153824914 and b is -53.64678825105094\n",
      "Iteration 362, the loss is 175.46831872855222, parameters k is -15.800238809951395 and b is -53.636788251050945\n",
      "Iteration 363, the loss is 175.06335243472543, parameters k is -15.737392466077877 and b is -53.62678825105095\n",
      "Iteration 364, the loss is 174.65838614089856, parameters k is -15.674546122204358 and b is -53.61678825105095\n",
      "Iteration 365, the loss is 174.25341984707183, parameters k is -15.61169977833084 and b is -53.60678825105095\n",
      "Iteration 366, the loss is 173.84845355324475, parameters k is -15.548853434457321 and b is -53.59678825105095\n",
      "Iteration 367, the loss is 173.44348725941796, parameters k is -15.486007090583803 and b is -53.586788251050955\n",
      "Iteration 368, the loss is 173.03852096559143, parameters k is -15.423160746710284 and b is -53.57678825105096\n",
      "Iteration 369, the loss is 172.63355467176453, parameters k is -15.360314402836766 and b is -53.56678825105096\n",
      "Iteration 370, the loss is 172.2285883779376, parameters k is -15.297468058963247 and b is -53.55678825105096\n",
      "Iteration 371, the loss is 171.8236220841107, parameters k is -15.234621715089729 and b is -53.54678825105096\n",
      "Iteration 372, the loss is 171.41865579028382, parameters k is -15.17177537121621 and b is -53.536788251050965\n",
      "Iteration 373, the loss is 171.01368949645703, parameters k is -15.108929027342691 and b is -53.52678825105097\n",
      "Iteration 374, the loss is 170.60872320263022, parameters k is -15.046082683469173 and b is -53.51678825105097\n",
      "Iteration 375, the loss is 170.2037569088035, parameters k is -14.983236339595654 and b is -53.50678825105097\n",
      "Iteration 376, the loss is 169.79879061497653, parameters k is -14.920389995722136 and b is -53.49678825105097\n",
      "Iteration 377, the loss is 169.39382432114945, parameters k is -14.857543651848617 and b is -53.486788251050974\n",
      "Iteration 378, the loss is 168.9888580273226, parameters k is -14.794697307975099 and b is -53.476788251050976\n",
      "Iteration 379, the loss is 168.58389173349596, parameters k is -14.73185096410158 and b is -53.46678825105098\n",
      "Iteration 380, the loss is 168.17892543966906, parameters k is -14.669004620228062 and b is -53.45678825105098\n",
      "Iteration 381, the loss is 167.7739591458424, parameters k is -14.606158276354543 and b is -53.44678825105098\n",
      "Iteration 382, the loss is 167.3689928520153, parameters k is -14.543311932481025 and b is -53.436788251050984\n",
      "Iteration 383, the loss is 166.96402655818846, parameters k is -14.480465588607506 and b is -53.426788251050986\n",
      "Iteration 384, the loss is 166.55906026436176, parameters k is -14.417619244733988 and b is -53.41678825105099\n",
      "Iteration 385, the loss is 166.15409397053492, parameters k is -14.35477290086047 and b is -53.40678825105099\n",
      "Iteration 386, the loss is 165.74912767670804, parameters k is -14.291926556986951 and b is -53.39678825105099\n",
      "Iteration 387, the loss is 165.34416138288114, parameters k is -14.229080213113432 and b is -53.386788251050994\n",
      "Iteration 388, the loss is 164.93919508905418, parameters k is -14.166233869239914 and b is -53.376788251050996\n",
      "Iteration 389, the loss is 164.53422879522734, parameters k is -14.103387525366395 and b is -53.366788251051\n",
      "Iteration 390, the loss is 164.1292625014006, parameters k is -14.040541181492877 and b is -53.356788251051\n",
      "Iteration 391, the loss is 163.72429620757373, parameters k is -13.977694837619358 and b is -53.346788251051\n",
      "Iteration 392, the loss is 163.3193299137468, parameters k is -13.91484849374584 and b is -53.336788251051004\n",
      "Iteration 393, the loss is 162.91436361992004, parameters k is -13.852002149872321 and b is -53.326788251051006\n",
      "Iteration 394, the loss is 162.5093973260934, parameters k is -13.789155805998803 and b is -53.31678825105101\n",
      "Iteration 395, the loss is 162.1044310322663, parameters k is -13.726309462125284 and b is -53.30678825105101\n",
      "Iteration 396, the loss is 161.69946473843956, parameters k is -13.663463118251766 and b is -53.29678825105101\n",
      "Iteration 397, the loss is 161.2944984446128, parameters k is -13.600616774378247 and b is -53.286788251051014\n",
      "Iteration 398, the loss is 160.88953215078578, parameters k is -13.537770430504729 and b is -53.276788251051016\n",
      "Iteration 399, the loss is 160.48456585695885, parameters k is -13.47492408663121 and b is -53.26678825105102\n",
      "Iteration 400, the loss is 160.07959956313226, parameters k is -13.412077742757692 and b is -53.25678825105102\n",
      "Iteration 401, the loss is 159.6746332693053, parameters k is -13.349231398884173 and b is -53.24678825105102\n",
      "Iteration 402, the loss is 159.26966697547837, parameters k is -13.286385055010655 and b is -53.236788251051024\n",
      "Iteration 403, the loss is 158.86470068165167, parameters k is -13.223538711137136 and b is -53.226788251051026\n",
      "Iteration 404, the loss is 158.45973438782465, parameters k is -13.160692367263618 and b is -53.21678825105103\n",
      "Iteration 405, the loss is 158.0547680939979, parameters k is -13.0978460233901 and b is -53.20678825105103\n",
      "Iteration 406, the loss is 157.64980180017113, parameters k is -13.03499967951658 and b is -53.19678825105103\n",
      "Iteration 407, the loss is 157.24483550634434, parameters k is -12.972153335643062 and b is -53.186788251051034\n",
      "Iteration 408, the loss is 156.8398692125173, parameters k is -12.909306991769544 and b is -53.176788251051036\n",
      "Iteration 409, the loss is 156.4349029186905, parameters k is -12.846460647896025 and b is -53.16678825105104\n",
      "Iteration 410, the loss is 156.02993662486372, parameters k is -12.783614304022507 and b is -53.15678825105104\n",
      "Iteration 411, the loss is 155.62497033103688, parameters k is -12.720767960148988 and b is -53.14678825105104\n",
      "Iteration 412, the loss is 155.22000403720992, parameters k is -12.65792161627547 and b is -53.136788251051044\n",
      "Iteration 413, the loss is 154.8150377433831, parameters k is -12.595075272401951 and b is -53.126788251051046\n",
      "Iteration 414, the loss is 154.41007144955628, parameters k is -12.532228928528433 and b is -53.11678825105105\n",
      "Iteration 415, the loss is 154.00510515572924, parameters k is -12.469382584654914 and b is -53.10678825105105\n",
      "Iteration 416, the loss is 153.60013886190237, parameters k is -12.406536240781396 and b is -53.09678825105105\n",
      "Iteration 417, the loss is 153.1951725680754, parameters k is -12.343689896907877 and b is -53.086788251051054\n",
      "Iteration 418, the loss is 152.7902062742488, parameters k is -12.280843553034359 and b is -53.076788251051056\n",
      "Iteration 419, the loss is 152.38523998042209, parameters k is -12.21799720916084 and b is -53.06678825105106\n",
      "Iteration 420, the loss is 151.9802736865952, parameters k is -12.155150865287322 and b is -53.05678825105106\n",
      "Iteration 421, the loss is 151.57530739276842, parameters k is -12.092304521413803 and b is -53.04678825105106\n",
      "Iteration 422, the loss is 151.1703410989415, parameters k is -12.029458177540285 and b is -53.036788251051064\n",
      "Iteration 423, the loss is 150.76537480511442, parameters k is -11.966611833666766 and b is -53.026788251051066\n",
      "Iteration 424, the loss is 150.3604085112878, parameters k is -11.903765489793248 and b is -53.01678825105107\n",
      "Iteration 425, the loss is 149.9554422174609, parameters k is -11.84091914591973 and b is -53.00678825105107\n",
      "Iteration 426, the loss is 149.55047592363405, parameters k is -11.77807280204621 and b is -52.99678825105107\n",
      "Iteration 427, the loss is 149.14550962980718, parameters k is -11.715226458172692 and b is -52.986788251051074\n",
      "Iteration 428, the loss is 148.74054333598048, parameters k is -11.652380114299174 and b is -52.976788251051076\n",
      "Iteration 429, the loss is 148.33557704215352, parameters k is -11.589533770425655 and b is -52.96678825105108\n",
      "Iteration 430, the loss is 147.93061074832647, parameters k is -11.526687426552137 and b is -52.95678825105108\n",
      "Iteration 431, the loss is 147.52564445449983, parameters k is -11.463841082678618 and b is -52.94678825105108\n",
      "Iteration 432, the loss is 147.12067816067304, parameters k is -11.4009947388051 and b is -52.936788251051084\n",
      "Iteration 433, the loss is 146.71571186684602, parameters k is -11.338148394931581 and b is -52.926788251051086\n",
      "Iteration 434, the loss is 146.31074557301923, parameters k is -11.275302051058063 and b is -52.91678825105109\n",
      "Iteration 435, the loss is 145.90577927919236, parameters k is -11.212455707184544 and b is -52.90678825105109\n",
      "Iteration 436, the loss is 145.50081298536568, parameters k is -11.149609363311026 and b is -52.89678825105109\n",
      "Iteration 437, the loss is 145.0958466915388, parameters k is -11.086763019437507 and b is -52.886788251051094\n",
      "Iteration 438, the loss is 144.69088039771174, parameters k is -11.023916675563989 and b is -52.876788251051096\n",
      "Iteration 439, the loss is 144.285914103885, parameters k is -10.96107033169047 and b is -52.8667882510511\n",
      "Iteration 440, the loss is 143.88094781005822, parameters k is -10.898223987816952 and b is -52.8567882510511\n",
      "Iteration 441, the loss is 143.47598151623134, parameters k is -10.835377643943433 and b is -52.8467882510511\n",
      "Iteration 442, the loss is 143.07101522240453, parameters k is -10.772531300069915 and b is -52.836788251051104\n",
      "Iteration 443, the loss is 142.66604892857765, parameters k is -10.709684956196396 and b is -52.826788251051106\n",
      "Iteration 444, the loss is 142.26108263475084, parameters k is -10.646838612322878 and b is -52.81678825105111\n",
      "Iteration 445, the loss is 141.856116340924, parameters k is -10.583992268449359 and b is -52.80678825105111\n",
      "Iteration 446, the loss is 141.45115004709731, parameters k is -10.52114592457584 and b is -52.79678825105111\n",
      "Iteration 447, the loss is 141.04618375327033, parameters k is -10.458299580702322 and b is -52.786788251051114\n",
      "Iteration 448, the loss is 140.64121745944348, parameters k is -10.395453236828804 and b is -52.776788251051116\n",
      "Iteration 449, the loss is 140.23625116561658, parameters k is -10.332606892955285 and b is -52.76678825105112\n",
      "Iteration 450, the loss is 139.83128487178976, parameters k is -10.269760549081766 and b is -52.75678825105112\n",
      "Iteration 451, the loss is 139.4263185779629, parameters k is -10.206914205208248 and b is -52.74678825105112\n",
      "Iteration 452, the loss is 139.02135228413613, parameters k is -10.14406786133473 and b is -52.736788251051124\n",
      "Iteration 453, the loss is 138.6163859903092, parameters k is -10.081221517461211 and b is -52.726788251051126\n",
      "Iteration 454, the loss is 138.21141969648232, parameters k is -10.018375173587692 and b is -52.71678825105113\n",
      "Iteration 455, the loss is 137.80645340265562, parameters k is -9.955528829714174 and b is -52.70678825105113\n",
      "Iteration 456, the loss is 137.40148710882858, parameters k is -9.892682485840655 and b is -52.69678825105113\n",
      "Iteration 457, the loss is 136.9965208150018, parameters k is -9.829836141967137 and b is -52.686788251051134\n",
      "Iteration 458, the loss is 136.59155452117483, parameters k is -9.766989798093618 and b is -52.676788251051136\n",
      "Iteration 459, the loss is 136.186588227348, parameters k is -9.7041434542201 and b is -52.66678825105114\n",
      "Iteration 460, the loss is 135.78162193352128, parameters k is -9.641297110346581 and b is -52.65678825105114\n",
      "Iteration 461, the loss is 135.37665563969435, parameters k is -9.578450766473063 and b is -52.64678825105114\n",
      "Iteration 462, the loss is 134.97168934586756, parameters k is -9.515604422599544 and b is -52.636788251051144\n",
      "Iteration 463, the loss is 134.56672305204077, parameters k is -9.452758078726026 and b is -52.626788251051146\n",
      "Iteration 464, the loss is 134.16175675821387, parameters k is -9.389911734852507 and b is -52.61678825105115\n",
      "Iteration 465, the loss is 133.75679046438705, parameters k is -9.327065390978989 and b is -52.60678825105115\n",
      "Iteration 466, the loss is 133.35182417056018, parameters k is -9.26421904710547 and b is -52.59678825105115\n",
      "Iteration 467, the loss is 132.9468578767333, parameters k is -9.201372703231952 and b is -52.58678825105115\n",
      "Iteration 468, the loss is 132.54189158290654, parameters k is -9.138526359358433 and b is -52.576788251051155\n",
      "Iteration 469, the loss is 132.1369252890796, parameters k is -9.075680015484915 and b is -52.56678825105116\n",
      "Iteration 470, the loss is 131.73195899525277, parameters k is -9.012833671611396 and b is -52.55678825105116\n",
      "Iteration 471, the loss is 131.3269927014261, parameters k is -8.949987327737878 and b is -52.54678825105116\n",
      "Iteration 472, the loss is 130.922026407599, parameters k is -8.88714098386436 and b is -52.53678825105116\n",
      "Iteration 473, the loss is 130.5170601137723, parameters k is -8.82429463999084 and b is -52.526788251051165\n",
      "Iteration 474, the loss is 130.11209381994527, parameters k is -8.761448296117322 and b is -52.51678825105117\n",
      "Iteration 475, the loss is 129.70712752611857, parameters k is -8.698601952243804 and b is -52.50678825105117\n",
      "Iteration 476, the loss is 129.30216123229178, parameters k is -8.635755608370285 and b is -52.49678825105117\n",
      "Iteration 477, the loss is 128.89719493846488, parameters k is -8.572909264496767 and b is -52.48678825105117\n",
      "Iteration 478, the loss is 128.49222864463803, parameters k is -8.510062920623248 and b is -52.476788251051175\n",
      "Iteration 479, the loss is 128.08726235081113, parameters k is -8.44721657674973 and b is -52.46678825105118\n",
      "Iteration 480, the loss is 127.68229605698433, parameters k is -8.384370232876211 and b is -52.45678825105118\n",
      "Iteration 481, the loss is 127.27732976315751, parameters k is -8.321523889002693 and b is -52.44678825105118\n",
      "Iteration 482, the loss is 126.87236346933071, parameters k is -8.258677545129174 and b is -52.43678825105118\n",
      "Iteration 483, the loss is 126.4673971755038, parameters k is -8.195831201255656 and b is -52.426788251051185\n",
      "Iteration 484, the loss is 126.0624308816769, parameters k is -8.132984857382137 and b is -52.41678825105119\n",
      "Iteration 485, the loss is 125.6574645878501, parameters k is -8.070138513508619 and b is -52.40678825105119\n",
      "Iteration 486, the loss is 125.25249829402335, parameters k is -8.0072921696351 and b is -52.39678825105119\n",
      "Iteration 487, the loss is 124.8475320001964, parameters k is -7.944445825761583 and b is -52.38678825105119\n",
      "Iteration 488, the loss is 124.44256570636963, parameters k is -7.881599481888065 and b is -52.376788251051195\n",
      "Iteration 489, the loss is 124.03759941254272, parameters k is -7.818753138014547 and b is -52.3667882510512\n",
      "Iteration 490, the loss is 123.6326331187159, parameters k is -7.75590679414103 and b is -52.3567882510512\n",
      "Iteration 491, the loss is 123.22766682488897, parameters k is -7.693060450267512 and b is -52.3467882510512\n",
      "Iteration 492, the loss is 122.8227005310622, parameters k is -7.6302141063939946 and b is -52.3367882510512\n",
      "Iteration 493, the loss is 122.41773423723531, parameters k is -7.567367762520477 and b is -52.326788251051205\n",
      "Iteration 494, the loss is 122.01276794340849, parameters k is -7.504521418646959 and b is -52.31678825105121\n",
      "Iteration 495, the loss is 121.60780164958166, parameters k is -7.441675074773442 and b is -52.30678825105121\n",
      "Iteration 496, the loss is 121.20283535575479, parameters k is -7.378828730899924 and b is -52.29678825105121\n",
      "Iteration 497, the loss is 120.79786906192801, parameters k is -7.3159823870264065 and b is -52.28678825105121\n",
      "Iteration 498, the loss is 120.39290276810124, parameters k is -7.253136043152889 and b is -52.276788251051215\n",
      "Iteration 499, the loss is 119.98793647427432, parameters k is -7.190289699279371 and b is -52.26678825105122\n",
      "Iteration 500, the loss is 119.58297018044756, parameters k is -7.127443355405854 and b is -52.25678825105122\n",
      "Iteration 501, the loss is 119.1780038866207, parameters k is -7.064597011532336 and b is -52.24678825105122\n",
      "Iteration 502, the loss is 118.77303759279386, parameters k is -7.001750667658818 and b is -52.23678825105122\n",
      "Iteration 503, the loss is 118.36807129896705, parameters k is -6.938904323785301 and b is -52.226788251051225\n",
      "Iteration 504, the loss is 117.96310500514005, parameters k is -6.876057979911783 and b is -52.21678825105123\n",
      "Iteration 505, the loss is 117.55813871131322, parameters k is -6.8132116360382655 and b is -52.20678825105123\n",
      "Iteration 506, the loss is 117.15317241748635, parameters k is -6.750365292164748 and b is -52.19678825105123\n",
      "Iteration 507, the loss is 116.74820612365974, parameters k is -6.68751894829123 and b is -52.18678825105123\n",
      "Iteration 508, the loss is 116.34323982983277, parameters k is -6.624672604417713 and b is -52.176788251051235\n",
      "Iteration 509, the loss is 115.93827353600588, parameters k is -6.561826260544195 and b is -52.16678825105124\n",
      "Iteration 510, the loss is 115.53330724217898, parameters k is -6.498979916670677 and b is -52.15678825105124\n",
      "Iteration 511, the loss is 115.12834094835227, parameters k is -6.43613357279716 and b is -52.14678825105124\n",
      "Iteration 512, the loss is 114.7233746545255, parameters k is -6.373287228923642 and b is -52.13678825105124\n",
      "Iteration 513, the loss is 114.31840836069843, parameters k is -6.310440885050125 and b is -52.126788251051245\n",
      "Iteration 514, the loss is 113.91344206687165, parameters k is -6.247594541176607 and b is -52.11678825105125\n",
      "Iteration 515, the loss is 113.5084757730449, parameters k is -6.184748197303089 and b is -52.10678825105125\n",
      "Iteration 516, the loss is 113.10350947921782, parameters k is -6.121901853429572 and b is -52.09678825105125\n",
      "Iteration 517, the loss is 112.69854318539126, parameters k is -6.059055509556054 and b is -52.08678825105125\n",
      "Iteration 518, the loss is 112.2935768915643, parameters k is -5.9962091656825365 and b is -52.076788251051255\n",
      "Iteration 519, the loss is 111.88861059773747, parameters k is -5.933362821809019 and b is -52.06678825105126\n",
      "Iteration 520, the loss is 111.48364430391052, parameters k is -5.870516477935501 and b is -52.05678825105126\n",
      "Iteration 521, the loss is 111.07867801008378, parameters k is -5.807670134061984 and b is -52.04678825105126\n",
      "Iteration 522, the loss is 110.67371171625702, parameters k is -5.744823790188466 and b is -52.03678825105126\n",
      "Iteration 523, the loss is 110.26874542243009, parameters k is -5.681977446314948 and b is -52.026788251051265\n",
      "Iteration 524, the loss is 109.86377912860333, parameters k is -5.619131102441431 and b is -52.01678825105127\n",
      "Iteration 525, the loss is 109.45881283477648, parameters k is -5.556284758567913 and b is -52.00678825105127\n",
      "Iteration 526, the loss is 109.05384654094955, parameters k is -5.4934384146943955 and b is -51.99678825105127\n",
      "Iteration 527, the loss is 108.6488802471227, parameters k is -5.430592070820878 and b is -51.98678825105127\n",
      "Iteration 528, the loss is 108.2439139532958, parameters k is -5.36774572694736 and b is -51.976788251051275\n",
      "Iteration 529, the loss is 107.83894765946884, parameters k is -5.304899383073843 and b is -51.96678825105128\n",
      "Iteration 530, the loss is 107.43398136564221, parameters k is -5.242053039200325 and b is -51.95678825105128\n",
      "Iteration 531, the loss is 107.0290150718153, parameters k is -5.179206695326807 and b is -51.94678825105128\n",
      "Iteration 532, the loss is 106.6240487779885, parameters k is -5.11636035145329 and b is -51.93678825105128\n",
      "Iteration 533, the loss is 106.21908248416172, parameters k is -5.053514007579772 and b is -51.926788251051285\n",
      "Iteration 534, the loss is 105.81411619033496, parameters k is -4.990667663706255 and b is -51.91678825105129\n",
      "Iteration 535, the loss is 105.40914989650796, parameters k is -4.927821319832737 and b is -51.90678825105129\n",
      "Iteration 536, the loss is 105.00418360268112, parameters k is -4.864974975959219 and b is -51.89678825105129\n",
      "Iteration 537, the loss is 104.59921730885436, parameters k is -4.802128632085702 and b is -51.88678825105129\n",
      "Iteration 538, the loss is 104.19425101502742, parameters k is -4.739282288212184 and b is -51.876788251051295\n",
      "Iteration 539, the loss is 103.78928472120067, parameters k is -4.6764359443386665 and b is -51.8667882510513\n",
      "Iteration 540, the loss is 103.38431842737386, parameters k is -4.613589600465149 and b is -51.8567882510513\n",
      "Iteration 541, the loss is 102.97935213354693, parameters k is -4.550743256591631 and b is -51.8467882510513\n",
      "Iteration 542, the loss is 102.57438583972014, parameters k is -4.487896912718114 and b is -51.8367882510513\n",
      "Iteration 543, the loss is 102.16941954589323, parameters k is -4.425050568844596 and b is -51.826788251051305\n",
      "Iteration 544, the loss is 101.76445325206637, parameters k is -4.362204224971078 and b is -51.81678825105131\n",
      "Iteration 545, the loss is 101.35948695823963, parameters k is -4.299357881097561 and b is -51.80678825105131\n",
      "Iteration 546, the loss is 100.95452066441277, parameters k is -4.236511537224043 and b is -51.79678825105131\n",
      "Iteration 547, the loss is 100.54955437058597, parameters k is -4.1736651933505255 and b is -51.78678825105131\n",
      "Iteration 548, the loss is 100.14458807675899, parameters k is -4.110818849477008 and b is -51.776788251051315\n",
      "Iteration 549, the loss is 99.73962178293223, parameters k is -4.04797250560349 and b is -51.76678825105132\n",
      "Iteration 550, the loss is 99.33465548910532, parameters k is -3.9851261617299722 and b is -51.75678825105132\n",
      "Iteration 551, the loss is 98.92968919527858, parameters k is -3.922279817856454 and b is -51.74678825105132\n",
      "Iteration 552, the loss is 98.52472290145168, parameters k is -3.859433473982936 and b is -51.73678825105132\n",
      "Iteration 553, the loss is 98.11975660762478, parameters k is -3.796587130109418 and b is -51.726788251051325\n",
      "Iteration 554, the loss is 97.71479031379803, parameters k is -3.7337407862359 and b is -51.71678825105133\n",
      "Iteration 555, the loss is 97.30982401997126, parameters k is -3.670894442362382 and b is -51.70678825105133\n",
      "Iteration 556, the loss is 96.90485772614437, parameters k is -3.608048098488864 and b is -51.69678825105133\n",
      "Iteration 557, the loss is 96.4998914323175, parameters k is -3.545201754615346 and b is -51.68678825105133\n",
      "Iteration 558, the loss is 96.09492513849058, parameters k is -3.4823554107418278 and b is -51.676788251051335\n",
      "Iteration 559, the loss is 95.68995884466368, parameters k is -3.4195090668683097 and b is -51.66678825105134\n",
      "Iteration 560, the loss is 95.28499255083695, parameters k is -3.3566627229947916 and b is -51.65678825105134\n",
      "Iteration 561, the loss is 94.88002625701007, parameters k is -3.2938163791212736 and b is -51.64678825105134\n",
      "Iteration 562, the loss is 94.47505996318324, parameters k is -3.2309700352477555 and b is -51.63678825105134\n",
      "Iteration 563, the loss is 94.07009366935651, parameters k is -3.1681236913742374 and b is -51.626788251051345\n",
      "Iteration 564, the loss is 93.6651273755295, parameters k is -3.1052773475007194 and b is -51.61678825105135\n",
      "Iteration 565, the loss is 93.26016108170272, parameters k is -3.0424310036272013 and b is -51.60678825105135\n",
      "Iteration 566, the loss is 92.8551947878759, parameters k is -2.9795846597536833 and b is -51.59678825105135\n",
      "Iteration 567, the loss is 92.45022849404897, parameters k is -2.916738315880165 and b is -51.58678825105135\n",
      "Iteration 568, the loss is 92.0452622002222, parameters k is -2.853891972006647 and b is -51.576788251051354\n",
      "Iteration 569, the loss is 91.6402959063952, parameters k is -2.791045628133129 and b is -51.566788251051356\n",
      "Iteration 570, the loss is 91.23532961256844, parameters k is -2.728199284259611 and b is -51.55678825105136\n",
      "Iteration 571, the loss is 90.8303633187416, parameters k is -2.665352940386093 and b is -51.54678825105136\n",
      "Iteration 572, the loss is 90.42539702491467, parameters k is -2.602506596512575 and b is -51.53678825105136\n",
      "Iteration 573, the loss is 90.02043073108807, parameters k is -2.539660252639057 and b is -51.526788251051364\n",
      "Iteration 574, the loss is 89.61546443726101, parameters k is -2.4768139087655388 and b is -51.516788251051366\n",
      "Iteration 575, the loss is 89.21049814343431, parameters k is -2.4139675648920207 and b is -51.50678825105137\n",
      "Iteration 576, the loss is 88.80553184960749, parameters k is -2.3511212210185026 and b is -51.49678825105137\n",
      "Iteration 577, the loss is 88.40056555578053, parameters k is -2.2882748771449846 and b is -51.48678825105137\n",
      "Iteration 578, the loss is 87.99559926195384, parameters k is -2.2254285332714665 and b is -51.476788251051374\n",
      "Iteration 579, the loss is 87.5906329681269, parameters k is -2.1625821893979484 and b is -51.466788251051376\n",
      "Iteration 580, the loss is 87.1856666743, parameters k is -2.0997358455244304 and b is -51.45678825105138\n",
      "Iteration 581, the loss is 86.78070038047325, parameters k is -2.0368895016509123 and b is -51.44678825105138\n",
      "Iteration 582, the loss is 86.37573408664622, parameters k is -1.9740431577773945 and b is -51.43678825105138\n",
      "Iteration 583, the loss is 85.97076779281954, parameters k is -1.9111968139038766 and b is -51.426788251051384\n",
      "Iteration 584, the loss is 85.56580149899271, parameters k is -1.8483504700303588 and b is -51.416788251051386\n",
      "Iteration 585, the loss is 85.16083520516578, parameters k is -1.785504126156841 and b is -51.40678825105139\n",
      "Iteration 586, the loss is 84.75586891133902, parameters k is -1.7226577822833231 and b is -51.39678825105139\n",
      "Iteration 587, the loss is 84.35090261751215, parameters k is -1.6598114384098053 and b is -51.38678825105139\n",
      "Iteration 588, the loss is 83.94593632368537, parameters k is -1.5969650945362874 and b is -51.376788251051394\n",
      "Iteration 589, the loss is 83.54097002985844, parameters k is -1.5341187506627696 and b is -51.366788251051396\n",
      "Iteration 590, the loss is 83.13600373603168, parameters k is -1.4712724067892518 and b is -51.3567882510514\n",
      "Iteration 591, the loss is 82.73103744220478, parameters k is -1.408426062915734 and b is -51.3467882510514\n",
      "Iteration 592, the loss is 82.32607114837786, parameters k is -1.345579719042216 and b is -51.3367882510514\n",
      "Iteration 593, the loss is 81.92110485455107, parameters k is -1.2827333751686982 and b is -51.326788251051404\n",
      "Iteration 594, the loss is 81.51613856072419, parameters k is -1.2198870312951804 and b is -51.316788251051406\n",
      "Iteration 595, the loss is 81.11117226689734, parameters k is -1.1570406874216626 and b is -51.30678825105141\n",
      "Iteration 596, the loss is 80.70620597307055, parameters k is -1.0941943435481447 and b is -51.29678825105141\n",
      "Iteration 597, the loss is 80.30123967924371, parameters k is -1.0313479996746269 and b is -51.28678825105141\n",
      "Iteration 598, the loss is 79.89627338541676, parameters k is -0.968501655801109 and b is -51.276788251051414\n",
      "Iteration 599, the loss is 79.49130709159006, parameters k is -0.9056553119275912 and b is -51.266788251051416\n",
      "Iteration 600, the loss is 79.08634079776319, parameters k is -0.8428089680540733 and b is -51.25678825105142\n",
      "Iteration 601, the loss is 78.68137450393621, parameters k is -0.7799626241805555 and b is -51.24678825105142\n",
      "Iteration 602, the loss is 78.2764082101096, parameters k is -0.7171162803070377 and b is -51.23678825105142\n",
      "Iteration 603, the loss is 77.87144191628256, parameters k is -0.6542699364335198 and b is -51.226788251051424\n",
      "Iteration 604, the loss is 77.46647562245579, parameters k is -0.591423592560002 and b is -51.216788251051426\n",
      "Iteration 605, the loss is 77.06150932862909, parameters k is -0.5285772486864841 and b is -51.20678825105143\n",
      "Iteration 606, the loss is 76.65654303480213, parameters k is -0.4657309048129663 and b is -51.19678825105143\n",
      "Iteration 607, the loss is 76.25157674097531, parameters k is -0.40288456093944847 and b is -51.18678825105143\n",
      "Iteration 608, the loss is 75.84661044714842, parameters k is -0.3400382170659306 and b is -51.176788251051434\n",
      "Iteration 609, the loss is 75.44164415332169, parameters k is -0.2771918731924128 and b is -51.166788251051436\n",
      "Iteration 610, the loss is 75.03667785949476, parameters k is -0.21434552931889492 and b is -51.15678825105144\n",
      "Iteration 611, the loss is 74.63171156566789, parameters k is -0.15149918544537705 and b is -51.14678825105144\n",
      "Iteration 612, the loss is 74.22674527184112, parameters k is -0.08865284157185918 and b is -51.13678825105144\n",
      "Iteration 613, the loss is 73.82177897801415, parameters k is -0.025806497698341313 and b is -51.126788251051444\n",
      "Iteration 614, the loss is 73.4168126841874, parameters k is 0.037039846175176555 and b is -51.116788251051446\n",
      "Iteration 615, the loss is 73.01184639036056, parameters k is 0.09988619004869442 and b is -51.10678825105145\n",
      "Iteration 616, the loss is 72.60688009653363, parameters k is 0.1627325339222123 and b is -51.09678825105145\n",
      "Iteration 617, the loss is 72.20191380270685, parameters k is 0.22557887779573016 and b is -51.08678825105145\n",
      "Iteration 618, the loss is 71.79694750888002, parameters k is 0.288425221669248 and b is -51.076788251051454\n",
      "Iteration 619, the loss is 71.39198121505318, parameters k is 0.35127156554276584 and b is -51.066788251051456\n",
      "Iteration 620, the loss is 70.98701492122638, parameters k is 0.4141179094162837 and b is -51.05678825105146\n",
      "Iteration 621, the loss is 70.58204862739943, parameters k is 0.4769642532898015 and b is -51.04678825105146\n",
      "Iteration 622, the loss is 70.17708233357263, parameters k is 0.5398105971633194 and b is -51.03678825105146\n",
      "Iteration 623, the loss is 69.77211603974585, parameters k is 0.6026569410368372 and b is -51.026788251051464\n",
      "Iteration 624, the loss is 69.367149745919, parameters k is 0.665503284910355 and b is -51.016788251051466\n",
      "Iteration 625, the loss is 68.96218345209216, parameters k is 0.7283496287838729 and b is -51.00678825105147\n",
      "Iteration 626, the loss is 68.55721715826517, parameters k is 0.7911959726573907 and b is -50.99678825105147\n",
      "Iteration 627, the loss is 68.1522508644384, parameters k is 0.8540423165309086 and b is -50.98678825105147\n",
      "Iteration 628, the loss is 67.74728457061158, parameters k is 0.9168886604044264 and b is -50.976788251051474\n",
      "Iteration 629, the loss is 67.34231827678477, parameters k is 0.9797350042779442 and b is -50.966788251051476\n",
      "Iteration 630, the loss is 66.93735198295794, parameters k is 1.042581348151462 and b is -50.95678825105148\n",
      "Iteration 631, the loss is 66.53238568913105, parameters k is 1.10542769202498 and b is -50.94678825105148\n",
      "Iteration 632, the loss is 66.12741939530424, parameters k is 1.1682740358984978 and b is -50.93678825105148\n",
      "Iteration 633, the loss is 65.72245310147743, parameters k is 1.2311203797720156 and b is -50.926788251051484\n",
      "Iteration 634, the loss is 65.31748680765054, parameters k is 1.2939667236455334 and b is -50.916788251051486\n",
      "Iteration 635, the loss is 64.91252051382364, parameters k is 1.3568130675190513 and b is -50.90678825105149\n",
      "Iteration 636, the loss is 64.50755421999682, parameters k is 1.4196594113925691 and b is -50.89678825105149\n",
      "Iteration 637, the loss is 64.10258792617, parameters k is 1.482505755266087 and b is -50.88678825105149\n",
      "Iteration 638, the loss is 63.697621632343164, parameters k is 1.5453520991396048 and b is -50.876788251051494\n",
      "Iteration 639, the loss is 63.292655338516305, parameters k is 1.6081984430131226 and b is -50.866788251051496\n",
      "Iteration 640, the loss is 62.88768904468943, parameters k is 1.6710447868866405 and b is -50.8567882510515\n",
      "Iteration 641, the loss is 62.48272275086264, parameters k is 1.7338911307601583 and b is -50.8467882510515\n",
      "Iteration 642, the loss is 62.07775645703576, parameters k is 1.7967374746336762 and b is -50.8367882510515\n",
      "Iteration 643, the loss is 61.672790163208916, parameters k is 1.859583818507194 and b is -50.826788251051504\n",
      "Iteration 644, the loss is 61.267823869382035, parameters k is 1.9224301623807118 and b is -50.816788251051506\n",
      "Iteration 645, the loss is 60.86285757555529, parameters k is 1.9852765062542297 and b is -50.80678825105151\n",
      "Iteration 646, the loss is 60.45789128172837, parameters k is 2.0481228501277475 and b is -50.79678825105151\n",
      "Iteration 647, the loss is 60.05292498790154, parameters k is 2.1109691940012656 and b is -50.78678825105151\n",
      "Iteration 648, the loss is 59.64795869407472, parameters k is 2.1738155378747837 and b is -50.776788251051514\n",
      "Iteration 649, the loss is 59.24299240024786, parameters k is 2.2366618817483017 and b is -50.766788251051516\n",
      "Iteration 650, the loss is 58.83802610642101, parameters k is 2.29950822562182 and b is -50.75678825105152\n",
      "Iteration 651, the loss is 58.43305981259424, parameters k is 2.362354569495338 and b is -50.74678825105152\n",
      "Iteration 652, the loss is 58.02809351876727, parameters k is 2.425200913368856 and b is -50.73678825105152\n",
      "Iteration 653, the loss is 57.62312722494044, parameters k is 2.488047257242374 and b is -50.726788251051524\n",
      "Iteration 654, the loss is 57.218160931113694, parameters k is 2.550893601115892 and b is -50.716788251051526\n",
      "Iteration 655, the loss is 56.81319463728686, parameters k is 2.61373994498941 and b is -50.70678825105153\n",
      "Iteration 656, the loss is 56.408228343459996, parameters k is 2.676586288862928 and b is -50.69678825105153\n",
      "Iteration 657, the loss is 56.0032620496331, parameters k is 2.739432632736446 and b is -50.68678825105153\n",
      "Iteration 658, the loss is 55.59829575580625, parameters k is 2.8022789766099643 and b is -50.67678825105153\n",
      "Iteration 659, the loss is 55.19332946197941, parameters k is 2.8651253204834823 and b is -50.666788251051535\n",
      "Iteration 660, the loss is 54.78836316815261, parameters k is 2.9279716643570004 and b is -50.65678825105154\n",
      "Iteration 661, the loss is 54.38339687432567, parameters k is 2.9908180082305185 and b is -50.64678825105154\n",
      "Iteration 662, the loss is 53.97843058049892, parameters k is 3.0536643521040365 and b is -50.63678825105154\n",
      "Iteration 663, the loss is 53.573464286672085, parameters k is 3.1165106959775546 and b is -50.62678825105154\n",
      "Iteration 664, the loss is 53.1684979928452, parameters k is 3.1793570398510727 and b is -50.616788251051545\n",
      "Iteration 665, the loss is 52.76353169901835, parameters k is 3.2422033837245907 and b is -50.60678825105155\n",
      "Iteration 666, the loss is 52.358565405191584, parameters k is 3.3050497275981088 and b is -50.59678825105155\n",
      "Iteration 667, the loss is 51.95359911136462, parameters k is 3.367896071471627 and b is -50.58678825105155\n",
      "Iteration 668, the loss is 51.54863281753785, parameters k is 3.430742415345145 and b is -50.57678825105155\n",
      "Iteration 669, the loss is 51.143666523710955, parameters k is 3.493588759218663 and b is -50.566788251051555\n",
      "Iteration 670, the loss is 50.73870022988417, parameters k is 3.556435103092181 and b is -50.55678825105156\n",
      "Iteration 671, the loss is 50.33373393605727, parameters k is 3.619281446965699 and b is -50.54678825105156\n",
      "Iteration 672, the loss is 49.928767642230454, parameters k is 3.682127790839217 and b is -50.53678825105156\n",
      "Iteration 673, the loss is 49.523801348403616, parameters k is 3.744974134712735 and b is -50.52678825105156\n",
      "Iteration 674, the loss is 49.11883505457673, parameters k is 3.8078204785862533 and b is -50.516788251051565\n",
      "Iteration 675, the loss is 48.71386876074985, parameters k is 3.8706668224597713 and b is -50.50678825105157\n",
      "Iteration 676, the loss is 48.30890246692302, parameters k is 3.9335131663332894 and b is -50.49678825105157\n",
      "Iteration 677, the loss is 47.90393617309622, parameters k is 3.9963595102068075 and b is -50.48678825105157\n",
      "Iteration 678, the loss is 47.498969879269396, parameters k is 4.059205854080325 and b is -50.47678825105157\n",
      "Iteration 679, the loss is 47.09400358544252, parameters k is 4.122052197953843 and b is -50.466788251051575\n",
      "Iteration 680, the loss is 46.68903729161565, parameters k is 4.18489854182736 and b is -50.45678825105158\n",
      "Iteration 681, the loss is 46.28407099778889, parameters k is 4.247744885700878 and b is -50.44678825105158\n",
      "Iteration 682, the loss is 45.87910470396205, parameters k is 4.3105912295743956 and b is -50.43678825105158\n",
      "Iteration 683, the loss is 45.47413841013515, parameters k is 4.373437573447913 and b is -50.42678825105158\n",
      "Iteration 684, the loss is 45.06917211630831, parameters k is 4.436283917321431 and b is -50.416788251051585\n",
      "Iteration 685, the loss is 44.66420582248152, parameters k is 4.499130261194948 and b is -50.40678825105159\n",
      "Iteration 686, the loss is 44.25923952865464, parameters k is 4.561976605068466 and b is -50.39678825105159\n",
      "Iteration 687, the loss is 43.854273234827836, parameters k is 4.624822948941984 and b is -50.38678825105159\n",
      "Iteration 688, the loss is 43.44930694100099, parameters k is 4.687669292815501 and b is -50.37678825105159\n",
      "Iteration 689, the loss is 43.0443406471741, parameters k is 4.750515636689019 and b is -50.366788251051595\n",
      "Iteration 690, the loss is 42.639374353347264, parameters k is 4.8133619805625365 and b is -50.3567882510516\n",
      "Iteration 691, the loss is 42.23440805952045, parameters k is 4.876208324436054 and b is -50.3467882510516\n",
      "Iteration 692, the loss is 41.82944176569356, parameters k is 4.939054668309572 and b is -50.3367882510516\n",
      "Iteration 693, the loss is 41.4244754718668, parameters k is 5.001901012183089 and b is -50.3267882510516\n",
      "Iteration 694, the loss is 41.019509178039954, parameters k is 5.064747356056607 and b is -50.316788251051605\n",
      "Iteration 695, the loss is 40.61454288421307, parameters k is 5.127593699930125 and b is -50.30678825105161\n",
      "Iteration 696, the loss is 40.209576590386234, parameters k is 5.190440043803642 and b is -50.29678825105161\n",
      "Iteration 697, the loss is 39.80461029655939, parameters k is 5.25328638767716 and b is -50.28678825105161\n",
      "Iteration 698, the loss is 39.39964400273258, parameters k is 5.3161327315506774 and b is -50.27678825105161\n",
      "Iteration 699, the loss is 38.9946777089057, parameters k is 5.378979075424195 and b is -50.266788251051615\n",
      "Iteration 700, the loss is 38.589711415078895, parameters k is 5.441825419297713 and b is -50.25678825105162\n",
      "Iteration 701, the loss is 38.18474512125206, parameters k is 5.50467176317123 and b is -50.24678825105162\n",
      "Iteration 702, the loss is 37.779778827425176, parameters k is 5.567518107044748 and b is -50.23678825105162\n",
      "Iteration 703, the loss is 37.37481253359833, parameters k is 5.6303644509182655 and b is -50.22678825105162\n",
      "Iteration 704, the loss is 36.96984623977151, parameters k is 5.693210794791783 and b is -50.216788251051625\n",
      "Iteration 705, the loss is 36.56487994594466, parameters k is 5.756057138665301 and b is -50.20678825105163\n",
      "Iteration 706, the loss is 36.15991365211783, parameters k is 5.818903482538818 and b is -50.19678825105163\n",
      "Iteration 707, the loss is 35.754947358290984, parameters k is 5.881749826412336 and b is -50.18678825105163\n",
      "Iteration 708, the loss is 35.349981064464124, parameters k is 5.944596170285854 and b is -50.17678825105163\n",
      "Iteration 709, the loss is 34.945014770637286, parameters k is 6.007442514159371 and b is -50.166788251051635\n",
      "Iteration 710, the loss is 34.5400484768105, parameters k is 6.070288858032889 and b is -50.15678825105164\n",
      "Iteration 711, the loss is 34.13508218298361, parameters k is 6.1331352019064065 and b is -50.14678825105164\n",
      "Iteration 712, the loss is 33.730115889156785, parameters k is 6.195981545779924 and b is -50.13678825105164\n",
      "Iteration 713, the loss is 33.32514959532997, parameters k is 6.258827889653442 and b is -50.12678825105164\n",
      "Iteration 714, the loss is 32.92018330150311, parameters k is 6.321674233526959 and b is -50.116788251051645\n",
      "Iteration 715, the loss is 32.51521700767623, parameters k is 6.384520577400477 and b is -50.10678825105165\n",
      "Iteration 716, the loss is 32.1102507138494, parameters k is 6.447366921273995 and b is -50.09678825105165\n",
      "Iteration 717, the loss is 31.705284420022565, parameters k is 6.510213265147512 and b is -50.08678825105165\n",
      "Iteration 718, the loss is 31.3003181261957, parameters k is 6.57305960902103 and b is -50.07678825105165\n",
      "Iteration 719, the loss is 30.895351832368846, parameters k is 6.635905952894547 and b is -50.066788251051655\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 720, the loss is 30.490385538542036, parameters k is 6.698752296768065 and b is -50.05678825105166\n",
      "Iteration 721, the loss is 30.0854192447152, parameters k is 6.761598640641583 and b is -50.04678825105166\n",
      "Iteration 722, the loss is 29.68045295088834, parameters k is 6.8244449845151 and b is -50.03678825105166\n",
      "Iteration 723, the loss is 29.275486657061524, parameters k is 6.887291328388618 and b is -50.02678825105166\n",
      "Iteration 724, the loss is 28.870520363234625, parameters k is 6.9501376722621355 and b is -50.016788251051665\n",
      "Iteration 725, the loss is 28.46555406940779, parameters k is 7.012984016135653 and b is -50.00678825105167\n",
      "Iteration 726, the loss is 28.060587775580988, parameters k is 7.075830360009171 and b is -49.99678825105167\n",
      "Iteration 727, the loss is 27.65562148175416, parameters k is 7.138676703882688 and b is -49.98678825105167\n",
      "Iteration 728, the loss is 27.250655187927308, parameters k is 7.201523047756206 and b is -49.97678825105167\n",
      "Iteration 729, the loss is 26.84568889410046, parameters k is 7.264369391629724 and b is -49.966788251051675\n",
      "Iteration 730, the loss is 26.44072260027364, parameters k is 7.327215735503241 and b is -49.95678825105168\n",
      "Iteration 731, the loss is 26.035756306446764, parameters k is 7.390062079376759 and b is -49.94678825105168\n",
      "Iteration 732, the loss is 25.63079001261993, parameters k is 7.4529084232502765 and b is -49.93678825105168\n",
      "Iteration 733, the loss is 25.225823718793084, parameters k is 7.515754767123794 and b is -49.92678825105168\n",
      "Iteration 734, the loss is 24.820857424966277, parameters k is 7.578601110997312 and b is -49.916788251051685\n",
      "Iteration 735, the loss is 24.415891131139414, parameters k is 7.641447454870829 and b is -49.90678825105169\n",
      "Iteration 736, the loss is 24.01092483731257, parameters k is 7.704293798744347 and b is -49.89678825105169\n",
      "Iteration 737, the loss is 23.605958543485748, parameters k is 7.767140142617865 and b is -49.88678825105169\n",
      "Iteration 738, the loss is 23.2009922496589, parameters k is 7.829986486491382 and b is -49.87678825105169\n",
      "Iteration 739, the loss is 22.796025955832057, parameters k is 7.8928328303649 and b is -49.866788251051695\n",
      "Iteration 740, the loss is 22.391059662005212, parameters k is 7.955679174238417 and b is -49.8567882510517\n",
      "Iteration 741, the loss is 21.986093368178388, parameters k is 8.018525518111936 and b is -49.8467882510517\n",
      "Iteration 742, the loss is 21.581127074351514, parameters k is 8.081371861985454 and b is -49.8367882510517\n",
      "Iteration 743, the loss is 21.176160780524665, parameters k is 8.144218205858973 and b is -49.8267882510517\n",
      "Iteration 744, the loss is 20.77254325545514, parameters k is 8.207064549732491 and b is -49.816788251051705\n",
      "Iteration 745, the loss is 20.37200579680771, parameters k is 8.269563858032887 and b is -49.806827776743404\n",
      "Iteration 746, the loss is 19.97146833816029, parameters k is 8.332063166333283 and b is -49.7968673024351\n",
      "Iteration 747, the loss is 19.570930879512854, parameters k is 8.394562474633679 and b is -49.7869068281268\n",
      "Iteration 748, the loss is 19.170705067735174, parameters k is 8.457061782934074 and b is -49.7769463538185\n",
      "Iteration 749, the loss is 18.773589759426926, parameters k is 8.519293027993363 and b is -49.7670254052019\n",
      "Iteration 750, the loss is 18.378068194923884, parameters k is 8.581524273052652 and b is -49.757104456585296\n",
      "Iteration 751, the loss is 17.984699447076615, parameters k is 8.643485794791783 and b is -49.7472230336604\n",
      "Iteration 752, the loss is 17.594495576742414, parameters k is 8.70517190151115 and b is -49.737381136427196\n",
      "Iteration 753, the loss is 17.204291706408203, parameters k is 8.766858008230518 and b is -49.727539239193995\n",
      "Iteration 754, the loss is 16.814087836074, parameters k is 8.828544114949885 and b is -49.71769734196079\n",
      "Iteration 755, the loss is 16.426248724505925, parameters k is 8.890230221669253 and b is -49.70785544472759\n",
      "Iteration 756, the loss is 16.042873857091617, parameters k is 8.951372968704826 and b is -49.69809259887779\n",
      "Iteration 757, the loss is 15.660598030910183, parameters k is 9.012515715740399 and b is -49.68832975302799\n",
      "Iteration 758, the loss is 15.280359735796617, parameters k is 9.073407751313521 and b is -49.67860643286989\n",
      "Iteration 759, the loss is 14.900362528476546, parameters k is 9.134299786886643 and b is -49.66888311271179\n",
      "Iteration 760, the loss is 14.524340215474854, parameters k is 9.19489960902103 and b is -49.659199318245385\n",
      "Iteration 761, the loss is 14.15163533462634, parameters k is 9.255244055661347 and b is -49.649555049470685\n",
      "Iteration 762, the loss is 13.786812406090428, parameters k is 9.315093166333284 and b is -49.63998983207938\n",
      "Iteration 763, the loss is 13.428050647293329, parameters k is 9.37442156554277 and b is -49.630503666071476\n",
      "Iteration 764, the loss is 13.07355649914192, parameters k is 9.433213126807592 and b is -49.62109655144697\n",
      "Iteration 765, the loss is 12.719390901995254, parameters k is 9.492004688072415 and b is -49.61168943682246\n",
      "Iteration 766, the loss is 12.370168078946474, parameters k is 9.550553087281902 and b is -49.602321847889655\n",
      "Iteration 767, the loss is 12.028329489773402, parameters k is 9.608584174238423 and b is -49.593033310340246\n",
      "Iteration 768, the loss is 11.69827847501191, parameters k is 9.665384628783878 and b is -49.58394240124934\n",
      "Iteration 769, the loss is 11.372024514193923, parameters k is 9.721931960799688 and b is -49.57489101785013\n",
      "Iteration 770, the loss is 11.053361088513846, parameters k is 9.777970360009174 and b is -49.56591868583432\n",
      "Iteration 771, the loss is 10.74848162486122, parameters k is 9.832754450918266 and b is -49.557143982277005\n",
      "Iteration 772, the loss is 10.453107016408156, parameters k is 9.886573087281903 and b is -49.548527381486494\n",
      "Iteration 773, the loss is 10.162165536005832, parameters k is 9.93988148649139 and b is -49.53998983207938\n",
      "Iteration 774, the loss is 9.874792611443768, parameters k is 9.992935498349096 and b is -49.53149180836396\n",
      "Iteration 775, the loss is 9.591276706524122, parameters k is 10.04574243510799 and b is -49.523033310340246\n",
      "Iteration 776, the loss is 9.313681072938522, parameters k is 10.09781439162973 and b is -49.51469338939163\n",
      "Iteration 777, the loss is 9.044238186074157, parameters k is 10.149149786886646 and b is -49.506472045518116\n",
      "Iteration 778, the loss is 8.778356252363704, parameters k is 10.200233126807595 and b is -49.4982902273363\n",
      "Iteration 779, the loss is 8.522591598059291, parameters k is 10.250583937084274 and b is -49.49022698622958\n",
      "Iteration 780, the loss is 8.27612035229278, parameters k is 10.299699964752259 and b is -49.482361373581355\n",
      "Iteration 781, the loss is 8.04046935578729, parameters k is 10.348096110997318 and b is -49.47461433800823\n",
      "Iteration 782, the loss is 7.816442244981012, parameters k is 10.395029332341192 and b is -49.467104456585304\n",
      "Iteration 783, the loss is 7.600232892518553, parameters k is 10.441192277005223 and b is -49.45971315223748\n",
      "Iteration 784, the loss is 7.392017917875007, parameters k is 10.486379174238424 and b is -49.452479950656446\n",
      "Iteration 785, the loss is 7.189180217404575, parameters k is 10.531090735503247 and b is -49.44532580045882\n",
      "Iteration 786, the loss is 7.001805710679488, parameters k is 10.574273877795736 and b is -49.43840880441139\n",
      "Iteration 787, the loss is 6.8243947330241665, parameters k is 10.615961802696923 and b is -49.43172896251416\n",
      "Iteration 788, the loss is 6.6527390397225385, parameters k is 10.657177711787831 and b is -49.42512817200032\n",
      "Iteration 789, the loss is 6.486126969232007, parameters k is 10.697646190048701 and b is -49.418645958561584\n",
      "Iteration 790, the loss is 6.325320289930284, parameters k is 10.73735095289455 and b is -49.412282322197946\n",
      "Iteration 791, the loss is 6.1678275144214405, parameters k is 10.776615636689016 and b is -49.40599773721771\n",
      "Iteration 792, the loss is 6.012889697851443, parameters k is 10.81561490546372 and b is -49.39975267792917\n",
      "Iteration 793, the loss is 5.864469576969687, parameters k is 10.854131170285854 and b is -49.393586670024035\n",
      "Iteration 794, the loss is 5.724449910811062, parameters k is 10.891314589258185 and b is -49.38761829057739\n",
      "Iteration 795, the loss is 5.592552231107967, parameters k is 10.927502158428146 and b is -49.381808013897555\n",
      "Iteration 796, the loss is 5.4727884370463, parameters k is 10.9620526129736 and b is -49.37623489136791\n",
      "Iteration 797, the loss is 5.366692611499178, parameters k is 10.994578126807593 and b is -49.37097797437186\n",
      "Iteration 798, the loss is 5.270263294958413, parameters k is 11.025601110997316 and b is -49.365958211526014\n",
      "Iteration 799, the loss is 5.178471475921584, parameters k is 11.055791723645537 and b is -49.361057025755265\n",
      "Iteration 800, the loss is 5.094222923863307, parameters k is 11.084731941036841 and b is -49.35635346844301\n",
      "Iteration 801, the loss is 5.017266461327898, parameters k is 11.112166071471623 and b is -49.351887065280955\n",
      "Iteration 802, the loss is 4.944901270299863, parameters k is 11.139098027993363 and b is -49.3474997135023\n",
      "Iteration 803, the loss is 4.88249591751885, parameters k is 11.164184964752256 and b is -49.343389041565544\n",
      "Iteration 804, the loss is 4.830412858481866, parameters k is 11.187138778981504 and b is -49.33959457516238\n",
      "Iteration 805, the loss is 4.785420433490926, parameters k is 11.208374095187038 and b is -49.33607678860111\n",
      "Iteration 806, the loss is 4.745522587305484, parameters k is 11.228586802696919 and b is -49.33271710480665\n",
      "Iteration 807, the loss is 4.711169214102458, parameters k is 11.24700089360601 and b is -49.32963410085408\n",
      "Iteration 808, the loss is 4.679488648782848, parameters k is 11.264914589258185 and b is -49.326630148284906\n",
      "Iteration 809, the loss is 4.65203921383772, parameters k is 11.28163227700522 and b is -49.323823824174234\n",
      "Iteration 810, the loss is 4.629497772036029, parameters k is 11.296693166333284 and b is -49.32129417990546\n",
      "Iteration 811, the loss is 4.609957173353485, parameters k is 11.310617257242376 and b is -49.31896216409518\n",
      "Iteration 812, the loss is 4.591549352109288, parameters k is 11.324258225621822 and b is -49.316669673976605\n",
      "Iteration 813, the loss is 4.576146656587599, parameters k is 11.336634055661348 and b is -49.31457481231653\n",
      "Iteration 814, the loss is 4.563113301168185, parameters k is 11.348447790839213 and b is -49.31255900203985\n",
      "Iteration 815, the loss is 4.554528597099242, parameters k is 11.358012435107987 and b is -49.310898922988464\n",
      "Iteration 816, the loss is 4.548498780895942, parameters k is 11.365819727598106 and b is -49.30951552377898\n",
      "Iteration 817, the loss is 4.543354471297871, parameters k is 11.37287970783526 and b is -49.30825070164459\n",
      "Iteration 818, the loss is 4.5385421105597645, parameters k is 11.379939688072414 and b is -49.3069858795102\n",
      "Iteration 819, the loss is 4.534896588324769, parameters k is 11.386034055661346 and b is -49.30587916014261\n",
      "Iteration 820, the loss is 4.532051036144331, parameters k is 11.391456842222611 and b is -49.30485149215842\n",
      "Iteration 821, the loss is 4.529822724604743, parameters k is 11.396393304673204 and b is -49.30390287555763\n",
      "Iteration 822, the loss is 4.528253323446824, parameters k is 11.400388858032887 and b is -49.30311236172364\n",
      "Iteration 823, the loss is 4.526792325937686, parameters k is 11.404144055661346 and b is -49.30236137358135\n",
      "Iteration 824, the loss is 4.525574880093589, parameters k is 11.40766494498941 and b is -49.30164991113075\n",
      "Iteration 825, the loss is 4.524700534216639, parameters k is 11.410670537874786 and b is -49.30101750006356\n",
      "Iteration 826, the loss is 4.524036631130357, parameters k is 11.413332099139609 and b is -49.30042461468806\n",
      "Iteration 827, the loss is 4.523572318527959, parameters k is 11.415499747360952 and b is -49.29991078069597\n",
      "Iteration 828, the loss is 4.523293969942159, parameters k is 11.417181585305617 and b is -49.299475998087274\n",
      "Iteration 829, the loss is 4.523143661364887, parameters k is 11.418354846175182 and b is -49.299120266861976\n",
      "Iteration 830, the loss is 4.5230181695967095, parameters k is 11.419528107044748 and b is -49.29876453563668\n",
      "Iteration 831, the loss is 4.522923425515372, parameters k is 11.420448680167278 and b is -49.298448330103085\n",
      "Iteration 832, the loss is 4.5228312415350755, parameters k is 11.421369253289807 and b is -49.29813212456949\n",
      "Iteration 833, the loss is 4.522777526265313, parameters k is 11.422047929179135 and b is -49.297855444727595\n",
      "Iteration 834, the loss is 4.522723810995542, parameters k is 11.422726605068462 and b is -49.2975787648857\n",
      "Iteration 835, the loss is 4.5226700957257755, parameters k is 11.42340528095779 and b is -49.2973020850438\n",
      "Iteration 836, the loss is 4.522616380456006, parameters k is 11.424083956847117 and b is -49.29702540520191\n",
      "Iteration 837, the loss is 4.522562665186244, parameters k is 11.424762632736444 and b is -49.29674872536001\n",
      "Iteration 838, the loss is 4.522508949916473, parameters k is 11.425441308625771 and b is -49.296472045518115\n",
      "Iteration 839, the loss is 4.522455234646706, parameters k is 11.426119984515099 and b is -49.29619536567622\n",
      "Iteration 840, the loss is 4.522401519376936, parameters k is 11.426798660404426 and b is -49.29591868583432\n",
      "Iteration 841, the loss is 4.522347804107175, parameters k is 11.427477336293753 and b is -49.29564200599243\n",
      "Iteration 842, the loss is 4.522294088837412, parameters k is 11.42815601218308 and b is -49.29536532615053\n",
      "Iteration 843, the loss is 4.52224037356764, parameters k is 11.428834688072408 and b is -49.295088646308635\n",
      "Iteration 844, the loss is 4.522186658297874, parameters k is 11.429513363961735 and b is -49.29481196646674\n",
      "Iteration 845, the loss is 4.522132943028107, parameters k is 11.430192039851063 and b is -49.29453528662484\n",
      "Iteration 846, the loss is 4.522079227758341, parameters k is 11.43087071574039 and b is -49.29425860678295\n",
      "Iteration 847, the loss is 4.52202551248857, parameters k is 11.431549391629718 and b is -49.29398192694105\n",
      "Iteration 848, the loss is 4.521971797218809, parameters k is 11.432228067519045 and b is -49.293705247099155\n",
      "Iteration 849, the loss is 4.52191808194904, parameters k is 11.432906743408372 and b is -49.29342856725726\n",
      "Iteration 850, the loss is 4.52186436667928, parameters k is 11.4335854192977 and b is -49.29315188741536\n",
      "Iteration 851, the loss is 4.521810651409512, parameters k is 11.434264095187027 and b is -49.292875207573466\n",
      "Iteration 852, the loss is 4.521756936139739, parameters k is 11.434942771076354 and b is -49.29259852773157\n",
      "Iteration 853, the loss is 4.521703220869974, parameters k is 11.435621446965682 and b is -49.292321847889674\n",
      "Iteration 854, the loss is 4.521649505600209, parameters k is 11.436300122855009 and b is -49.29204516804778\n",
      "Iteration 855, the loss is 4.521595790330438, parameters k is 11.436978798744336 and b is -49.29176848820588\n",
      "Iteration 856, the loss is 4.5215420750606725, parameters k is 11.437657474633664 and b is -49.291491808363986\n",
      "Iteration 857, the loss is 4.521488359790908, parameters k is 11.438336150522991 and b is -49.29121512852209\n",
      "Iteration 858, the loss is 4.52143464452114, parameters k is 11.439014826412318 and b is -49.290938448680194\n",
      "Iteration 859, the loss is 4.521380929251372, parameters k is 11.439693502301646 and b is -49.2906617688383\n",
      "Iteration 860, the loss is 4.521327213981608, parameters k is 11.440372178190973 and b is -49.2903850889964\n",
      "Iteration 861, the loss is 4.521273498711845, parameters k is 11.4410508540803 and b is -49.290108409154506\n",
      "Iteration 862, the loss is 4.5212197834420715, parameters k is 11.441729529969628 and b is -49.28983172931261\n",
      "Iteration 863, the loss is 4.521166068172314, parameters k is 11.442408205858955 and b is -49.289555049470714\n",
      "Iteration 864, the loss is 4.521112352902544, parameters k is 11.443086881748282 and b is -49.28927836962882\n",
      "Iteration 865, the loss is 4.521058637632774, parameters k is 11.44376555763761 and b is -49.28900168978692\n",
      "Iteration 866, the loss is 4.521004922363002, parameters k is 11.444444233526937 and b is -49.288725009945026\n",
      "Iteration 867, the loss is 4.520951207093245, parameters k is 11.445122909416265 and b is -49.28844833010313\n",
      "Iteration 868, the loss is 4.520897491823475, parameters k is 11.445801585305592 and b is -49.28817165026123\n",
      "Iteration 869, the loss is 4.520852745570714, parameters k is 11.44648026119492 and b is -49.28789497041934\n",
      "Iteration 870, the loss is 4.520829764093074, parameters k is 11.446896881748279 and b is -49.28765781626914\n",
      "Iteration 871, the loss is 4.520806782615428, parameters k is 11.447313502301638 and b is -49.28742066211894\n",
      "Iteration 872, the loss is 4.5207838011377826, parameters k is 11.447730122854997 and b is -49.28718350796874\n",
      "Iteration 873, the loss is 4.520760819660141, parameters k is 11.448146743408357 and b is -49.28694635381854\n",
      "Iteration 874, the loss is 4.520737838182496, parameters k is 11.448563363961716 and b is -49.286709199668344\n",
      "Iteration 875, the loss is 4.52071485670485, parameters k is 11.448979984515075 and b is -49.286472045518146\n",
      "Iteration 876, the loss is 4.5206918752272065, parameters k is 11.449396605068435 and b is -49.28623489136795\n",
      "Iteration 877, the loss is 4.5206688937495665, parameters k is 11.449813225621794 and b is -49.28599773721775\n",
      "Iteration 878, the loss is 4.520645912271922, parameters k is 11.450229846175153 and b is -49.28576058306755\n",
      "Iteration 879, the loss is 4.5206229307942785, parameters k is 11.450646466728513 and b is -49.28552342891735\n",
      "Iteration 880, the loss is 4.5205999493166305, parameters k is 11.451063087281872 and b is -49.28528627476715\n",
      "Iteration 881, the loss is 4.520576967838992, parameters k is 11.451479707835231 and b is -49.285049120616954\n",
      "Iteration 882, the loss is 4.520561918269659, parameters k is 11.45189632838859 and b is -49.284811966466755\n",
      "Iteration 883, the loss is 4.520558455168423, parameters k is 11.452059312578314 and b is -49.284614338008254\n",
      "Iteration 884, the loss is 4.520555207260742, parameters k is 11.45197281060203 and b is -49.28445623524146\n",
      "Iteration 885, the loss is 4.520551959353064, parameters k is 11.451886308625745 and b is -49.28429813247466\n",
      "Iteration 886, the loss is 4.520550212038815, parameters k is 11.451799806649461 and b is -49.284140029707864\n",
      "Iteration 887, the loss is 4.520546996730231, parameters k is 11.451962790839184 and b is -49.28394240124936\n",
      "Iteration 888, the loss is 4.520543748822552, parameters k is 11.4518762888629 and b is -49.283784298482566\n",
      "Iteration 889, the loss is 4.520540500914882, parameters k is 11.451789786886616 and b is -49.28362619571577\n",
      "Iteration 890, the loss is 4.520538505807969, parameters k is 11.451703284910332 and b is -49.28346809294897\n",
      "Iteration 891, the loss is 4.520535538292044, parameters k is 11.451866269100055 and b is -49.28327046449047\n",
      "Iteration 892, the loss is 4.520532290384366, parameters k is 11.45177976712377 and b is -49.283112361723674\n",
      "Iteration 893, the loss is 4.520529042476693, parameters k is 11.451693265147487 and b is -49.28295425895688\n",
      "Iteration 894, the loss is 4.520526799577133, parameters k is 11.451606763171203 and b is -49.28279615619008\n",
      "Iteration 895, the loss is 4.520524079853853, parameters k is 11.451769747360926 and b is -49.28259852773158\n",
      "Iteration 896, the loss is 4.52052083194618, parameters k is 11.451683245384642 and b is -49.28244042496478\n",
      "Iteration 897, the loss is 4.520517584038504, parameters k is 11.451596743408357 and b is -49.282282322197986\n",
      "Iteration 898, the loss is 4.5205150933462885, parameters k is 11.451510241432073 and b is -49.28212421943119\n",
      "Iteration 899, the loss is 4.520512621415668, parameters k is 11.451673225621796 and b is -49.28192659097269\n",
      "Iteration 900, the loss is 4.520509373507991, parameters k is 11.451586723645512 and b is -49.28176848820589\n",
      "Iteration 901, the loss is 4.520506125600313, parameters k is 11.451500221669228 and b is -49.281610385439095\n",
      "Iteration 902, the loss is 4.520503387115443, parameters k is 11.451413719692944 and b is -49.2814522826723\n",
      "Iteration 903, the loss is 4.52050116297748, parameters k is 11.451576703882667 and b is -49.2812546542138\n",
      "Iteration 904, the loss is 4.520497915069807, parameters k is 11.451490201906383 and b is -49.281096551447\n",
      "Iteration 905, the loss is 4.520494667162128, parameters k is 11.451403699930099 and b is -49.2809384486802\n",
      "Iteration 906, the loss is 4.520491680884611, parameters k is 11.451317197953815 and b is -49.280780345913406\n",
      "Iteration 907, the loss is 4.520489704539291, parameters k is 11.451480182143538 and b is -49.280582717454905\n",
      "Iteration 908, the loss is 4.520486456631619, parameters k is 11.451393680167254 and b is -49.28042461468811\n",
      "Iteration 909, the loss is 4.52048320872394, parameters k is 11.45130717819097 and b is -49.28026651192131\n",
      "Iteration 910, the loss is 4.520479974653757, parameters k is 11.451220676214685 and b is -49.280108409154515\n",
      "Iteration 911, the loss is 4.520478246101103, parameters k is 11.451383660404408 and b is -49.279910780696014\n",
      "Iteration 912, the loss is 4.520474998193421, parameters k is 11.451297158428124 and b is -49.27975267792922\n",
      "Iteration 913, the loss is 4.520471750285747, parameters k is 11.45121065645184 and b is -49.27959457516242\n",
      "Iteration 914, the loss is 4.520468502378073, parameters k is 11.451124154475556 and b is -49.27943647239562\n",
      "Iteration 915, the loss is 4.520466553707758, parameters k is 11.451037652499272 and b is -49.27927836962883\n",
      "Iteration 916, the loss is 4.520463539755241, parameters k is 11.451200636688995 and b is -49.279080741170326\n",
      "Iteration 917, the loss is 4.520460291847561, parameters k is 11.45111413471271 and b is -49.27892263840353\n",
      "Iteration 918, the loss is 4.520457043939882, parameters k is 11.451027632736427 and b is -49.27876453563673\n",
      "Iteration 919, the loss is 4.520454847476916, parameters k is 11.450941130760143 and b is -49.278606432869935\n",
      "Iteration 920, the loss is 4.520452081317047, parameters k is 11.451104114949866 and b is -49.278408804411434\n",
      "Iteration 921, the loss is 4.520448833409372, parameters k is 11.451017612973581 and b is -49.27825070164464\n",
      "Iteration 922, the loss is 4.5204455855016965, parameters k is 11.450931110997297 and b is -49.27809259887784\n",
      "Iteration 923, the loss is 4.520443141246071, parameters k is 11.450844609021013 and b is -49.277934496111044\n",
      "Iteration 924, the loss is 4.5204406228788585, parameters k is 11.451007593210736 and b is -49.27773686765254\n",
      "Iteration 925, the loss is 4.520437374971186, parameters k is 11.450921091234452 and b is -49.277578764885746\n",
      "Iteration 926, the loss is 4.5204341270635044, parameters k is 11.450834589258168 and b is -49.27742066211895\n",
      "Iteration 927, the loss is 4.52043143501523, parameters k is 11.450748087281884 and b is -49.27726255935215\n",
      "Iteration 928, the loss is 4.52042916444067, parameters k is 11.450911071471607 and b is -49.27706493089365\n",
      "Iteration 929, the loss is 4.520425916532999, parameters k is 11.450824569495323 and b is -49.276906828126855\n",
      "Iteration 930, the loss is 4.520422668625314, parameters k is 11.450738067519039 and b is -49.27674872536006\n",
      "Iteration 931, the loss is 4.5204197287843915, parameters k is 11.450651565542755 and b is -49.27659062259326\n",
      "Iteration 932, the loss is 4.520417706002482, parameters k is 11.450814549732478 and b is -49.27639299413476\n",
      "Iteration 933, the loss is 4.5204144580948045, parameters k is 11.450728047756193 and b is -49.27623489136796\n",
      "Iteration 934, the loss is 4.520411210187127, parameters k is 11.45064154577991 and b is -49.276076788601166\n",
      "Iteration 935, the loss is 4.520408022553548, parameters k is 11.450555043803625 and b is -49.27591868583437\n",
      "Iteration 936, the loss is 4.5204062475643, parameters k is 11.450718027993348 and b is -49.27572105737587\n",
      "Iteration 937, the loss is 4.520402999656618, parameters k is 11.450631526017064 and b is -49.27556295460907\n",
      "Iteration 938, the loss is 4.520399751748948, parameters k is 11.45054502404078 and b is -49.275404851842275\n",
      "Iteration 939, the loss is 4.520396503841266, parameters k is 11.450458522064496 and b is -49.27524674907548\n",
      "Iteration 940, the loss is 4.520394601607548, parameters k is 11.450372020088212 and b is -49.27508864630868\n",
      "Iteration 941, the loss is 4.520391541218433, parameters k is 11.450535004277935 and b is -49.27489101785018\n",
      "Iteration 942, the loss is 4.520388293310754, parameters k is 11.45044850230165 and b is -49.27473291508338\n",
      "Iteration 943, the loss is 4.520385045403074, parameters k is 11.450362000325367 and b is -49.27457481231659\n",
      "Iteration 944, the loss is 4.520382895376703, parameters k is 11.450275498349082 and b is -49.27441670954979\n",
      "Iteration 945, the loss is 4.520380082780241, parameters k is 11.450438482538805 and b is -49.27421908109129\n",
      "Iteration 946, the loss is 4.52037683487257, parameters k is 11.450351980562521 and b is -49.27406097832449\n",
      "Iteration 947, the loss is 4.520373586964885, parameters k is 11.450265478586237 and b is -49.273902875557695\n",
      "Iteration 948, the loss is 4.520371189145864, parameters k is 11.450178976609953 and b is -49.2737447727909\n",
      "Iteration 949, the loss is 4.520368624342053, parameters k is 11.450341960799676 and b is -49.2735471443324\n",
      "Iteration 950, the loss is 4.520365376434375, parameters k is 11.450255458823392 and b is -49.2733890415656\n",
      "Iteration 951, the loss is 4.520362128526697, parameters k is 11.450168956847108 and b is -49.273230938798804\n",
      "Iteration 952, the loss is 4.520359482915017, parameters k is 11.450082454870824 and b is -49.27307283603201\n",
      "Iteration 953, the loss is 4.520357165903868, parameters k is 11.450245439060547 and b is -49.272875207573506\n",
      "Iteration 954, the loss is 4.52035391799619, parameters k is 11.450158937084263 and b is -49.27271710480671\n",
      "Iteration 955, the loss is 4.520350670088518, parameters k is 11.450072435107979 and b is -49.27255900203991\n",
      "Iteration 956, the loss is 4.520347776684176, parameters k is 11.449985933131694 and b is -49.272400899273116\n",
      "Iteration 957, the loss is 4.520345707465673, parameters k is 11.450148917321417 and b is -49.272203270814614\n",
      "Iteration 958, the loss is 4.5203424595580035, parameters k is 11.450062415345133 and b is -49.27204516804782\n",
      "Iteration 959, the loss is 4.520339211650325, parameters k is 11.44997591336885 and b is -49.27188706528102\n",
      "Iteration 960, the loss is 4.520336070453339, parameters k is 11.449889411392565 and b is -49.271728962514224\n",
      "Iteration 961, the loss is 4.520334249027488, parameters k is 11.450052395582288 and b is -49.27153133405572\n",
      "Iteration 962, the loss is 4.520331001119811, parameters k is 11.449965893606004 and b is -49.271373231288926\n",
      "Iteration 963, the loss is 4.520327753212138, parameters k is 11.44987939162972 and b is -49.27121512852213\n",
      "Iteration 964, the loss is 4.52032450530446, parameters k is 11.449792889653436 and b is -49.27105702575533\n",
      "Iteration 965, the loss is 4.520322649507339, parameters k is 11.449706387677152 and b is -49.270898922988536\n",
      "Iteration 966, the loss is 4.520319542681623, parameters k is 11.449869371866875 and b is -49.270701294530035\n",
      "Iteration 967, the loss is 4.520316294773945, parameters k is 11.44978286989059 and b is -49.27054319176324\n",
      "Iteration 968, the loss is 4.520313046866269, parameters k is 11.449696367914306 and b is -49.27038508899644\n",
      "Iteration 969, the loss is 4.520310943276489, parameters k is 11.449609865938022 and b is -49.270226986229645\n",
      "Iteration 970, the loss is 4.520308084243434, parameters k is 11.449772850127745 and b is -49.27002935777114\n",
      "Iteration 971, the loss is 4.5203048363357565, parameters k is 11.449686348151461 and b is -49.26987125500435\n",
      "Iteration 972, the loss is 4.52030158842808, parameters k is 11.449599846175177 and b is -49.26971315223755\n",
      "Iteration 973, the loss is 4.520299237045651, parameters k is 11.449513344198893 and b is -49.26955504947075\n",
      "Iteration 974, the loss is 4.520296625805247, parameters k is 11.449676328388616 and b is -49.26935742101225\n",
      "Iteration 975, the loss is 4.520293377897572, parameters k is 11.449589826412332 and b is -49.269199318245455\n",
      "Iteration 976, the loss is 4.5202901299898945, parameters k is 11.449503324436048 and b is -49.26904121547866\n",
      "Iteration 977, the loss is 4.520287530814804, parameters k is 11.449416822459764 and b is -49.26888311271186\n",
      "Iteration 978, the loss is 4.520285167367062, parameters k is 11.449579806649487 and b is -49.26868548425336\n",
      "Iteration 979, the loss is 4.520281919459379, parameters k is 11.449493304673203 and b is -49.268527381486564\n",
      "Iteration 980, the loss is 4.5202786715517, parameters k is 11.449406802696918 and b is -49.26836927871977\n",
      "Iteration 981, the loss is 4.520275824583968, parameters k is 11.449320300720634 and b is -49.26821117595297\n",
      "Iteration 982, the loss is 4.520273708928869, parameters k is 11.449483284910357 and b is -49.26801354749447\n",
      "Iteration 983, the loss is 4.520270461021199, parameters k is 11.449396782934073 and b is -49.26785544472767\n",
      "Iteration 984, the loss is 4.520267213113517, parameters k is 11.44931028095779 and b is -49.267697341960876\n",
      "Iteration 985, the loss is 4.520264118353123, parameters k is 11.449223778981505 and b is -49.26753923919408\n",
      "Iteration 986, the loss is 4.52026225049068, parameters k is 11.449386763171228 and b is -49.26734161073558\n",
      "Iteration 987, the loss is 4.520259002583006, parameters k is 11.449300261194944 and b is -49.26718350796878\n",
      "Iteration 988, the loss is 4.520255754675328, parameters k is 11.44921375921866 and b is -49.267025405201984\n",
      "Iteration 989, the loss is 4.520252506767653, parameters k is 11.449127257242376 and b is -49.26686730243519\n",
      "Iteration 990, the loss is 4.520250697407127, parameters k is 11.449040755266092 and b is -49.26670919966839\n",
      "Iteration 991, the loss is 4.5202475441448176, parameters k is 11.449203739455815 and b is -49.26651157120989\n",
      "Iteration 992, the loss is 4.520244296237141, parameters k is 11.44911723747953 and b is -49.26635346844309\n",
      "Iteration 993, the loss is 4.520241048329464, parameters k is 11.449030735503246 and b is -49.266195365676296\n",
      "Iteration 994, the loss is 4.520238991176281, parameters k is 11.448944233526962 and b is -49.2660372629095\n",
      "Iteration 995, the loss is 4.520236085706628, parameters k is 11.449107217716685 and b is -49.265839634451\n",
      "Iteration 996, the loss is 4.52023283779895, parameters k is 11.449020715740401 and b is -49.2656815316842\n",
      "Iteration 997, the loss is 4.520229589891275, parameters k is 11.448934213764117 and b is -49.265523428917405\n",
      "Iteration 998, the loss is 4.520227284945435, parameters k is 11.448847711787833 and b is -49.26536532615061\n",
      "Iteration 999, the loss is 4.520224627268441, parameters k is 11.449010695977556 and b is -49.26516769769211\n",
      "Iteration 1000, the loss is 4.520221379360764, parameters k is 11.448924194001272 and b is -49.26500959492531\n",
      "Iteration 1001, the loss is 4.520218131453083, parameters k is 11.448837692024988 and b is -49.26485149215851\n",
      "Iteration 1002, the loss is 4.520215578714595, parameters k is 11.448751190048704 and b is -49.26469338939172\n",
      "Iteration 1003, the loss is 4.520213168830247, parameters k is 11.448914174238427 and b is -49.264495760933215\n",
      "Iteration 1004, the loss is 4.520209920922576, parameters k is 11.448827672262142 and b is -49.26433765816642\n",
      "Iteration 1005, the loss is 4.520206673014897, parameters k is 11.448741170285858 and b is -49.26417955539962\n",
      "Iteration 1006, the loss is 4.520203872483752, parameters k is 11.448654668309574 and b is -49.264021452632825\n",
      "Iteration 1007, the loss is 4.520201710392064, parameters k is 11.448817652499297 and b is -49.263823824174324\n",
      "Iteration 1008, the loss is 4.520198462484386, parameters k is 11.448731150523013 and b is -49.26366572140753\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1009, the loss is 4.520195214576709, parameters k is 11.448644648546729 and b is -49.26350761864073\n",
      "Iteration 1010, the loss is 4.520192166252911, parameters k is 11.448558146570445 and b is -49.263349515873934\n",
      "Iteration 1011, the loss is 4.520190251953877, parameters k is 11.448721130760168 and b is -49.26315188741543\n",
      "Iteration 1012, the loss is 4.5201870040462016, parameters k is 11.448634628783884 and b is -49.262993784648636\n",
      "Iteration 1013, the loss is 4.520183756138525, parameters k is 11.4485481268076 and b is -49.26283568188184\n",
      "Iteration 1014, the loss is 4.520180508230844, parameters k is 11.448461624831316 and b is -49.26267757911504\n",
      "Iteration 1015, the loss is 4.520178745306911, parameters k is 11.448375122855031 and b is -49.262519476348245\n",
      "Iteration 1016, the loss is 4.5201755456080095, parameters k is 11.448538107044754 and b is -49.262321847889744\n",
      "Iteration 1017, the loss is 4.520172297700331, parameters k is 11.44845160506847 and b is -49.26216374512295\n",
      "Iteration 1018, the loss is 4.520169049792658, parameters k is 11.448365103092186 and b is -49.26200564235615\n",
      "Iteration 1019, the loss is 4.520167039076069, parameters k is 11.448278601115902 and b is -49.261847539589354\n",
      "Iteration 1020, the loss is 4.52016408716982, parameters k is 11.448441585305625 and b is -49.26164991113085\n",
      "Iteration 1021, the loss is 4.520160839262146, parameters k is 11.448355083329341 and b is -49.261491808364056\n",
      "Iteration 1022, the loss is 4.5201575913544625, parameters k is 11.448268581353057 and b is -49.26133370559726\n",
      "Iteration 1023, the loss is 4.520155332845227, parameters k is 11.448182079376773 and b is -49.26117560283046\n",
      "Iteration 1024, the loss is 4.520152628731631, parameters k is 11.448345063566496 and b is -49.26097797437196\n",
      "Iteration 1025, the loss is 4.520149380823954, parameters k is 11.448258561590212 and b is -49.260819871605165\n",
      "Iteration 1026, the loss is 4.520146132916277, parameters k is 11.448172059613928 and b is -49.26066176883837\n",
      "Iteration 1027, the loss is 4.520143626614384, parameters k is 11.448085557637643 and b is -49.26050366607157\n",
      "Iteration 1028, the loss is 4.520141170293447, parameters k is 11.448248541827367 and b is -49.26030603761307\n",
      "Iteration 1029, the loss is 4.520137922385768, parameters k is 11.448162039851082 and b is -49.26014793484627\n",
      "Iteration 1030, the loss is 4.520134674478089, parameters k is 11.448075537874798 and b is -49.25998983207948\n",
      "Iteration 1031, the loss is 4.520131920383543, parameters k is 11.447989035898514 and b is -49.25983172931268\n",
      "Iteration 1032, the loss is 4.520129711855262, parameters k is 11.448152020088237 and b is -49.25963410085418\n",
      "Iteration 1033, the loss is 4.520126463947579, parameters k is 11.448065518111953 and b is -49.25947599808738\n",
      "Iteration 1034, the loss is 4.520123216039907, parameters k is 11.447979016135669 and b is -49.259317895320585\n",
      "Iteration 1035, the loss is 4.520120214152699, parameters k is 11.447892514159385 and b is -49.25915979255379\n",
      "Iteration 1036, the loss is 4.520118253417063, parameters k is 11.448055498349108 and b is -49.25896216409529\n",
      "Iteration 1037, the loss is 4.520115005509391, parameters k is 11.447968996372824 and b is -49.25880406132849\n",
      "Iteration 1038, the loss is 4.520111757601711, parameters k is 11.44788249439654 and b is -49.258645958561694\n",
      "Iteration 1039, the loss is 4.520108509694036, parameters k is 11.447795992420255 and b is -49.2584878557949\n",
      "Iteration 1040, the loss is 4.520106793206704, parameters k is 11.447709490443971 and b is -49.2583297530281\n",
      "Iteration 1041, the loss is 4.5201035470712005, parameters k is 11.447872474633694 and b is -49.2581321245696\n",
      "Iteration 1042, the loss is 4.520100299163526, parameters k is 11.44778597265741 and b is -49.2579740218028\n",
      "Iteration 1043, the loss is 4.52009705125585, parameters k is 11.447699470681126 and b is -49.257815919036005\n",
      "Iteration 1044, the loss is 4.520095086975859, parameters k is 11.447612968704842 and b is -49.25765781626921\n",
      "Iteration 1045, the loss is 4.520092088633013, parameters k is 11.447775952894565 and b is -49.25746018781071\n",
      "Iteration 1046, the loss is 4.520088840725337, parameters k is 11.447689450918281 and b is -49.25730208504391\n",
      "Iteration 1047, the loss is 4.5200855928176535, parameters k is 11.447602948941997 and b is -49.257143982277114\n",
      "Iteration 1048, the loss is 4.520083380745011, parameters k is 11.447516446965713 and b is -49.25698587951032\n",
      "Iteration 1049, the loss is 4.52008063019483, parameters k is 11.447679431155436 and b is -49.256788251051816\n",
      "Iteration 1050, the loss is 4.5200773822871465, parameters k is 11.447592929179152 and b is -49.25663014828502\n",
      "Iteration 1051, the loss is 4.520074134379477, parameters k is 11.447506427202867 and b is -49.25647204551822\n",
      "Iteration 1052, the loss is 4.5200716745141705, parameters k is 11.447419925226583 and b is -49.256313942751426\n",
      "Iteration 1053, the loss is 4.520069171756635, parameters k is 11.447582909416306 and b is -49.256116314292925\n",
      "Iteration 1054, the loss is 4.520065923848958, parameters k is 11.447496407440022 and b is -49.25595821152613\n",
      "Iteration 1055, the loss is 4.5200626759412845, parameters k is 11.447409905463738 and b is -49.25580010875933\n",
      "Iteration 1056, the loss is 4.520059968283332, parameters k is 11.447323403487454 and b is -49.255642005992534\n",
      "Iteration 1057, the loss is 4.520057713318453, parameters k is 11.447486387677177 and b is -49.25544437753403\n",
      "Iteration 1058, the loss is 4.520054465410768, parameters k is 11.447399885700893 and b is -49.255286274767236\n",
      "Iteration 1059, the loss is 4.520051217503093, parameters k is 11.447313383724609 and b is -49.25512817200044\n",
      "Iteration 1060, the loss is 4.520048262052487, parameters k is 11.447226881748325 and b is -49.25497006923364\n",
      "Iteration 1061, the loss is 4.520046254880262, parameters k is 11.447389865938048 and b is -49.25477244077514\n",
      "Iteration 1062, the loss is 4.520043006972591, parameters k is 11.447303363961764 and b is -49.254614338008345\n",
      "Iteration 1063, the loss is 4.5200397590649, parameters k is 11.44721686198548 and b is -49.25445623524155\n",
      "Iteration 1064, the loss is 4.520036555821646, parameters k is 11.447130360009195 and b is -49.25429813247475\n",
      "Iteration 1065, the loss is 4.52003479644207, parameters k is 11.447293344198918 and b is -49.25410050401625\n",
      "Iteration 1066, the loss is 4.520031548534395, parameters k is 11.447206842222634 and b is -49.253942401249454\n",
      "Iteration 1067, the loss is 4.52002830062672, parameters k is 11.44712034024635 and b is -49.25378429848266\n",
      "Iteration 1068, the loss is 4.52002505271904, parameters k is 11.447033838270066 and b is -49.25362619571586\n",
      "Iteration 1069, the loss is 4.520023134875643, parameters k is 11.446947336293782 and b is -49.25346809294906\n",
      "Iteration 1070, the loss is 4.520020090096207, parameters k is 11.447110320483505 and b is -49.25327046449056\n",
      "Iteration 1071, the loss is 4.520016842188529, parameters k is 11.44702381850722 and b is -49.253112361723765\n",
      "Iteration 1072, the loss is 4.520013594280856, parameters k is 11.446937316530937 and b is -49.25295425895697\n",
      "Iteration 1073, the loss is 4.520011428644801, parameters k is 11.446850814554653 and b is -49.25279615619017\n",
      "Iteration 1074, the loss is 4.520008631658015, parameters k is 11.447013798744376 and b is -49.25259852773167\n",
      "Iteration 1075, the loss is 4.520005383750343, parameters k is 11.446927296768092 and b is -49.252440424964874\n",
      "Iteration 1076, the loss is 4.520002135842664, parameters k is 11.446840794791807 and b is -49.25228232219808\n",
      "Iteration 1077, the loss is 4.519999722413958, parameters k is 11.446754292815523 and b is -49.25212421943128\n",
      "Iteration 1078, the loss is 4.5199971732198305, parameters k is 11.446917277005246 and b is -49.25192659097278\n",
      "Iteration 1079, the loss is 4.519993925312154, parameters k is 11.446830775028962 and b is -49.25176848820598\n",
      "Iteration 1080, the loss is 4.519990677404477, parameters k is 11.446744273052678 and b is -49.251610385439186\n",
      "Iteration 1081, the loss is 4.519988016183113, parameters k is 11.446657771076394 and b is -49.25145228267239\n",
      "Iteration 1082, the loss is 4.519985714781645, parameters k is 11.446820755266117 and b is -49.25125465421389\n",
      "Iteration 1083, the loss is 4.519982466873965, parameters k is 11.446734253289833 and b is -49.25109655144709\n",
      "Iteration 1084, the loss is 4.519979218966284, parameters k is 11.446647751313549 and b is -49.250938448680294\n",
      "Iteration 1085, the loss is 4.519976309952274, parameters k is 11.446561249337265 and b is -49.2507803459135\n",
      "Iteration 1086, the loss is 4.519974256343459, parameters k is 11.446724233526988 and b is -49.250582717454996\n",
      "Iteration 1087, the loss is 4.519971008435777, parameters k is 11.446637731550704 and b is -49.2504246146882\n",
      "Iteration 1088, the loss is 4.519967760528102, parameters k is 11.44655122957442 and b is -49.2502665119214\n",
      "Iteration 1089, the loss is 4.519964603721431, parameters k is 11.446464727598135 and b is -49.250108409154606\n",
      "Iteration 1090, the loss is 4.519962797905267, parameters k is 11.446627711787858 and b is -49.249910780696105\n",
      "Iteration 1091, the loss is 4.51995954999759, parameters k is 11.446541209811574 and b is -49.24975267792931\n",
      "Iteration 1092, the loss is 4.519956302089912, parameters k is 11.44645470783529 and b is -49.24959457516251\n",
      "Iteration 1093, the loss is 4.519953054182232, parameters k is 11.446368205859006 and b is -49.249436472395715\n",
      "Iteration 1094, the loss is 4.519951182775431, parameters k is 11.446281703882722 and b is -49.24927836962892\n",
      "Iteration 1095, the loss is 4.519948091559403, parameters k is 11.446444688072445 and b is -49.24908074117042\n",
      "Iteration 1096, the loss is 4.519944843651723, parameters k is 11.44635818609616 and b is -49.24892263840362\n",
      "Iteration 1097, the loss is 4.519941595744047, parameters k is 11.446271684119877 and b is -49.24876453563682\n",
      "Iteration 1098, the loss is 4.519939476544591, parameters k is 11.446185182143592 and b is -49.24860643287003\n",
      "Iteration 1099, the loss is 4.519936633121212, parameters k is 11.446348166333316 and b is -49.248408804411525\n",
      "Iteration 1100, the loss is 4.519933385213536, parameters k is 11.446261664357031 and b is -49.24825070164473\n",
      "Iteration 1101, the loss is 4.519930137305856, parameters k is 11.446175162380747 and b is -49.24809259887793\n",
      "Iteration 1102, the loss is 4.519927770313744, parameters k is 11.446088660404463 and b is -49.247934496111135\n",
      "Iteration 1103, the loss is 4.519925174683024, parameters k is 11.446251644594186 and b is -49.247736867652634\n",
      "Iteration 1104, the loss is 4.5199219267753445, parameters k is 11.446165142617902 and b is -49.24757876488584\n",
      "Iteration 1105, the loss is 4.519918678867664, parameters k is 11.446078640641618 and b is -49.24742066211904\n",
      "Iteration 1106, the loss is 4.519916064082906, parameters k is 11.445992138665334 and b is -49.247262559352244\n",
      "Iteration 1107, the loss is 4.51991371624483, parameters k is 11.446155122855057 and b is -49.24706493089374\n",
      "Iteration 1108, the loss is 4.519910468337158, parameters k is 11.446068620878773 and b is -49.246906828126946\n",
      "Iteration 1109, the loss is 4.519907220429483, parameters k is 11.445982118902489 and b is -49.24674872536015\n",
      "Iteration 1110, the loss is 4.5199043578520595, parameters k is 11.445895616926204 and b is -49.24659062259335\n",
      "Iteration 1111, the loss is 4.519902257806642, parameters k is 11.446058601115928 and b is -49.24639299413485\n",
      "Iteration 1112, the loss is 4.519899009898972, parameters k is 11.445972099139643 and b is -49.246234891368054\n",
      "Iteration 1113, the loss is 4.519895761991293, parameters k is 11.44588559716336 and b is -49.24607678860126\n",
      "Iteration 1114, the loss is 4.519892651621221, parameters k is 11.445799095187075 and b is -49.24591868583446\n",
      "Iteration 1115, the loss is 4.519890799368458, parameters k is 11.445962079376798 and b is -49.24572105737596\n",
      "Iteration 1116, the loss is 4.519887551460782, parameters k is 11.445875577400514 and b is -49.24556295460916\n",
      "Iteration 1117, the loss is 4.5198843035531056, parameters k is 11.44578907542423 and b is -49.245404851842366\n",
      "Iteration 1118, the loss is 4.519881055645427, parameters k is 11.445702573447946 and b is -49.24524674907557\n",
      "Iteration 1119, the loss is 4.519879230675221, parameters k is 11.445616071471662 and b is -49.24508864630877\n",
      "Iteration 1120, the loss is 4.519876093022593, parameters k is 11.445779055661385 and b is -49.24489101785027\n",
      "Iteration 1121, the loss is 4.519872845114915, parameters k is 11.4456925536851 and b is -49.244732915083475\n",
      "Iteration 1122, the loss is 4.519869597207234, parameters k is 11.445606051708817 and b is -49.24457481231668\n",
      "Iteration 1123, the loss is 4.5198675244443764, parameters k is 11.445519549732532 and b is -49.24441670954988\n",
      "Iteration 1124, the loss is 4.519864634584403, parameters k is 11.445682533922255 and b is -49.24421908109138\n",
      "Iteration 1125, the loss is 4.519861386676733, parameters k is 11.445596031945971 and b is -49.24406097832458\n",
      "Iteration 1126, the loss is 4.51985813876905, parameters k is 11.445509529969687 and b is -49.24390287555779\n",
      "Iteration 1127, the loss is 4.519855818213532, parameters k is 11.445423027993403 and b is -49.24374477279099\n",
      "Iteration 1128, the loss is 4.519853176146215, parameters k is 11.445586012183126 and b is -49.24354714433249\n",
      "Iteration 1129, the loss is 4.519849928238543, parameters k is 11.445499510206842 and b is -49.24338904156569\n",
      "Iteration 1130, the loss is 4.519846680330861, parameters k is 11.445413008230558 and b is -49.243230938798895\n",
      "Iteration 1131, the loss is 4.51984411198269, parameters k is 11.445326506254274 and b is -49.2430728360321\n",
      "Iteration 1132, the loss is 4.519841717708023, parameters k is 11.445489490443997 and b is -49.2428752075736\n",
      "Iteration 1133, the loss is 4.51983846980035, parameters k is 11.445402988467713 and b is -49.2427171048068\n",
      "Iteration 1134, the loss is 4.519835221892678, parameters k is 11.445316486491429 and b is -49.242559002040004\n",
      "Iteration 1135, the loss is 4.519832405751852, parameters k is 11.445229984515144 and b is -49.24240089927321\n",
      "Iteration 1136, the loss is 4.519830259269841, parameters k is 11.445392968704867 and b is -49.242203270814706\n",
      "Iteration 1137, the loss is 4.519827011362164, parameters k is 11.445306466728583 and b is -49.24204516804791\n",
      "Iteration 1138, the loss is 4.519823763454486, parameters k is 11.4452199647523 and b is -49.24188706528111\n",
      "Iteration 1139, the loss is 4.519820699521006, parameters k is 11.445133462776015 and b is -49.241728962514316\n",
      "Iteration 1140, the loss is 4.519818800831656, parameters k is 11.445296446965738 and b is -49.241531334055814\n",
      "Iteration 1141, the loss is 4.5198155529239745, parameters k is 11.445209944989454 and b is -49.24137323128902\n",
      "Iteration 1142, the loss is 4.519812305016298, parameters k is 11.44512344301317 and b is -49.24121512852222\n",
      "Iteration 1143, the loss is 4.519809057108616, parameters k is 11.445036941036886 and b is -49.241057025755424\n",
      "Iteration 1144, the loss is 4.5198072785750085, parameters k is 11.444950439060602 and b is -49.24089892298863\n",
      "Iteration 1145, the loss is 4.519804094485783, parameters k is 11.445113423250325 and b is -49.240701294530126\n",
      "Iteration 1146, the loss is 4.51980084657811, parameters k is 11.44502692127404 and b is -49.24054319176333\n",
      "Iteration 1147, the loss is 4.519797598670431, parameters k is 11.444940419297756 and b is -49.24038508899653\n",
      "Iteration 1148, the loss is 4.519795572344166, parameters k is 11.444853917321472 and b is -49.240226986229736\n",
      "Iteration 1149, the loss is 4.519792636047596, parameters k is 11.445016901511195 and b is -49.240029357771235\n",
      "Iteration 1150, the loss is 4.519789388139921, parameters k is 11.444930399534911 and b is -49.23987125500444\n",
      "Iteration 1151, the loss is 4.519786140232246, parameters k is 11.444843897558627 and b is -49.23971315223764\n",
      "Iteration 1152, the loss is 4.51978386611332, parameters k is 11.444757395582343 and b is -49.239555049470844\n",
      "Iteration 1153, the loss is 4.519781177609408, parameters k is 11.444920379772066 and b is -49.23935742101234\n",
      "Iteration 1154, the loss is 4.519777929701735, parameters k is 11.444833877795782 and b is -49.23919931824555\n",
      "Iteration 1155, the loss is 4.519774681794058, parameters k is 11.444747375819498 and b is -49.23904121547875\n",
      "Iteration 1156, the loss is 4.5197721598824785, parameters k is 11.444660873843214 and b is -49.23888311271195\n",
      "Iteration 1157, the loss is 4.51976971917122, parameters k is 11.444823858032937 and b is -49.23868548425345\n",
      "Iteration 1158, the loss is 4.519766471263541, parameters k is 11.444737356056653 and b is -49.238527381486655\n",
      "Iteration 1159, the loss is 4.5197632233558656, parameters k is 11.444650854080368 and b is -49.23836927871986\n",
      "Iteration 1160, the loss is 4.519760453651639, parameters k is 11.444564352104084 and b is -49.23821117595306\n",
      "Iteration 1161, the loss is 4.519758260733034, parameters k is 11.444727336293807 and b is -49.23801354749456\n",
      "Iteration 1162, the loss is 4.519755012825356, parameters k is 11.444640834317523 and b is -49.237855444727764\n",
      "Iteration 1163, the loss is 4.519751764917679, parameters k is 11.444554332341239 and b is -49.23769734196097\n",
      "Iteration 1164, the loss is 4.519748747420793, parameters k is 11.444467830364955 and b is -49.23753923919417\n",
      "Iteration 1165, the loss is 4.519746802294848, parameters k is 11.444630814554678 and b is -49.23734161073567\n",
      "Iteration 1166, the loss is 4.519743554387167, parameters k is 11.444544312578394 and b is -49.23718350796887\n",
      "Iteration 1167, the loss is 4.519740306479491, parameters k is 11.44445781060211 and b is -49.237025405202075\n",
      "Iteration 1168, the loss is 4.519737058571814, parameters k is 11.444371308625826 and b is -49.23686730243528\n",
      "Iteration 1169, the loss is 4.5197353264747955, parameters k is 11.444284806649542 and b is -49.23670919966848\n",
      "Iteration 1170, the loss is 4.519732095948979, parameters k is 11.444447790839265 and b is -49.23651157120998\n",
      "Iteration 1171, the loss is 4.519728848041307, parameters k is 11.44436128886298 and b is -49.236353468443184\n",
      "Iteration 1172, the loss is 4.519725600133622, parameters k is 11.444274786886696 and b is -49.23619536567639\n",
      "Iteration 1173, the loss is 4.519723620243954, parameters k is 11.444188284910412 and b is -49.23603726290959\n",
      "Iteration 1174, the loss is 4.519720637510789, parameters k is 11.444351269100135 and b is -49.23583963445109\n",
      "Iteration 1175, the loss is 4.519717389603112, parameters k is 11.444264767123851 and b is -49.23568153168429\n",
      "Iteration 1176, the loss is 4.5197141416954345, parameters k is 11.444178265147567 and b is -49.235523428917496\n",
      "Iteration 1177, the loss is 4.519711914013113, parameters k is 11.444091763171283 and b is -49.2353653261507\n",
      "Iteration 1178, the loss is 4.519709179072601, parameters k is 11.444254747361006 and b is -49.2351676976922\n",
      "Iteration 1179, the loss is 4.51970593116492, parameters k is 11.444168245384722 and b is -49.2350095949254\n",
      "Iteration 1180, the loss is 4.519702683257249, parameters k is 11.444081743408438 and b is -49.234851492158604\n",
      "Iteration 1181, the loss is 4.519700207782271, parameters k is 11.443995241432154 and b is -49.23469338939181\n",
      "Iteration 1182, the loss is 4.519697720634413, parameters k is 11.444158225621877 and b is -49.23449576093331\n",
      "Iteration 1183, the loss is 4.519694472726739, parameters k is 11.444071723645592 and b is -49.23433765816651\n",
      "Iteration 1184, the loss is 4.519691224819062, parameters k is 11.443985221669308 and b is -49.23417955539971\n",
      "Iteration 1185, the loss is 4.519688501551422, parameters k is 11.443898719693024 and b is -49.234021452632916\n",
      "Iteration 1186, the loss is 4.519686262196226, parameters k is 11.444061703882747 and b is -49.233823824174415\n",
      "Iteration 1187, the loss is 4.5196830142885505, parameters k is 11.443975201906463 and b is -49.23366572140762\n",
      "Iteration 1188, the loss is 4.519679766380877, parameters k is 11.443888699930179 and b is -49.23350761864082\n",
      "Iteration 1189, the loss is 4.519676795320577, parameters k is 11.443802197953895 and b is -49.233349515874025\n",
      "Iteration 1190, the loss is 4.519674803758042, parameters k is 11.443965182143618 and b is -49.233151887415524\n",
      "Iteration 1191, the loss is 4.519671555850359, parameters k is 11.443878680167334 and b is -49.23299378464873\n",
      "Iteration 1192, the loss is 4.519668307942683, parameters k is 11.44379217819105 and b is -49.23283568188193\n",
      "Iteration 1193, the loss is 4.519665089089739, parameters k is 11.443705676214766 and b is -49.23267757911513\n",
      "Iteration 1194, the loss is 4.519663345319846, parameters k is 11.443868660404489 and b is -49.23247995065663\n",
      "Iteration 1195, the loss is 4.519660097412174, parameters k is 11.443782158428204 and b is -49.232321847889835\n",
      "Iteration 1196, the loss is 4.519656849504496, parameters k is 11.44369565645192 and b is -49.23216374512304\n",
      "Iteration 1197, the loss is 4.519653601596822, parameters k is 11.443609154475636 and b is -49.23200564235624\n",
      "Iteration 1198, the loss is 4.519651668143737, parameters k is 11.443522652499352 and b is -49.231847539589445\n",
      "Iteration 1199, the loss is 4.51964863897398, parameters k is 11.443685636689075 and b is -49.231649911130944\n",
      "Iteration 1200, the loss is 4.519645391066305, parameters k is 11.443599134712791 and b is -49.23149180836415\n",
      "Iteration 1201, the loss is 4.519642143158634, parameters k is 11.443512632736507 and b is -49.23133370559735\n",
      "Iteration 1202, the loss is 4.5196399619128975, parameters k is 11.443426130760223 and b is -49.231175602830554\n",
      "Iteration 1203, the loss is 4.519637180535793, parameters k is 11.443589114949946 and b is -49.23097797437205\n",
      "Iteration 1204, the loss is 4.519633932628118, parameters k is 11.443502612973662 and b is -49.230819871605256\n",
      "Iteration 1205, the loss is 4.519630684720443, parameters k is 11.443416110997378 and b is -49.23066176883846\n",
      "Iteration 1206, the loss is 4.519628255682056, parameters k is 11.443329609021093 and b is -49.23050366607166\n",
      "Iteration 1207, the loss is 4.519625722097604, parameters k is 11.443492593210816 and b is -49.23030603761316\n",
      "Iteration 1208, the loss is 4.519622474189929, parameters k is 11.443406091234532 and b is -49.230147934846364\n",
      "Iteration 1209, the loss is 4.519619226282252, parameters k is 11.443319589258248 and b is -49.22998983207957\n",
      "Iteration 1210, the loss is 4.519616549451215, parameters k is 11.443233087281964 and b is -49.22983172931277\n",
      "Iteration 1211, the loss is 4.519614263659415, parameters k is 11.443396071471687 and b is -49.22963410085427\n",
      "Iteration 1212, the loss is 4.51961101575174, parameters k is 11.443309569495403 and b is -49.22947599808747\n",
      "Iteration 1213, the loss is 4.519607767844067, parameters k is 11.443223067519119 and b is -49.229317895320676\n",
      "Iteration 1214, the loss is 4.519604843220373, parameters k is 11.443136565542835 and b is -49.22915979255388\n",
      "Iteration 1215, the loss is 4.519602805221233, parameters k is 11.443299549732558 and b is -49.22896216409538\n",
      "Iteration 1216, the loss is 4.519599557313553, parameters k is 11.443213047756274 and b is -49.22880406132858\n",
      "Iteration 1217, the loss is 4.519596309405874, parameters k is 11.44312654577999 and b is -49.228645958561785\n",
      "Iteration 1218, the loss is 4.519593136989529, parameters k is 11.443040043803705 and b is -49.22848785579499\n",
      "Iteration 1219, the loss is 4.519591346783045, parameters k is 11.443203027993428 and b is -49.22829022733649\n",
      "Iteration 1220, the loss is 4.519588098875365, parameters k is 11.443116526017144 and b is -49.22813212456969\n",
      "Iteration 1221, the loss is 4.519584850967687, parameters k is 11.44303002404086 and b is -49.22797402180289\n",
      "Iteration 1222, the loss is 4.519581603060013, parameters k is 11.442943522064576 and b is -49.2278159190361\n",
      "Iteration 1223, the loss is 4.519579716043529, parameters k is 11.442857020088292 and b is -49.2276578162693\n",
      "Iteration 1224, the loss is 4.5195766404371795, parameters k is 11.443020004278015 and b is -49.2274601878108\n",
      "Iteration 1225, the loss is 4.519573392529497, parameters k is 11.442933502301731 and b is -49.227302085044\n",
      "Iteration 1226, the loss is 4.519570144621823, parameters k is 11.442847000325447 and b is -49.227143982277205\n",
      "Iteration 1227, the loss is 4.519568009812685, parameters k is 11.442760498349163 and b is -49.22698587951041\n",
      "Iteration 1228, the loss is 4.519565181998986, parameters k is 11.442923482538886 and b is -49.22678825105191\n",
      "Iteration 1229, the loss is 4.5195619340913105, parameters k is 11.442836980562602 and b is -49.22663014828511\n",
      "Iteration 1230, the loss is 4.519558686183633, parameters k is 11.442750478586317 and b is -49.226472045518314\n",
      "Iteration 1231, the loss is 4.519556303581842, parameters k is 11.442663976610033 and b is -49.22631394275152\n",
      "Iteration 1232, the loss is 4.5195537235608, parameters k is 11.442826960799756 and b is -49.226116314293016\n",
      "Iteration 1233, the loss is 4.5195504756531175, parameters k is 11.442740458823472 and b is -49.22595821152622\n",
      "Iteration 1234, the loss is 4.519547227745446, parameters k is 11.442653956847188 and b is -49.22580010875942\n",
      "Iteration 1235, the loss is 4.519544597351002, parameters k is 11.442567454870904 and b is -49.225642005992626\n",
      "Iteration 1236, the loss is 4.519542265122607, parameters k is 11.442730439060627 and b is -49.225444377534124\n",
      "Iteration 1237, the loss is 4.519539017214934, parameters k is 11.442643937084343 and b is -49.22528627476733\n",
      "Iteration 1238, the loss is 4.519535769307252, parameters k is 11.442557435108059 and b is -49.22512817200053\n",
      "Iteration 1239, the loss is 4.519532891120159, parameters k is 11.442470933131775 and b is -49.224970069233734\n",
      "Iteration 1240, the loss is 4.519530806684426, parameters k is 11.442633917321498 and b is -49.22477244077523\n",
      "Iteration 1241, the loss is 4.519527558776747, parameters k is 11.442547415345214 and b is -49.224614338008436\n",
      "Iteration 1242, the loss is 4.51952431086907, parameters k is 11.44246091336893 and b is -49.22445623524164\n",
      "Iteration 1243, the loss is 4.519521184889313, parameters k is 11.442374411392645 and b is -49.22429813247484\n",
      "Iteration 1244, the loss is 4.519519348246237, parameters k is 11.442537395582368 and b is -49.22410050401634\n",
      "Iteration 1245, the loss is 4.519516100338558, parameters k is 11.442450893606084 and b is -49.223942401249545\n",
      "Iteration 1246, the loss is 4.5195128524308785, parameters k is 11.4423643916298 and b is -49.22378429848275\n",
      "Iteration 1247, the loss is 4.519509604523199, parameters k is 11.442277889653516 and b is -49.22362619571595\n",
      "Iteration 1248, the loss is 4.519507763943313, parameters k is 11.442191387677232 and b is -49.223468092949155\n",
      "Iteration 1249, the loss is 4.519504641900369, parameters k is 11.442354371866955 and b is -49.22327046449065\n",
      "Iteration 1250, the loss is 4.519501393992693, parameters k is 11.44226786989067 and b is -49.22311236172386\n",
      "Iteration 1251, the loss is 4.519498146085018, parameters k is 11.442181367914387 and b is -49.22295425895706\n",
      "Iteration 1252, the loss is 4.519496057712471, parameters k is 11.442094865938103 and b is -49.22279615619026\n",
      "Iteration 1253, the loss is 4.519493183462176, parameters k is 11.442257850127826 and b is -49.22259852773176\n",
      "Iteration 1254, the loss is 4.519489935554509, parameters k is 11.442171348151541 and b is -49.222440424964965\n",
      "Iteration 1255, the loss is 4.519486687646829, parameters k is 11.442084846175257 and b is -49.22228232219817\n",
      "Iteration 1256, the loss is 4.519484351481633, parameters k is 11.441998344198973 and b is -49.22212421943137\n",
      "Iteration 1257, the loss is 4.519481725023993, parameters k is 11.442161328388696 and b is -49.22192659097287\n",
      "Iteration 1258, the loss is 4.519478477116314, parameters k is 11.442074826412412 and b is -49.221768488206074\n",
      "Iteration 1259, the loss is 4.519475229208638, parameters k is 11.441988324436128 and b is -49.22161038543928\n",
      "Iteration 1260, the loss is 4.519472645250789, parameters k is 11.441901822459844 and b is -49.22145228267248\n",
      "Iteration 1261, the loss is 4.519470266585805, parameters k is 11.442064806649567 and b is -49.22125465421398\n",
      "Iteration 1262, the loss is 4.519467018678129, parameters k is 11.441978304673283 and b is -49.22109655144718\n",
      "Iteration 1263, the loss is 4.519463770770449, parameters k is 11.441891802696999 and b is -49.220938448680386\n",
      "Iteration 1264, the loss is 4.519460939019945, parameters k is 11.441805300720715 and b is -49.22078034591359\n",
      "Iteration 1265, the loss is 4.519458808147621, parameters k is 11.441968284910438 and b is -49.22058271745509\n",
      "Iteration 1266, the loss is 4.519455560239935, parameters k is 11.441881782934153 and b is -49.22042461468829\n",
      "Iteration 1267, the loss is 4.519452312332261, parameters k is 11.44179528095787 and b is -49.220266511921494\n",
      "Iteration 1268, the loss is 4.519449232789102, parameters k is 11.441708778981585 and b is -49.2201084091547\n",
      "Iteration 1269, the loss is 4.519447349709427, parameters k is 11.441871763171308 and b is -49.219910780696196\n",
      "Iteration 1270, the loss is 4.5194441018017555, parameters k is 11.441785261195024 and b is -49.2197526779294\n",
      "Iteration 1271, the loss is 4.519440853894074, parameters k is 11.44169875921874 and b is -49.2195945751626\n",
      "Iteration 1272, the loss is 4.5194376059864005, parameters k is 11.441612257242456 and b is -49.219436472395806\n",
      "Iteration 1273, the loss is 4.519435811843101, parameters k is 11.441525755266172 and b is -49.21927836962901\n",
      "Iteration 1274, the loss is 4.519432643363564, parameters k is 11.441688739455895 and b is -49.21908074117051\n",
      "Iteration 1275, the loss is 4.519429395455885, parameters k is 11.44160223747961 and b is -49.21892263840371\n",
      "Iteration 1276, the loss is 4.519426147548204, parameters k is 11.441515735503327 and b is -49.218764535636915\n",
      "Iteration 1277, the loss is 4.519424105612262, parameters k is 11.441429233527042 and b is -49.21860643287012\n",
      "Iteration 1278, the loss is 4.519421184925375, parameters k is 11.441592217716765 and b is -49.21840880441162\n",
      "Iteration 1279, the loss is 4.519417937017699, parameters k is 11.441505715740481 and b is -49.21825070164482\n",
      "Iteration 1280, the loss is 4.5194146891100235, parameters k is 11.441419213764197 and b is -49.21809259887802\n",
      "Iteration 1281, the loss is 4.519412399381417, parameters k is 11.441332711787913 and b is -49.217934496111226\n",
      "Iteration 1282, the loss is 4.519409726487185, parameters k is 11.441495695977636 and b is -49.217736867652725\n",
      "Iteration 1283, the loss is 4.519406478579505, parameters k is 11.441409194001352 and b is -49.21757876488593\n",
      "Iteration 1284, the loss is 4.519403230671833, parameters k is 11.441322692025068 and b is -49.21742066211913\n",
      "Iteration 1285, the loss is 4.519400693150574, parameters k is 11.441236190048784 and b is -49.217262559352335\n",
      "Iteration 1286, the loss is 4.519398268049001, parameters k is 11.441399174238507 and b is -49.217064930893834\n",
      "Iteration 1287, the loss is 4.519395020141321, parameters k is 11.441312672262223 and b is -49.21690682812704\n",
      "Iteration 1288, the loss is 4.519391772233642, parameters k is 11.441226170285939 and b is -49.21674872536024\n",
      "Iteration 1289, the loss is 4.519388986919735, parameters k is 11.441139668309654 and b is -49.21659062259344\n",
      "Iteration 1290, the loss is 4.519386809610806, parameters k is 11.441302652499378 and b is -49.21639299413494\n",
      "Iteration 1291, the loss is 4.519383561703131, parameters k is 11.441216150523093 and b is -49.216234891368146\n",
      "Iteration 1292, the loss is 4.519380313795457, parameters k is 11.44112964854681 and b is -49.21607678860135\n",
      "Iteration 1293, the loss is 4.51937728068889, parameters k is 11.441043146570525 and b is -49.21591868583455\n",
      "Iteration 1294, the loss is 4.519375351172619, parameters k is 11.441206130760248 and b is -49.21572105737605\n",
      "Iteration 1295, the loss is 4.519372103264944, parameters k is 11.441119628783964 and b is -49.215562954609254\n",
      "Iteration 1296, the loss is 4.519368855357267, parameters k is 11.44103312680768 and b is -49.21540485184246\n",
      "Iteration 1297, the loss is 4.519365607449588, parameters k is 11.440946624831396 and b is -49.21524674907566\n",
      "Iteration 1298, the loss is 4.519363859742892, parameters k is 11.440860122855112 and b is -49.215088646308864\n",
      "Iteration 1299, the loss is 4.519360644826755, parameters k is 11.441023107044835 and b is -49.21489101785036\n",
      "Iteration 1300, the loss is 4.519357396919078, parameters k is 11.44093660506855 and b is -49.214732915083566\n",
      "Iteration 1301, the loss is 4.519354149011407, parameters k is 11.440850103092266 and b is -49.21457481231677\n",
      "Iteration 1302, the loss is 4.519352153512049, parameters k is 11.440763601115982 and b is -49.21441670954997\n",
      "Iteration 1303, the loss is 4.5193491863885695, parameters k is 11.440926585305705 and b is -49.21421908109147\n",
      "Iteration 1304, the loss is 4.51934593848089, parameters k is 11.440840083329421 and b is -49.214060978324675\n",
      "Iteration 1305, the loss is 4.519342690573207, parameters k is 11.440753581353137 and b is -49.21390287555788\n",
      "Iteration 1306, the loss is 4.519340447281208, parameters k is 11.440667079376853 and b is -49.21374477279108\n",
      "Iteration 1307, the loss is 4.519337727950381, parameters k is 11.440830063566576 and b is -49.21354714433258\n",
      "Iteration 1308, the loss is 4.519334480042703, parameters k is 11.440743561590292 and b is -49.21338904156578\n",
      "Iteration 1309, the loss is 4.519331232135024, parameters k is 11.440657059614008 and b is -49.213230938798986\n",
      "Iteration 1310, the loss is 4.519328741050358, parameters k is 11.440570557637724 and b is -49.21307283603219\n",
      "Iteration 1311, the loss is 4.519326269512185, parameters k is 11.440733541827447 and b is -49.21287520757369\n",
      "Iteration 1312, the loss is 4.5193230216045155, parameters k is 11.440647039851163 and b is -49.21271710480689\n",
      "Iteration 1313, the loss is 4.519319773696835, parameters k is 11.440560537874878 and b is -49.212559002040095\n",
      "Iteration 1314, the loss is 4.519317034819521, parameters k is 11.440474035898594 and b is -49.2124008992733\n",
      "Iteration 1315, the loss is 4.519314811074002, parameters k is 11.440637020088317 and b is -49.2122032708148\n",
      "Iteration 1316, the loss is 4.519311563166324, parameters k is 11.440550518112033 and b is -49.212045168048\n",
      "Iteration 1317, the loss is 4.519308315258651, parameters k is 11.44046401613575 and b is -49.2118870652812\n",
      "Iteration 1318, the loss is 4.519305328588676, parameters k is 11.440377514159465 and b is -49.21172896251441\n",
      "Iteration 1319, the loss is 4.519303352635812, parameters k is 11.440540498349188 and b is -49.211531334055906\n",
      "Iteration 1320, the loss is 4.519300104728136, parameters k is 11.440453996372904 and b is -49.21137323128911\n",
      "Iteration 1321, the loss is 4.519296856820458, parameters k is 11.44036749439662 and b is -49.21121512852231\n",
      "Iteration 1322, the loss is 4.519293622357836, parameters k is 11.440280992420336 and b is -49.211057025755515\n",
      "Iteration 1323, the loss is 4.519291894197624, parameters k is 11.440443976610059 and b is -49.210859397297014\n",
      "Iteration 1324, the loss is 4.519288646289949, parameters k is 11.440357474633775 and b is -49.21070129453022\n",
      "Iteration 1325, the loss is 4.519285398382269, parameters k is 11.44027097265749 and b is -49.21054319176342\n",
      "Iteration 1326, the loss is 4.519282150474595, parameters k is 11.440184470681206 and b is -49.210385088996624\n",
      "Iteration 1327, the loss is 4.519280201411831, parameters k is 11.440097968704922 and b is -49.21022698622983\n",
      "Iteration 1328, the loss is 4.51927718785176, parameters k is 11.440260952894645 and b is -49.210029357771326\n",
      "Iteration 1329, the loss is 4.519273939944082, parameters k is 11.440174450918361 and b is -49.20987125500453\n",
      "Iteration 1330, the loss is 4.519270692036404, parameters k is 11.440087948942077 and b is -49.20971315223773\n",
      "Iteration 1331, the loss is 4.519268495180995, parameters k is 11.440001446965793 and b is -49.209555049470936\n",
      "Iteration 1332, the loss is 4.5192657294135685, parameters k is 11.440164431155516 and b is -49.209357421012434\n",
      "Iteration 1333, the loss is 4.519262481505888, parameters k is 11.440077929179232 and b is -49.20919931824564\n",
      "Iteration 1334, the loss is 4.519259233598213, parameters k is 11.439991427202948 and b is -49.20904121547884\n",
      "Iteration 1335, the loss is 4.519256788950147, parameters k is 11.439904925226664 and b is -49.208883112712044\n",
      "Iteration 1336, the loss is 4.519254270975382, parameters k is 11.440067909416387 and b is -49.20868548425354\n",
      "Iteration 1337, the loss is 4.519251023067707, parameters k is 11.439981407440103 and b is -49.208527381486746\n",
      "Iteration 1338, the loss is 4.519247775160026, parameters k is 11.439894905463818 and b is -49.20836927871995\n",
      "Iteration 1339, the loss is 4.519245082719312, parameters k is 11.439808403487534 and b is -49.20821117595315\n",
      "Iteration 1340, the loss is 4.519242812537197, parameters k is 11.439971387677257 and b is -49.20801354749465\n",
      "Iteration 1341, the loss is 4.519239564629516, parameters k is 11.439884885700973 and b is -49.207855444727855\n",
      "Iteration 1342, the loss is 4.519236316721842, parameters k is 11.439798383724689 and b is -49.20769734196106\n",
      "Iteration 1343, the loss is 4.519233376488469, parameters k is 11.439711881748405 and b is -49.20753923919426\n",
      "Iteration 1344, the loss is 4.519231354099009, parameters k is 11.439874865938128 and b is -49.20734161073576\n",
      "Iteration 1345, the loss is 4.51922810619133, parameters k is 11.439788363961844 and b is -49.20718350796896\n",
      "Iteration 1346, the loss is 4.519224858283654, parameters k is 11.43970186198556 and b is -49.20702540520217\n",
      "Iteration 1347, the loss is 4.519221670257621, parameters k is 11.439615360009276 and b is -49.20686730243537\n",
      "Iteration 1348, the loss is 4.519219895660821, parameters k is 11.439778344198999 and b is -49.20666967397687\n",
      "Iteration 1349, the loss is 4.51921664775314, parameters k is 11.439691842222715 and b is -49.20651157121007\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1350, the loss is 4.519213399845463, parameters k is 11.43960534024643 and b is -49.206353468443275\n",
      "Iteration 1351, the loss is 4.519210151937789, parameters k is 11.439518838270146 and b is -49.20619536567648\n",
      "Iteration 1352, the loss is 4.519208249311623, parameters k is 11.439432336293862 and b is -49.20603726290968\n",
      "Iteration 1353, the loss is 4.519205189314954, parameters k is 11.439595320483585 and b is -49.20583963445118\n",
      "Iteration 1354, the loss is 4.519201941407279, parameters k is 11.439508818507301 and b is -49.205681531684384\n",
      "Iteration 1355, the loss is 4.519198693499599, parameters k is 11.439422316531017 and b is -49.20552342891759\n",
      "Iteration 1356, the loss is 4.519196543080778, parameters k is 11.439335814554733 and b is -49.20536532615079\n",
      "Iteration 1357, the loss is 4.519193730876767, parameters k is 11.439498798744456 and b is -49.20516769769229\n",
      "Iteration 1358, the loss is 4.519190482969091, parameters k is 11.439412296768172 and b is -49.20500959492549\n",
      "Iteration 1359, the loss is 4.519187235061415, parameters k is 11.439325794791888 and b is -49.204851492158696\n",
      "Iteration 1360, the loss is 4.519184836849938, parameters k is 11.439239292815603 and b is -49.2046933893919\n",
      "Iteration 1361, the loss is 4.519182272438578, parameters k is 11.439402277005327 and b is -49.2044957609334\n",
      "Iteration 1362, the loss is 4.519179024530893, parameters k is 11.439315775029042 and b is -49.2043376581666\n",
      "Iteration 1363, the loss is 4.5191757766232215, parameters k is 11.439229273052758 and b is -49.204179555399804\n",
      "Iteration 1364, the loss is 4.5191731306190945, parameters k is 11.439142771076474 and b is -49.20402145263301\n",
      "Iteration 1365, the loss is 4.519170814000387, parameters k is 11.439305755266197 and b is -49.203823824174506\n",
      "Iteration 1366, the loss is 4.519167566092713, parameters k is 11.439219253289913 and b is -49.20366572140771\n",
      "Iteration 1367, the loss is 4.519164318185032, parameters k is 11.439132751313629 and b is -49.20350761864091\n",
      "Iteration 1368, the loss is 4.519161424388253, parameters k is 11.439046249337345 and b is -49.203349515874116\n",
      "Iteration 1369, the loss is 4.519159355562197, parameters k is 11.439209233527068 and b is -49.203151887415615\n",
      "Iteration 1370, the loss is 4.519156107654521, parameters k is 11.439122731550784 and b is -49.20299378464882\n",
      "Iteration 1371, the loss is 4.519152859746849, parameters k is 11.4390362295745 and b is -49.20283568188202\n",
      "Iteration 1372, the loss is 4.519149718157411, parameters k is 11.438949727598215 and b is -49.202677579115225\n",
      "Iteration 1373, the loss is 4.519147897124012, parameters k is 11.439112711787939 and b is -49.20247995065672\n",
      "Iteration 1374, the loss is 4.519144649216334, parameters k is 11.439026209811654 and b is -49.20232184788993\n",
      "Iteration 1375, the loss is 4.519141401308661, parameters k is 11.43893970783537 and b is -49.20216374512313\n",
      "Iteration 1376, the loss is 4.519138153400984, parameters k is 11.438853205859086 and b is -49.20200564235633\n",
      "Iteration 1377, the loss is 4.519136297211408, parameters k is 11.438766703882802 and b is -49.20184753958954\n",
      "Iteration 1378, the loss is 4.519133190778143, parameters k is 11.438929688072525 and b is -49.201649911131035\n",
      "Iteration 1379, the loss is 4.519129942870466, parameters k is 11.438843186096241 and b is -49.20149180836424\n",
      "Iteration 1380, the loss is 4.5191266949627895, parameters k is 11.438756684119957 and b is -49.20133370559744\n",
      "Iteration 1381, the loss is 4.519124590980572, parameters k is 11.438670182143673 and b is -49.201175602830645\n",
      "Iteration 1382, the loss is 4.5191217323399595, parameters k is 11.438833166333396 and b is -49.200977974372144\n",
      "Iteration 1383, the loss is 4.519118484432279, parameters k is 11.438746664357112 and b is -49.20081987160535\n",
      "Iteration 1384, the loss is 4.519115236524604, parameters k is 11.438660162380828 and b is -49.20066176883855\n",
      "Iteration 1385, the loss is 4.519112884749723, parameters k is 11.438573660404543 and b is -49.200503666071754\n",
      "Iteration 1386, the loss is 4.519110273901769, parameters k is 11.438736644594266 and b is -49.20030603761325\n",
      "Iteration 1387, the loss is 4.519107025994094, parameters k is 11.438650142617982 and b is -49.200147934846456\n",
      "Iteration 1388, the loss is 4.519103778086417, parameters k is 11.438563640641698 and b is -49.19998983207966\n",
      "Iteration 1389, the loss is 4.51910117851888, parameters k is 11.438477138665414 and b is -49.19983172931286\n",
      "Iteration 1390, the loss is 4.519098815463578, parameters k is 11.438640122855137 and b is -49.19963410085436\n",
      "Iteration 1391, the loss is 4.519095567555902, parameters k is 11.438553620878853 and b is -49.199475998087564\n",
      "Iteration 1392, the loss is 4.51909231964823, parameters k is 11.438467118902569 and b is -49.19931789532077\n",
      "Iteration 1393, the loss is 4.5190894722880435, parameters k is 11.438380616926285 and b is -49.19915979255397\n",
      "Iteration 1394, the loss is 4.519087357025392, parameters k is 11.438543601116008 and b is -49.19896216409547\n",
      "Iteration 1395, the loss is 4.519084109117715, parameters k is 11.438457099139724 and b is -49.19880406132867\n",
      "Iteration 1396, the loss is 4.5190808612100355, parameters k is 11.43837059716344 and b is -49.198645958561876\n",
      "Iteration 1397, the loss is 4.519077766057204, parameters k is 11.438284095187155 and b is -49.19848785579508\n",
      "Iteration 1398, the loss is 4.519075898587208, parameters k is 11.438447079376878 and b is -49.19829022733658\n",
      "Iteration 1399, the loss is 4.5190726506795205, parameters k is 11.438360577400594 and b is -49.19813212456978\n",
      "Iteration 1400, the loss is 4.519069402771851, parameters k is 11.43827407542431 and b is -49.197974021802985\n",
      "Iteration 1401, the loss is 4.519066154864173, parameters k is 11.438187573448026 and b is -49.19781591903619\n",
      "Iteration 1402, the loss is 4.519064345111199, parameters k is 11.438101071471742 and b is -49.19765781626939\n",
      "Iteration 1403, the loss is 4.51906119224134, parameters k is 11.438264055661465 and b is -49.19746018781089\n",
      "Iteration 1404, the loss is 4.519057944333663, parameters k is 11.43817755368518 and b is -49.19730208504409\n",
      "Iteration 1405, the loss is 4.519054696425985, parameters k is 11.438091051708897 and b is -49.1971439822773\n",
      "Iteration 1406, the loss is 4.519052638880352, parameters k is 11.438004549732613 and b is -49.1969858795105\n",
      "Iteration 1407, the loss is 4.5190497338031514, parameters k is 11.438167533922336 and b is -49.196788251052\n",
      "Iteration 1408, the loss is 4.519046485895476, parameters k is 11.438081031946052 and b is -49.1966301482852\n",
      "Iteration 1409, the loss is 4.519043237987796, parameters k is 11.437994529969767 and b is -49.196472045518405\n",
      "Iteration 1410, the loss is 4.519040932649512, parameters k is 11.437908027993483 and b is -49.19631394275161\n",
      "Iteration 1411, the loss is 4.519038275364961, parameters k is 11.438071012183206 and b is -49.19611631429311\n",
      "Iteration 1412, the loss is 4.519035027457285, parameters k is 11.437984510206922 and b is -49.19595821152631\n",
      "Iteration 1413, the loss is 4.519031779549604, parameters k is 11.437898008230638 and b is -49.195800108759514\n",
      "Iteration 1414, the loss is 4.519029226418669, parameters k is 11.437811506254354 and b is -49.19564200599272\n",
      "Iteration 1415, the loss is 4.519026816926774, parameters k is 11.437974490444077 and b is -49.195444377534216\n",
      "Iteration 1416, the loss is 4.519023569019099, parameters k is 11.437887988467793 and b is -49.19528627476742\n",
      "Iteration 1417, the loss is 4.519020321111415, parameters k is 11.437801486491509 and b is -49.19512817200062\n",
      "Iteration 1418, the loss is 4.519017520187823, parameters k is 11.437714984515225 and b is -49.194970069233825\n",
      "Iteration 1419, the loss is 4.519015358488584, parameters k is 11.437877968704948 and b is -49.194772440775324\n",
      "Iteration 1420, the loss is 4.519012110580909, parameters k is 11.437791466728664 and b is -49.19461433800853\n",
      "Iteration 1421, the loss is 4.51900886267323, parameters k is 11.43770496475238 and b is -49.19445623524173\n",
      "Iteration 1422, the loss is 4.519005813956984, parameters k is 11.437618462776095 and b is -49.194298132474934\n",
      "Iteration 1423, the loss is 4.519003900050398, parameters k is 11.437781446965818 and b is -49.19410050401643\n",
      "Iteration 1424, the loss is 4.5190006521427195, parameters k is 11.437694944989534 and b is -49.193942401249636\n",
      "Iteration 1425, the loss is 4.518997404235041, parameters k is 11.43760844301325 and b is -49.19378429848284\n",
      "Iteration 1426, the loss is 4.518994156327368, parameters k is 11.437521941036966 and b is -49.19362619571604\n",
      "Iteration 1427, the loss is 4.518992393010988, parameters k is 11.437435439060682 and b is -49.193468092949246\n",
      "Iteration 1428, the loss is 4.518989193704533, parameters k is 11.437598423250405 and b is -49.193270464490745\n",
      "Iteration 1429, the loss is 4.518985945796852, parameters k is 11.43751192127412 and b is -49.19311236172395\n",
      "Iteration 1430, the loss is 4.51898269788918, parameters k is 11.437425419297837 and b is -49.19295425895715\n",
      "Iteration 1431, the loss is 4.518980686780147, parameters k is 11.437338917321553 and b is -49.192796156190354\n",
      "Iteration 1432, the loss is 4.518977735266344, parameters k is 11.437501901511276 and b is -49.19259852773185\n",
      "Iteration 1433, the loss is 4.518974487358666, parameters k is 11.437415399534991 and b is -49.192440424965056\n",
      "Iteration 1434, the loss is 4.518971239450989, parameters k is 11.437328897558707 and b is -49.19228232219826\n",
      "Iteration 1435, the loss is 4.518968980549301, parameters k is 11.437242395582423 and b is -49.19212421943146\n",
      "Iteration 1436, the loss is 4.518966276828151, parameters k is 11.437405379772146 and b is -49.19192659097296\n",
      "Iteration 1437, the loss is 4.518963028920477, parameters k is 11.437318877795862 and b is -49.191768488206165\n",
      "Iteration 1438, the loss is 4.518959781012799, parameters k is 11.437232375819578 and b is -49.19161038543937\n",
      "Iteration 1439, the loss is 4.518957274318459, parameters k is 11.437145873843294 and b is -49.19145228267257\n",
      "Iteration 1440, the loss is 4.518954818389967, parameters k is 11.437308858033017 and b is -49.19125465421407\n",
      "Iteration 1441, the loss is 4.518951570482287, parameters k is 11.437222356056733 and b is -49.191096551447274\n",
      "Iteration 1442, the loss is 4.518948322574615, parameters k is 11.437135854080449 and b is -49.19093844868048\n",
      "Iteration 1443, the loss is 4.5189455680876165, parameters k is 11.437049352104165 and b is -49.19078034591368\n",
      "Iteration 1444, the loss is 4.518943359951782, parameters k is 11.437212336293888 and b is -49.19058271745518\n",
      "Iteration 1445, the loss is 4.518940112044097, parameters k is 11.437125834317603 and b is -49.19042461468838\n",
      "Iteration 1446, the loss is 4.518936864136426, parameters k is 11.43703933234132 and b is -49.190266511921585\n",
      "Iteration 1447, the loss is 4.518933861856777, parameters k is 11.436952830365035 and b is -49.19010840915479\n",
      "Iteration 1448, the loss is 4.518931901513586, parameters k is 11.437115814554758 and b is -49.18991078069629\n",
      "Iteration 1449, the loss is 4.518928653605915, parameters k is 11.437029312578474 and b is -49.18975267792949\n",
      "Iteration 1450, the loss is 4.518925405698235, parameters k is 11.43694281060219 and b is -49.189594575162694\n",
      "Iteration 1451, the loss is 4.518922157790557, parameters k is 11.436856308625906 and b is -49.1894364723959\n",
      "Iteration 1452, the loss is 4.518920440910773, parameters k is 11.436769806649622 and b is -49.1892783696291\n",
      "Iteration 1453, the loss is 4.518917195167724, parameters k is 11.436932790839345 and b is -49.1890807411706\n",
      "Iteration 1454, the loss is 4.518913947260049, parameters k is 11.43684628886306 and b is -49.1889226384038\n",
      "Iteration 1455, the loss is 4.518910699352373, parameters k is 11.436759786886777 and b is -49.188764535637006\n",
      "Iteration 1456, the loss is 4.518908734679929, parameters k is 11.436673284910492 and b is -49.18860643287021\n",
      "Iteration 1457, the loss is 4.518905736729541, parameters k is 11.436836269100215 and b is -49.18840880441171\n",
      "Iteration 1458, the loss is 4.518902488821856, parameters k is 11.436749767123931 and b is -49.18825070164491\n",
      "Iteration 1459, the loss is 4.518899240914182, parameters k is 11.436663265147647 and b is -49.188092598878114\n",
      "Iteration 1460, the loss is 4.518897028449088, parameters k is 11.436576763171363 and b is -49.18793449611132\n",
      "Iteration 1461, the loss is 4.518894278291351, parameters k is 11.436739747361086 and b is -49.187736867652816\n",
      "Iteration 1462, the loss is 4.518891030383672, parameters k is 11.436653245384802 and b is -49.18757876488602\n",
      "Iteration 1463, the loss is 4.518887782475991, parameters k is 11.436566743408518 and b is -49.18742066211922\n",
      "Iteration 1464, the loss is 4.51888532221825, parameters k is 11.436480241432234 and b is -49.187262559352426\n",
      "Iteration 1465, the loss is 4.518882819853159, parameters k is 11.436643225621957 and b is -49.187064930893925\n",
      "Iteration 1466, the loss is 4.518879571945485, parameters k is 11.436556723645673 and b is -49.18690682812713\n",
      "Iteration 1467, the loss is 4.518876324037805, parameters k is 11.436470221669389 and b is -49.18674872536033\n",
      "Iteration 1468, the loss is 4.5188736159874034, parameters k is 11.436383719693104 and b is -49.186590622593535\n",
      "Iteration 1469, the loss is 4.518871361414968, parameters k is 11.436546703882827 and b is -49.18639299413503\n",
      "Iteration 1470, the loss is 4.518868113507294, parameters k is 11.436460201906543 and b is -49.18623489136824\n",
      "Iteration 1471, the loss is 4.518864865599617, parameters k is 11.43637369993026 and b is -49.18607678860144\n",
      "Iteration 1472, the loss is 4.5188619097565645, parameters k is 11.436287197953975 and b is -49.18591868583464\n",
      "Iteration 1473, the loss is 4.518859902976786, parameters k is 11.436450182143698 and b is -49.18572105737614\n",
      "Iteration 1474, the loss is 4.518856655069102, parameters k is 11.436363680167414 and b is -49.185562954609345\n",
      "Iteration 1475, the loss is 4.518853407161429, parameters k is 11.43627717819113 and b is -49.18540485184255\n",
      "Iteration 1476, the loss is 4.518850203525722, parameters k is 11.436190676214846 and b is -49.18524674907575\n",
      "Iteration 1477, the loss is 4.518848444538597, parameters k is 11.436353660404569 and b is -49.18504912061725\n",
      "Iteration 1478, the loss is 4.518845196630913, parameters k is 11.436267158428285 and b is -49.184891017850454\n",
      "Iteration 1479, the loss is 4.518841948723238, parameters k is 11.436180656452 and b is -49.18473291508366\n",
      "Iteration 1480, the loss is 4.51883870081556, parameters k is 11.436094154475716 and b is -49.18457481231686\n",
      "Iteration 1481, the loss is 4.51883678257972, parameters k is 11.436007652499432 and b is -49.184416709550064\n",
      "Iteration 1482, the loss is 4.51883373819273, parameters k is 11.436170636689155 and b is -49.18421908109156\n",
      "Iteration 1483, the loss is 4.518830490285053, parameters k is 11.436084134712871 and b is -49.184060978324766\n",
      "Iteration 1484, the loss is 4.518827242377373, parameters k is 11.435997632736587 and b is -49.18390287555797\n",
      "Iteration 1485, the loss is 4.518825076348873, parameters k is 11.435911130760303 and b is -49.18374477279117\n",
      "Iteration 1486, the loss is 4.518822279754541, parameters k is 11.436074114950026 and b is -49.18354714433267\n",
      "Iteration 1487, the loss is 4.5188190318468635, parameters k is 11.435987612973742 and b is -49.183389041565874\n",
      "Iteration 1488, the loss is 4.5188157839391865, parameters k is 11.435901110997458 and b is -49.18323093879908\n",
      "Iteration 1489, the loss is 4.518813370118031, parameters k is 11.435814609021174 and b is -49.18307283603228\n",
      "Iteration 1490, the loss is 4.518810821316354, parameters k is 11.435977593210897 and b is -49.18287520757378\n",
      "Iteration 1491, the loss is 4.5188075734086794, parameters k is 11.435891091234613 and b is -49.18271710480698\n",
      "Iteration 1492, the loss is 4.518804325500999, parameters k is 11.435804589258328 and b is -49.182559002040186\n",
      "Iteration 1493, the loss is 4.51880166388719, parameters k is 11.435718087282044 and b is -49.18240089927339\n",
      "Iteration 1494, the loss is 4.518799362878167, parameters k is 11.435881071471767 and b is -49.18220327081489\n",
      "Iteration 1495, the loss is 4.518796114970487, parameters k is 11.435794569495483 and b is -49.18204516804809\n",
      "Iteration 1496, the loss is 4.5187928670628095, parameters k is 11.435708067519199 and b is -49.181887065281295\n",
      "Iteration 1497, the loss is 4.518789957656349, parameters k is 11.435621565542915 and b is -49.1817289625145\n",
      "Iteration 1498, the loss is 4.518787904439974, parameters k is 11.435784549732638 and b is -49.181531334056\n",
      "Iteration 1499, the loss is 4.518784656532303, parameters k is 11.435698047756354 and b is -49.1813732312892\n",
      "Iteration 1500, the loss is 4.5187814086246245, parameters k is 11.43561154578007 and b is -49.1812151285224\n",
      "Iteration 1501, the loss is 4.518778251425502, parameters k is 11.435525043803786 and b is -49.18105702575561\n",
      "Iteration 1502, the loss is 4.518776446001785, parameters k is 11.435688027993509 and b is -49.180859397297105\n",
      "Iteration 1503, the loss is 4.51877319809411, parameters k is 11.435601526017225 and b is -49.18070129453031\n",
      "Iteration 1504, the loss is 4.5187699501864325, parameters k is 11.43551502404094 and b is -49.18054319176351\n",
      "Iteration 1505, the loss is 4.518766702278755, parameters k is 11.435428522064656 and b is -49.180385088996715\n",
      "Iteration 1506, the loss is 4.51876483047951, parameters k is 11.435342020088372 and b is -49.18022698622992\n",
      "Iteration 1507, the loss is 4.518761739655921, parameters k is 11.435505004278095 and b is -49.18002935777142\n",
      "Iteration 1508, the loss is 4.518758491748241, parameters k is 11.435418502301811 and b is -49.17987125500462\n",
      "Iteration 1509, the loss is 4.518755243840564, parameters k is 11.435332000325527 and b is -49.179713152237824\n",
      "Iteration 1510, the loss is 4.518753124248664, parameters k is 11.435245498349243 and b is -49.17955504947103\n",
      "Iteration 1511, the loss is 4.518750281217731, parameters k is 11.435408482538966 and b is -49.179357421012526\n",
      "Iteration 1512, the loss is 4.518747033310059, parameters k is 11.435321980562682 and b is -49.17919931824573\n",
      "Iteration 1513, the loss is 4.5187437854023775, parameters k is 11.435235478586398 and b is -49.17904121547893\n",
      "Iteration 1514, the loss is 4.518741418017819, parameters k is 11.435148976610114 and b is -49.178883112712136\n",
      "Iteration 1515, the loss is 4.5187388227795395, parameters k is 11.435311960799837 and b is -49.178685484253634\n",
      "Iteration 1516, the loss is 4.518735574871866, parameters k is 11.435225458823552 and b is -49.17852738148684\n",
      "Iteration 1517, the loss is 4.518732326964193, parameters k is 11.435138956847268 and b is -49.17836927872004\n",
      "Iteration 1518, the loss is 4.518729711786978, parameters k is 11.435052454870984 and b is -49.178211175953244\n",
      "Iteration 1519, the loss is 4.518727364341358, parameters k is 11.435215439060707 and b is -49.17801354749474\n",
      "Iteration 1520, the loss is 4.51872411643368, parameters k is 11.435128937084423 and b is -49.177855444727946\n",
      "Iteration 1521, the loss is 4.518720868526003, parameters k is 11.435042435108139 and b is -49.17769734196115\n",
      "Iteration 1522, the loss is 4.5187180055561385, parameters k is 11.434955933131855 and b is -49.17753923919435\n",
      "Iteration 1523, the loss is 4.518715905903168, parameters k is 11.435118917321578 and b is -49.17734161073585\n",
      "Iteration 1524, the loss is 4.518712657995493, parameters k is 11.435032415345294 and b is -49.177183507969055\n",
      "Iteration 1525, the loss is 4.518709410087817, parameters k is 11.43494591336901 and b is -49.17702540520226\n",
      "Iteration 1526, the loss is 4.518706299325295, parameters k is 11.434859411392726 and b is -49.17686730243546\n",
      "Iteration 1527, the loss is 4.518704447464976, parameters k is 11.435022395582449 and b is -49.17666967397696\n",
      "Iteration 1528, the loss is 4.518701199557305, parameters k is 11.434935893606164 and b is -49.17651157121016\n",
      "Iteration 1529, the loss is 4.518697951649625, parameters k is 11.43484939162988 and b is -49.17635346844337\n",
      "Iteration 1530, the loss is 4.518694703741945, parameters k is 11.434762889653596 and b is -49.17619536567657\n",
      "Iteration 1531, the loss is 4.518692878379293, parameters k is 11.434676387677312 and b is -49.17603726290977\n",
      "Iteration 1532, the loss is 4.518689741119118, parameters k is 11.434839371867035 and b is -49.17583963445127\n",
      "Iteration 1533, the loss is 4.518686493211441, parameters k is 11.434752869890751 and b is -49.175681531684475\n",
      "Iteration 1534, the loss is 4.5186832453037615, parameters k is 11.434666367914467 and b is -49.17552342891768\n",
      "Iteration 1535, the loss is 4.518681172148448, parameters k is 11.434579865938183 and b is -49.17536532615088\n",
      "Iteration 1536, the loss is 4.518678282680922, parameters k is 11.434742850127906 and b is -49.17516769769238\n",
      "Iteration 1537, the loss is 4.51867503477325, parameters k is 11.434656348151622 and b is -49.175009594925584\n",
      "Iteration 1538, the loss is 4.518671786865574, parameters k is 11.434569846175338 and b is -49.17485149215879\n",
      "Iteration 1539, the loss is 4.518669465917609, parameters k is 11.434483344199053 and b is -49.17469338939199\n",
      "Iteration 1540, the loss is 4.518666824242732, parameters k is 11.434646328388776 and b is -49.17449576093349\n",
      "Iteration 1541, the loss is 4.518663576335058, parameters k is 11.434559826412492 and b is -49.17433765816669\n",
      "Iteration 1542, the loss is 4.518660328427381, parameters k is 11.434473324436208 and b is -49.174179555399895\n",
      "Iteration 1543, the loss is 4.518657759686766, parameters k is 11.434386822459924 and b is -49.1740214526331\n",
      "Iteration 1544, the loss is 4.5186553658045465, parameters k is 11.434549806649647 and b is -49.1738238241746\n",
      "Iteration 1545, the loss is 4.518652117896873, parameters k is 11.434463304673363 and b is -49.1736657214078\n",
      "Iteration 1546, the loss is 4.518648869989198, parameters k is 11.434376802697079 and b is -49.173507618641004\n",
      "Iteration 1547, the loss is 4.5186460534559245, parameters k is 11.434290300720795 and b is -49.17334951587421\n",
      "Iteration 1548, the loss is 4.5186439073663625, parameters k is 11.434453284910518 and b is -49.173151887415706\n",
      "Iteration 1549, the loss is 4.518640659458685, parameters k is 11.434366782934234 and b is -49.17299378464891\n",
      "Iteration 1550, the loss is 4.518637411551004, parameters k is 11.43428028095795 and b is -49.17283568188211\n",
      "Iteration 1551, the loss is 4.518634347225079, parameters k is 11.434193778981665 and b is -49.172677579115316\n",
      "Iteration 1552, the loss is 4.518632448928169, parameters k is 11.434356763171388 and b is -49.172479950656815\n",
      "Iteration 1553, the loss is 4.518629201020494, parameters k is 11.434270261195104 and b is -49.17232184789002\n",
      "Iteration 1554, the loss is 4.51862595311282, parameters k is 11.43418375921882 and b is -49.17216374512322\n",
      "Iteration 1555, the loss is 4.518622705205143, parameters k is 11.434097257242536 and b is -49.172005642356424\n",
      "Iteration 1556, the loss is 4.518620926279079, parameters k is 11.434010755266252 and b is -49.17184753958963\n",
      "Iteration 1557, the loss is 4.518617742582305, parameters k is 11.434173739455975 and b is -49.17164991113113\n",
      "Iteration 1558, the loss is 4.518614494674628, parameters k is 11.434087237479691 and b is -49.17149180836433\n",
      "Iteration 1559, the loss is 4.518611246766952, parameters k is 11.434000735503407 and b is -49.17133370559753\n",
      "Iteration 1560, the loss is 4.518609220048241, parameters k is 11.433914233527123 and b is -49.171175602830736\n",
      "Iteration 1561, the loss is 4.518606284144125, parameters k is 11.434077217716846 and b is -49.170977974372235\n",
      "Iteration 1562, the loss is 4.51860303623644, parameters k is 11.433990715740562 and b is -49.17081987160544\n",
      "Iteration 1563, the loss is 4.518599788328763, parameters k is 11.433904213764277 and b is -49.17066176883864\n",
      "Iteration 1564, the loss is 4.518597513817393, parameters k is 11.433817711787993 and b is -49.170503666071845\n",
      "Iteration 1565, the loss is 4.518594825705928, parameters k is 11.433980695977716 and b is -49.170306037613344\n",
      "Iteration 1566, the loss is 4.518591577798254, parameters k is 11.433894194001432 and b is -49.17014793484655\n",
      "Iteration 1567, the loss is 4.518588329890575, parameters k is 11.433807692025148 and b is -49.16998983207975\n",
      "Iteration 1568, the loss is 4.518585807586554, parameters k is 11.433721190048864 and b is -49.16983172931295\n",
      "Iteration 1569, the loss is 4.518583367267742, parameters k is 11.433884174238587 and b is -49.16963410085445\n",
      "Iteration 1570, the loss is 4.518580119360067, parameters k is 11.433797672262303 and b is -49.169475998087655\n",
      "Iteration 1571, the loss is 4.518576871452391, parameters k is 11.433711170286019 and b is -49.16931789532086\n",
      "Iteration 1572, the loss is 4.518574101355713, parameters k is 11.433624668309735 and b is -49.16915979255406\n",
      "Iteration 1573, the loss is 4.518571908829554, parameters k is 11.433787652499458 and b is -49.16896216409556\n",
      "Iteration 1574, the loss is 4.5185686609218765, parameters k is 11.433701150523174 and b is -49.168804061328764\n",
      "Iteration 1575, the loss is 4.518565413014202, parameters k is 11.43361464854689 and b is -49.16864595856197\n",
      "Iteration 1576, the loss is 4.518562395124869, parameters k is 11.433528146570605 and b is -49.16848785579517\n",
      "Iteration 1577, the loss is 4.518560450391369, parameters k is 11.433691130760328 and b is -49.16829022733667\n",
      "Iteration 1578, the loss is 4.518557202483689, parameters k is 11.433604628784044 and b is -49.16813212456987\n",
      "Iteration 1579, the loss is 4.5185539545760145, parameters k is 11.43351812680776 and b is -49.167974021803076\n",
      "Iteration 1580, the loss is 4.518550706668338, parameters k is 11.433431624831476 and b is -49.16781591903628\n",
      "Iteration 1581, the loss is 4.518548974178874, parameters k is 11.433345122855192 and b is -49.16765781626948\n",
      "Iteration 1582, the loss is 4.518545744045501, parameters k is 11.433508107044915 and b is -49.16746018781098\n",
      "Iteration 1583, the loss is 4.518542496137822, parameters k is 11.43342160506863 and b is -49.167302085044184\n",
      "Iteration 1584, the loss is 4.518539248230147, parameters k is 11.433335103092347 and b is -49.16714398227739\n",
      "Iteration 1585, the loss is 4.518537267948034, parameters k is 11.433248601116063 and b is -49.16698587951059\n",
      "Iteration 1586, the loss is 4.51853428560731, parameters k is 11.433411585305786 and b is -49.16678825105209\n",
      "Iteration 1587, the loss is 4.51853103769963, parameters k is 11.433325083329501 and b is -49.16663014828529\n",
      "Iteration 1588, the loss is 4.518527789791959, parameters k is 11.433238581353217 and b is -49.166472045518496\n",
      "Iteration 1589, the loss is 4.518525561717182, parameters k is 11.433152079376933 and b is -49.1663139427517\n",
      "Iteration 1590, the loss is 4.518522827169125, parameters k is 11.433315063566656 and b is -49.1661163142932\n",
      "Iteration 1591, the loss is 4.518519579261448, parameters k is 11.433228561590372 and b is -49.1659582115264\n",
      "Iteration 1592, the loss is 4.518516331353766, parameters k is 11.433142059614088 and b is -49.165800108759605\n",
      "Iteration 1593, the loss is 4.518513855486344, parameters k is 11.433055557637804 and b is -49.16564200599281\n",
      "Iteration 1594, the loss is 4.518511368730938, parameters k is 11.433218541827527 and b is -49.16544437753431\n",
      "Iteration 1595, the loss is 4.518508120823256, parameters k is 11.433132039851243 and b is -49.16528627476751\n",
      "Iteration 1596, the loss is 4.518504872915582, parameters k is 11.433045537874959 and b is -49.16512817200071\n",
      "Iteration 1597, the loss is 4.518502149255495, parameters k is 11.432959035898675 and b is -49.16497006923392\n",
      "Iteration 1598, the loss is 4.518499910292746, parameters k is 11.433122020088398 and b is -49.164772440775415\n",
      "Iteration 1599, the loss is 4.518496662385067, parameters k is 11.433035518112113 and b is -49.16461433800862\n",
      "Iteration 1600, the loss is 4.518493414477388, parameters k is 11.43294901613583 and b is -49.16445623524182\n",
      "Iteration 1601, the loss is 4.51849044302466, parameters k is 11.432862514159545 and b is -49.164298132475025\n",
      "Iteration 1602, the loss is 4.518488451854558, parameters k is 11.433025498349268 and b is -49.164100504016524\n",
      "Iteration 1603, the loss is 4.518485203946889, parameters k is 11.432938996372984 and b is -49.16394240124973\n",
      "Iteration 1604, the loss is 4.518481956039204, parameters k is 11.4328524943967 and b is -49.16378429848293\n",
      "Iteration 1605, the loss is 4.518478736793814, parameters k is 11.432765992420416 and b is -49.163626195716134\n",
      "Iteration 1606, the loss is 4.5184769934163675, parameters k is 11.432928976610139 and b is -49.16342856725763\n",
      "Iteration 1607, the loss is 4.518473745508697, parameters k is 11.432842474633855 and b is -49.163270464490836\n",
      "Iteration 1608, the loss is 4.51847049760102, parameters k is 11.43275597265757 and b is -49.16311236172404\n",
      "Iteration 1609, the loss is 4.518467249693336, parameters k is 11.432669470681287 and b is -49.16295425895724\n",
      "Iteration 1610, the loss is 4.5184653158478145, parameters k is 11.432582968705002 and b is -49.162796156190446\n",
      "Iteration 1611, the loss is 4.518462287070506, parameters k is 11.432745952894726 and b is -49.162598527731944\n",
      "Iteration 1612, the loss is 4.51845903916283, parameters k is 11.432659450918441 and b is -49.16244042496515\n",
      "Iteration 1613, the loss is 4.5184557912551515, parameters k is 11.432572948942157 and b is -49.16228232219835\n",
      "Iteration 1614, the loss is 4.518453609616969, parameters k is 11.432486446965873 and b is -49.162124219431554\n",
      "Iteration 1615, the loss is 4.5184508286323135, parameters k is 11.432649431155596 and b is -49.16192659097305\n",
      "Iteration 1616, the loss is 4.518447580724641, parameters k is 11.432562929179312 and b is -49.161768488206256\n",
      "Iteration 1617, the loss is 4.518444332816965, parameters k is 11.432476427203028 and b is -49.16161038543946\n",
      "Iteration 1618, the loss is 4.518441903386129, parameters k is 11.432389925226744 and b is -49.16145228267266\n",
      "Iteration 1619, the loss is 4.5184393701941294, parameters k is 11.432552909416467 and b is -49.16125465421416\n",
      "Iteration 1620, the loss is 4.518436122286455, parameters k is 11.432466407440183 and b is -49.161096551447365\n",
      "Iteration 1621, the loss is 4.5184328743787665, parameters k is 11.432379905463899 and b is -49.16093844868057\n",
      "Iteration 1622, the loss is 4.51843019715529, parameters k is 11.432293403487614 and b is -49.16078034591377\n",
      "Iteration 1623, the loss is 4.5184279117559365, parameters k is 11.432456387677338 and b is -49.16058271745527\n",
      "Iteration 1624, the loss is 4.518424663848261, parameters k is 11.432369885701053 and b is -49.16042461468847\n",
      "Iteration 1625, the loss is 4.518421415940584, parameters k is 11.43228338372477 and b is -49.16026651192168\n",
      "Iteration 1626, the loss is 4.518418490924445, parameters k is 11.432196881748485 and b is -49.16010840915488\n",
      "Iteration 1627, the loss is 4.5184164533177515, parameters k is 11.432359865938208 and b is -49.15991078069638\n",
      "Iteration 1628, the loss is 4.518413205410072, parameters k is 11.432273363961924 and b is -49.15975267792958\n",
      "Iteration 1629, the loss is 4.518409957502396, parameters k is 11.43218686198564 and b is -49.159594575162785\n",
      "Iteration 1630, the loss is 4.518406784693604, parameters k is 11.432100360009356 and b is -49.15943647239599\n",
      "Iteration 1631, the loss is 4.518404994879564, parameters k is 11.432263344199079 and b is -49.15923884393749\n",
      "Iteration 1632, the loss is 4.518401746971883, parameters k is 11.432176842222795 and b is -49.15908074117069\n",
      "Iteration 1633, the loss is 4.518398499064211, parameters k is 11.43209034024651 and b is -49.158922638403894\n",
      "Iteration 1634, the loss is 4.51839525115653, parameters k is 11.432003838270226 and b is -49.1587645356371\n",
      "Iteration 1635, the loss is 4.518393363747605, parameters k is 11.431917336293942 and b is -49.1586064328703\n",
      "Iteration 1636, the loss is 4.518390288533702, parameters k is 11.432080320483665 and b is -49.1584088044118\n",
      "Iteration 1637, the loss is 4.5183870406260205, parameters k is 11.431993818507381 and b is -49.158250701645\n",
      "Iteration 1638, the loss is 4.518383792718343, parameters k is 11.431907316531097 and b is -49.158092598878206\n",
      "Iteration 1639, the loss is 4.518381657516762, parameters k is 11.431820814554813 and b is -49.15793449611141\n",
      "Iteration 1640, the loss is 4.518378830095511, parameters k is 11.431983798744536 and b is -49.15773686765291\n",
      "Iteration 1641, the loss is 4.518375582187832, parameters k is 11.431897296768252 and b is -49.15757876488611\n",
      "Iteration 1642, the loss is 4.518372334280153, parameters k is 11.431810794791968 and b is -49.157420662119314\n",
      "Iteration 1643, the loss is 4.518369951285917, parameters k is 11.431724292815684 and b is -49.15726255935252\n",
      "Iteration 1644, the loss is 4.518367371657322, parameters k is 11.431887277005407 and b is -49.157064930894016\n",
      "Iteration 1645, the loss is 4.518364123749646, parameters k is 11.431800775029123 and b is -49.15690682812722\n",
      "Iteration 1646, the loss is 4.518360875841967, parameters k is 11.431714273052838 and b is -49.15674872536042\n",
      "Iteration 1647, the loss is 4.518358245055078, parameters k is 11.431627771076554 and b is -49.156590622593626\n",
      "Iteration 1648, the loss is 4.518355913219133, parameters k is 11.431790755266277 and b is -49.156392994135125\n",
      "Iteration 1649, the loss is 4.5183526653114585, parameters k is 11.431704253289993 and b is -49.15623489136833\n",
      "Iteration 1650, the loss is 4.51834941740378, parameters k is 11.43161775131371 and b is -49.15607678860153\n",
      "Iteration 1651, the loss is 4.5183465388242325, parameters k is 11.431531249337425 and b is -49.155918685834735\n",
      "Iteration 1652, the loss is 4.518344454780946, parameters k is 11.431694233527148 and b is -49.15572105737623\n",
      "Iteration 1653, the loss is 4.518341206873274, parameters k is 11.431607731550864 and b is -49.15556295460944\n",
      "Iteration 1654, the loss is 4.518337958965591, parameters k is 11.43152122957458 and b is -49.15540485184264\n",
      "Iteration 1655, the loss is 4.518334832593386, parameters k is 11.431434727598296 and b is -49.15524674907584\n",
      "Iteration 1656, the loss is 4.518332996342754, parameters k is 11.431597711788019 and b is -49.15504912061734\n",
      "Iteration 1657, the loss is 4.518329748435076, parameters k is 11.431511209811735 and b is -49.154891017850545\n",
      "Iteration 1658, the loss is 4.518326500527401, parameters k is 11.43142470783545 and b is -49.15473291508375\n",
      "Iteration 1659, the loss is 4.518323252619729, parameters k is 11.431338205859166 and b is -49.15457481231695\n",
      "Iteration 1660, the loss is 4.51832170621551, parameters k is 11.431251703882882 and b is -49.154416709550155\n",
      "Iteration 1661, the loss is 4.518319859089442, parameters k is 11.431668324436242 and b is -49.154179555399956\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1662, the loss is 4.51831661118177, parameters k is 11.431581822459957 and b is -49.15402145263316\n",
      "Iteration 1663, the loss is 4.5183133632740855, parameters k is 11.431495320483673 and b is -49.15386334986636\n",
      "Iteration 1664, the loss is 4.5183101153664085, parameters k is 11.43140881850739 and b is -49.153705247099566\n",
      "Iteration 1665, the loss is 4.518306867458731, parameters k is 11.431322316531105 and b is -49.15354714433277\n",
      "Iteration 1666, the loss is 4.518303619551059, parameters k is 11.431235814554821 and b is -49.15338904156597\n",
      "Iteration 1667, the loss is 4.518300371643381, parameters k is 11.431149312578537 and b is -49.153230938799176\n",
      "Iteration 1668, the loss is 4.518297931544047, parameters k is 11.431062810602253 and b is -49.15307283603238\n",
      "Iteration 1669, the loss is 4.518295409020544, parameters k is 11.431225794791976 and b is -49.15287520757388\n",
      "Iteration 1670, the loss is 4.518292161112868, parameters k is 11.431139292815692 and b is -49.15271710480708\n",
      "Iteration 1671, the loss is 4.518288913205191, parameters k is 11.431052790839408 and b is -49.152559002040284\n",
      "Iteration 1672, the loss is 4.518286225313204, parameters k is 11.430966288863123 and b is -49.15240089927349\n",
      "Iteration 1673, the loss is 4.518283950582355, parameters k is 11.431129273052846 and b is -49.15220327081499\n",
      "Iteration 1674, the loss is 4.518280702674679, parameters k is 11.431042771076562 and b is -49.15204516804819\n",
      "Iteration 1675, the loss is 4.5182774547669995, parameters k is 11.430956269100278 and b is -49.15188706528139\n",
      "Iteration 1676, the loss is 4.51827451908236, parameters k is 11.430869767123994 and b is -49.151728962514596\n",
      "Iteration 1677, the loss is 4.518272492144165, parameters k is 11.431032751313717 and b is -49.151531334056095\n",
      "Iteration 1678, the loss is 4.518269244236492, parameters k is 11.430946249337433 and b is -49.1513732312893\n",
      "Iteration 1679, the loss is 4.518265996328813, parameters k is 11.430859747361149 and b is -49.1512151285225\n",
      "Iteration 1680, the loss is 4.518262812851518, parameters k is 11.430773245384865 and b is -49.151057025755705\n",
      "Iteration 1681, the loss is 4.518261033705981, parameters k is 11.430936229574588 and b is -49.150859397297204\n",
      "Iteration 1682, the loss is 4.518257785798304, parameters k is 11.430849727598304 and b is -49.15070129453041\n",
      "Iteration 1683, the loss is 4.518254537890627, parameters k is 11.43076322562202 and b is -49.15054319176361\n",
      "Iteration 1684, the loss is 4.518251289982949, parameters k is 11.430676723645735 and b is -49.15038508899681\n",
      "Iteration 1685, the loss is 4.518249903896731, parameters k is 11.430590221669451 and b is -49.15022698623002\n",
      "Iteration 1686, the loss is 4.51824789645266, parameters k is 11.43100684222281 and b is -49.14998983207982\n",
      "Iteration 1687, the loss is 4.518244648544987, parameters k is 11.430920340246526 and b is -49.14983172931302\n",
      "Iteration 1688, the loss is 4.518241400637308, parameters k is 11.430833838270242 and b is -49.149673626546225\n",
      "Iteration 1689, the loss is 4.518238152729636, parameters k is 11.430747336293958 and b is -49.14951552377943\n",
      "Iteration 1690, the loss is 4.518234904821957, parameters k is 11.430660834317674 and b is -49.14935742101263\n",
      "Iteration 1691, the loss is 4.518231656914282, parameters k is 11.43057433234139 and b is -49.149199318245834\n",
      "Iteration 1692, the loss is 4.518228409006602, parameters k is 11.430487830365106 and b is -49.14904121547904\n",
      "Iteration 1693, the loss is 4.518225911802174, parameters k is 11.430401328388822 and b is -49.14888311271224\n",
      "Iteration 1694, the loss is 4.518223446383764, parameters k is 11.430564312578545 and b is -49.14868548425374\n",
      "Iteration 1695, the loss is 4.518220198476086, parameters k is 11.43047781060226 and b is -49.14852738148694\n",
      "Iteration 1696, the loss is 4.518216950568414, parameters k is 11.430391308625977 and b is -49.148369278720146\n",
      "Iteration 1697, the loss is 4.518214205571332, parameters k is 11.430304806649692 and b is -49.14821117595335\n",
      "Iteration 1698, the loss is 4.51821198794558, parameters k is 11.430467790839415 and b is -49.14801354749485\n",
      "Iteration 1699, the loss is 4.518208740037901, parameters k is 11.430381288863131 and b is -49.14785544472805\n",
      "Iteration 1700, the loss is 4.5182054921302255, parameters k is 11.430294786886847 and b is -49.147697341961255\n",
      "Iteration 1701, the loss is 4.5182024993404895, parameters k is 11.430208284910563 and b is -49.14753923919446\n",
      "Iteration 1702, the loss is 4.518200529507388, parameters k is 11.430371269100286 and b is -49.14734161073596\n",
      "Iteration 1703, the loss is 4.518197281599713, parameters k is 11.430284767124002 and b is -49.14718350796916\n",
      "Iteration 1704, the loss is 4.518194033692041, parameters k is 11.430198265147718 and b is -49.14702540520236\n",
      "Iteration 1705, the loss is 4.518190793109646, parameters k is 11.430111763171434 and b is -49.14686730243557\n",
      "Iteration 1706, the loss is 4.5181890710692025, parameters k is 11.430274747361157 and b is -49.146669673977065\n",
      "Iteration 1707, the loss is 4.518185823161526, parameters k is 11.430188245384873 and b is -49.14651157121027\n",
      "Iteration 1708, the loss is 4.5181825752538485, parameters k is 11.430101743408589 and b is -49.14635346844347\n",
      "Iteration 1709, the loss is 4.518179327346172, parameters k is 11.430015241432304 and b is -49.146195365676675\n",
      "Iteration 1710, the loss is 4.518178101577956, parameters k is 11.42992873945602 and b is -49.14603726290988\n",
      "Iteration 1711, the loss is 4.518175933815889, parameters k is 11.43034536000938 and b is -49.14580010875968\n",
      "Iteration 1712, the loss is 4.518172685908212, parameters k is 11.430258858033095 and b is -49.14564200599288\n",
      "Iteration 1713, the loss is 4.51816943800053, parameters k is 11.430172356056811 and b is -49.145483903226086\n",
      "Iteration 1714, the loss is 4.518166190092854, parameters k is 11.430085854080527 and b is -49.14532580045929\n",
      "Iteration 1715, the loss is 4.518162942185179, parameters k is 11.429999352104243 and b is -49.14516769769249\n",
      "Iteration 1716, the loss is 4.518159694277505, parameters k is 11.429912850127959 and b is -49.145009594925696\n",
      "Iteration 1717, the loss is 4.518156446369826, parameters k is 11.429826348151675 and b is -49.1448514921589\n",
      "Iteration 1718, the loss is 4.51815410074206, parameters k is 11.42973984617539 and b is -49.1446933893921\n",
      "Iteration 1719, the loss is 4.518153052839538, parameters k is 11.43015646672875 and b is -49.144456235241904\n",
      "Iteration 1720, the loss is 4.518149804931862, parameters k is 11.430069964752466 and b is -49.14429813247511\n",
      "Iteration 1721, the loss is 4.518146557024185, parameters k is 11.429983462776182 and b is -49.14414002970831\n",
      "Iteration 1722, the loss is 4.518143309116505, parameters k is 11.429896960799898 and b is -49.143981926941514\n",
      "Iteration 1723, the loss is 4.518140061208825, parameters k is 11.429810458823614 and b is -49.14382382417472\n",
      "Iteration 1724, the loss is 4.518136813301154, parameters k is 11.42972395684733 and b is -49.14366572140792\n",
      "Iteration 1725, the loss is 4.518133565393481, parameters k is 11.429637454871045 and b is -49.143507618641124\n",
      "Iteration 1726, the loss is 4.5181304119569585, parameters k is 11.429550952894761 and b is -49.14334951587433\n",
      "Iteration 1727, the loss is 4.518128602770646, parameters k is 11.429713937084484 and b is -49.143151887415826\n",
      "Iteration 1728, the loss is 4.518125354862967, parameters k is 11.4296274351082 and b is -49.14299378464903\n",
      "Iteration 1729, the loss is 4.518122106955286, parameters k is 11.429540933131916 and b is -49.14283568188223\n",
      "Iteration 1730, the loss is 4.518118859047614, parameters k is 11.429454431155632 and b is -49.142677579115436\n",
      "Iteration 1731, the loss is 4.518118040318484, parameters k is 11.429367929179348 and b is -49.14251947634864\n",
      "Iteration 1732, the loss is 4.518115465517325, parameters k is 11.429784549732707 and b is -49.14228232219844\n",
      "Iteration 1733, the loss is 4.518112217609648, parameters k is 11.429698047756423 and b is -49.14212421943164\n",
      "Iteration 1734, the loss is 4.518108969701972, parameters k is 11.429611545780139 and b is -49.14196611666485\n",
      "Iteration 1735, the loss is 4.518105721794295, parameters k is 11.429525043803855 and b is -49.14180801389805\n",
      "Iteration 1736, the loss is 4.518102473886623, parameters k is 11.42943854182757 and b is -49.14164991113125\n",
      "Iteration 1737, the loss is 4.518099225978943, parameters k is 11.429352039851286 and b is -49.14149180836446\n",
      "Iteration 1738, the loss is 4.5180959780712655, parameters k is 11.429265537875002 and b is -49.14133370559766\n",
      "Iteration 1739, the loss is 4.51809403948259, parameters k is 11.429179035898718 and b is -49.14117560283086\n",
      "Iteration 1740, the loss is 4.5180925845409785, parameters k is 11.429595656452078 and b is -49.140938448680664\n",
      "Iteration 1741, the loss is 4.5180893366333, parameters k is 11.429509154475793 and b is -49.14078034591387\n",
      "Iteration 1742, the loss is 4.518086088725629, parameters k is 11.42942265249951 and b is -49.14062224314707\n",
      "Iteration 1743, the loss is 4.518082840817947, parameters k is 11.429336150523225 and b is -49.140464140380274\n",
      "Iteration 1744, the loss is 4.518079592910272, parameters k is 11.429249648546941 and b is -49.14030603761348\n",
      "Iteration 1745, the loss is 4.518076345002599, parameters k is 11.429163146570657 and b is -49.14014793484668\n",
      "Iteration 1746, the loss is 4.518073097094924, parameters k is 11.429076644594373 and b is -49.139989832079884\n",
      "Iteration 1747, the loss is 4.518070038646692, parameters k is 11.428990142618089 and b is -49.13983172931309\n",
      "Iteration 1748, the loss is 4.518069703564632, parameters k is 11.429406763171448 and b is -49.13959457516289\n",
      "Iteration 1749, the loss is 4.5180664556569585, parameters k is 11.429320261195164 and b is -49.13943647239609\n",
      "Iteration 1750, the loss is 4.518063207749277, parameters k is 11.42923375921888 and b is -49.139278369629295\n",
      "Iteration 1751, the loss is 4.518059959841601, parameters k is 11.429147257242596 and b is -49.1391202668625\n",
      "Iteration 1752, the loss is 4.518056711933927, parameters k is 11.429060755266311 and b is -49.1389621640957\n",
      "Iteration 1753, the loss is 4.518053464026248, parameters k is 11.428974253290027 and b is -49.138804061328905\n",
      "Iteration 1754, the loss is 4.518050216118574, parameters k is 11.428887751313743 and b is -49.13864595856211\n",
      "Iteration 1755, the loss is 4.518046968210893, parameters k is 11.428801249337459 and b is -49.13848785579531\n",
      "Iteration 1756, the loss is 4.518045892188177, parameters k is 11.428714747361175 and b is -49.138329753028515\n",
      "Iteration 1757, the loss is 4.518043574680607, parameters k is 11.429131367914534 and b is -49.138092598878316\n",
      "Iteration 1758, the loss is 4.518040326772932, parameters k is 11.42904486593825 and b is -49.13793449611152\n",
      "Iteration 1759, the loss is 4.518037078865255, parameters k is 11.428958363961966 and b is -49.13777639334472\n",
      "Iteration 1760, the loss is 4.518033830957579, parameters k is 11.428871861985682 and b is -49.137618290577926\n",
      "Iteration 1761, the loss is 4.518030583049903, parameters k is 11.428785360009398 and b is -49.13746018781113\n",
      "Iteration 1762, the loss is 4.518027335142228, parameters k is 11.428698858033114 and b is -49.13730208504433\n",
      "Iteration 1763, the loss is 4.518024087234543, parameters k is 11.42861235605683 and b is -49.137143982277536\n",
      "Iteration 1764, the loss is 4.5180218913522845, parameters k is 11.428525854080545 and b is -49.13698587951074\n",
      "Iteration 1765, the loss is 4.518020693704262, parameters k is 11.428942474633905 and b is -49.13674872536054\n",
      "Iteration 1766, the loss is 4.518017445796585, parameters k is 11.42885597265762 and b is -49.136590622593744\n",
      "Iteration 1767, the loss is 4.518014197888908, parameters k is 11.428769470681337 and b is -49.13643251982695\n",
      "Iteration 1768, the loss is 4.518010949981234, parameters k is 11.428682968705052 and b is -49.13627441706015\n",
      "Iteration 1769, the loss is 4.51800770207356, parameters k is 11.428596466728768 and b is -49.13611631429335\n",
      "Iteration 1770, the loss is 4.518004454165873, parameters k is 11.428509964752484 and b is -49.13595821152656\n",
      "Iteration 1771, the loss is 4.518001206258203, parameters k is 11.4284234627762 and b is -49.13580010875976\n",
      "Iteration 1772, the loss is 4.5179979730878355, parameters k is 11.428336960799916 and b is -49.13564200599296\n",
      "Iteration 1773, the loss is 4.517996279535393, parameters k is 11.428504095187268 and b is -49.13544437753446\n",
      "Iteration 1774, the loss is 4.517993031627715, parameters k is 11.428417593210984 and b is -49.135286274767665\n",
      "Iteration 1775, the loss is 4.5179897837200444, parameters k is 11.4283310912347 and b is -49.13512817200087\n",
      "Iteration 1776, the loss is 4.5179865358123665, parameters k is 11.428244589258416 and b is -49.13497006923407\n",
      "Iteration 1777, the loss is 4.517985658022953, parameters k is 11.428158087282132 and b is -49.134811966467275\n",
      "Iteration 1778, the loss is 4.517983142282079, parameters k is 11.428574707835491 and b is -49.13457481231708\n",
      "Iteration 1779, the loss is 4.5179798943744025, parameters k is 11.428488205859207 and b is -49.13441670955028\n",
      "Iteration 1780, the loss is 4.517976646466726, parameters k is 11.428401703882923 and b is -49.13425860678348\n",
      "Iteration 1781, the loss is 4.517973398559049, parameters k is 11.428315201906639 and b is -49.134100504016686\n",
      "Iteration 1782, the loss is 4.517970150651371, parameters k is 11.428228699930354 and b is -49.13394240124989\n",
      "Iteration 1783, the loss is 4.517966902743692, parameters k is 11.42814219795407 and b is -49.13378429848309\n",
      "Iteration 1784, the loss is 4.5179636548360165, parameters k is 11.428055695977786 and b is -49.133626195716296\n",
      "Iteration 1785, the loss is 4.517961657187048, parameters k is 11.427969194001502 and b is -49.1334680929495\n",
      "Iteration 1786, the loss is 4.517960261305734, parameters k is 11.428385814554861 and b is -49.1332309387993\n",
      "Iteration 1787, the loss is 4.517957013398057, parameters k is 11.428299312578577 and b is -49.133072836032504\n",
      "Iteration 1788, the loss is 4.517953765490376, parameters k is 11.428212810602293 and b is -49.13291473326571\n",
      "Iteration 1789, the loss is 4.517950517582701, parameters k is 11.428126308626009 and b is -49.13275663049891\n",
      "Iteration 1790, the loss is 4.517947269675027, parameters k is 11.428039806649725 and b is -49.132598527732114\n",
      "Iteration 1791, the loss is 4.517944021767353, parameters k is 11.42795330467344 and b is -49.13244042496532\n",
      "Iteration 1792, the loss is 4.517940773859673, parameters k is 11.427866802697157 and b is -49.13228232219852\n",
      "Iteration 1793, the loss is 4.517937755318424, parameters k is 11.427780300720872 and b is -49.132124219431724\n",
      "Iteration 1794, the loss is 4.51793584713686, parameters k is 11.427947435108225 and b is -49.13192659097322\n",
      "Iteration 1795, the loss is 4.51793259922919, parameters k is 11.42786093313194 and b is -49.131768488206426\n",
      "Iteration 1796, the loss is 4.517929351321514, parameters k is 11.427774431155656 and b is -49.13161038543963\n",
      "Iteration 1797, the loss is 4.517926103413837, parameters k is 11.427687929179372 and b is -49.13145228267283\n",
      "Iteration 1798, the loss is 4.517925423857713, parameters k is 11.427601427203088 and b is -49.131294179906035\n",
      "Iteration 1799, the loss is 4.517922709883548, parameters k is 11.428018047756447 and b is -49.13105702575584\n",
      "Iteration 1800, the loss is 4.5179194619758745, parameters k is 11.427931545780163 and b is -49.13089892298904\n",
      "Iteration 1801, the loss is 4.517916214068193, parameters k is 11.42784504380388 and b is -49.13074082022224\n",
      "Iteration 1802, the loss is 4.5179129661605195, parameters k is 11.427758541827595 and b is -49.13058271745545\n",
      "Iteration 1803, the loss is 4.517909718252838, parameters k is 11.427672039851311 and b is -49.13042461468865\n",
      "Iteration 1804, the loss is 4.517906470345166, parameters k is 11.427585537875027 and b is -49.13026651192185\n",
      "Iteration 1805, the loss is 4.517903222437489, parameters k is 11.427499035898743 and b is -49.130108409155056\n",
      "Iteration 1806, the loss is 4.51790142302182, parameters k is 11.427412533922459 and b is -49.12995030638826\n",
      "Iteration 1807, the loss is 4.517899828907204, parameters k is 11.427829154475818 and b is -49.12971315223806\n",
      "Iteration 1808, the loss is 4.517896580999525, parameters k is 11.427742652499534 and b is -49.129555049471264\n",
      "Iteration 1809, the loss is 4.51789333309185, parameters k is 11.42765615052325 and b is -49.12939694670447\n",
      "Iteration 1810, the loss is 4.517890085184175, parameters k is 11.427569648546966 and b is -49.12923884393767\n",
      "Iteration 1811, the loss is 4.517886837276493, parameters k is 11.427483146570681 and b is -49.129080741170874\n",
      "Iteration 1812, the loss is 4.517883589368818, parameters k is 11.427396644594397 and b is -49.12892263840408\n",
      "Iteration 1813, the loss is 4.5178803414611455, parameters k is 11.427310142618113 and b is -49.12876453563728\n",
      "Iteration 1814, the loss is 4.5178775375490074, parameters k is 11.427223640641829 and b is -49.128606432870484\n",
      "Iteration 1815, the loss is 4.517875414738337, parameters k is 11.427390775029181 and b is -49.12840880441198\n",
      "Iteration 1816, the loss is 4.517872166830655, parameters k is 11.427304273052897 and b is -49.128250701645186\n",
      "Iteration 1817, the loss is 4.517868918922984, parameters k is 11.427217771076613 and b is -49.12809259887839\n",
      "Iteration 1818, the loss is 4.517865802012516, parameters k is 11.427131269100329 and b is -49.12793449611159\n",
      "Iteration 1819, the loss is 4.517863992200183, parameters k is 11.427298403487681 and b is -49.12773686765309\n",
      "Iteration 1820, the loss is 4.517860744292501, parameters k is 11.427211901511397 and b is -49.127578764886294\n",
      "Iteration 1821, the loss is 4.517857496384825, parameters k is 11.427125399535113 and b is -49.1274206621195\n",
      "Iteration 1822, the loss is 4.517854248477147, parameters k is 11.427038897558829 and b is -49.1272625593527\n",
      "Iteration 1823, the loss is 4.517853102821652, parameters k is 11.426952395582544 and b is -49.127104456585904\n",
      "Iteration 1824, the loss is 4.517850854946868, parameters k is 11.427369016135904 and b is -49.126867302435706\n",
      "Iteration 1825, the loss is 4.517847607039183, parameters k is 11.42728251415962 and b is -49.12670919966891\n",
      "Iteration 1826, the loss is 4.517844359131509, parameters k is 11.427196012183336 and b is -49.12655109690211\n",
      "Iteration 1827, the loss is 4.517841111223831, parameters k is 11.427109510207051 and b is -49.126392994135315\n",
      "Iteration 1828, the loss is 4.517837863316159, parameters k is 11.427023008230767 and b is -49.12623489136852\n",
      "Iteration 1829, the loss is 4.517834615408479, parameters k is 11.426936506254483 and b is -49.12607678860172\n",
      "Iteration 1830, the loss is 4.5178313675008015, parameters k is 11.426850004278199 and b is -49.125918685834925\n",
      "Iteration 1831, the loss is 4.517829101985754, parameters k is 11.426763502301915 and b is -49.12576058306813\n",
      "Iteration 1832, the loss is 4.517827973970514, parameters k is 11.427180122855274 and b is -49.12552342891793\n",
      "Iteration 1833, the loss is 4.51782472606284, parameters k is 11.42709362087899 and b is -49.12536532615113\n",
      "Iteration 1834, the loss is 4.517821478155161, parameters k is 11.427007118902706 and b is -49.125207223384336\n",
      "Iteration 1835, the loss is 4.517818230247482, parameters k is 11.426920616926422 and b is -49.12504912061754\n",
      "Iteration 1836, the loss is 4.517814982339808, parameters k is 11.426834114950138 and b is -49.12489101785074\n",
      "Iteration 1837, the loss is 4.517811734432131, parameters k is 11.426747612973854 and b is -49.124732915083946\n",
      "Iteration 1838, the loss is 4.517808486524453, parameters k is 11.42666111099757 and b is -49.12457481231715\n",
      "Iteration 1839, the loss is 4.5178055842431, parameters k is 11.426574609021285 and b is -49.12441670955035\n",
      "Iteration 1840, the loss is 4.517803559801651, parameters k is 11.426741743408638 and b is -49.12421908109185\n",
      "Iteration 1841, the loss is 4.517800311893972, parameters k is 11.426655241432353 and b is -49.124060978325055\n",
      "Iteration 1842, the loss is 4.517797063986295, parameters k is 11.42656873945607 and b is -49.12390287555826\n",
      "Iteration 1843, the loss is 4.517793848706613, parameters k is 11.426482237479785 and b is -49.12374477279146\n",
      "Iteration 1844, the loss is 4.517792137263494, parameters k is 11.426649371867137 and b is -49.12354714433296\n",
      "Iteration 1845, the loss is 4.5177888893558125, parameters k is 11.426562869890853 and b is -49.12338904156616\n",
      "Iteration 1846, the loss is 4.517785641448139, parameters k is 11.426476367914569 and b is -49.12323093879937\n",
      "Iteration 1847, the loss is 4.51778239354046, parameters k is 11.426389865938285 and b is -49.12307283603257\n",
      "Iteration 1848, the loss is 4.517780781785587, parameters k is 11.426303363962 and b is -49.12291473326577\n",
      "Iteration 1849, the loss is 4.517779000010175, parameters k is 11.42671998451536 and b is -49.122677579115575\n",
      "Iteration 1850, the loss is 4.5177757521025015, parameters k is 11.426633482539076 and b is -49.12251947634878\n",
      "Iteration 1851, the loss is 4.517772504194822, parameters k is 11.426546980562792 and b is -49.12236137358198\n",
      "Iteration 1852, the loss is 4.517769256287143, parameters k is 11.426460478586508 and b is -49.122203270815184\n",
      "Iteration 1853, the loss is 4.517766008379468, parameters k is 11.426373976610224 and b is -49.12204516804839\n",
      "Iteration 1854, the loss is 4.517762760471789, parameters k is 11.42628747463394 and b is -49.12188706528159\n",
      "Iteration 1855, the loss is 4.517759512564108, parameters k is 11.426200972657655 and b is -49.121728962514794\n",
      "Iteration 1856, the loss is 4.5177570326461005, parameters k is 11.426114470681371 and b is -49.121570859748\n",
      "Iteration 1857, the loss is 4.517754585841308, parameters k is 11.426281605068723 and b is -49.121373231289496\n",
      "Iteration 1858, the loss is 4.517751337933627, parameters k is 11.42619510309244 and b is -49.1212151285227\n",
      "Iteration 1859, the loss is 4.517748090025955, parameters k is 11.426108601116155 and b is -49.1210570257559\n",
      "Iteration 1860, the loss is 4.5177452971096175, parameters k is 11.426022099139871 and b is -49.120898922989106\n",
      "Iteration 1861, the loss is 4.517743163303145, parameters k is 11.426189233527223 and b is -49.120701294530605\n",
      "Iteration 1862, the loss is 4.517739915395471, parameters k is 11.42610273155094 and b is -49.12054319176381\n",
      "Iteration 1863, the loss is 4.517736667487796, parameters k is 11.426016229574655 and b is -49.12038508899701\n",
      "Iteration 1864, the loss is 4.517733561573127, parameters k is 11.425929727598371 and b is -49.120226986230215\n",
      "Iteration 1865, the loss is 4.51773174076499, parameters k is 11.426096861985723 and b is -49.12002935777171\n",
      "Iteration 1866, the loss is 4.5177284928573105, parameters k is 11.426010360009439 and b is -49.11987125500492\n",
      "Iteration 1867, the loss is 4.517725244949636, parameters k is 11.425923858033155 and b is -49.11971315223812\n",
      "Iteration 1868, the loss is 4.51772199704196, parameters k is 11.42583735605687 and b is -49.11955504947132\n",
      "Iteration 1869, the loss is 4.517720374714587, parameters k is 11.425750854080587 and b is -49.119396946704526\n",
      "Iteration 1870, the loss is 4.5177186035116765, parameters k is 11.426167474633946 and b is -49.11915979255433\n",
      "Iteration 1871, the loss is 4.5177153556039995, parameters k is 11.426080972657662 and b is -49.11900168978753\n",
      "Iteration 1872, the loss is 4.517712107696322, parameters k is 11.425994470681378 and b is -49.118843587020734\n",
      "Iteration 1873, the loss is 4.517708859788644, parameters k is 11.425907968705094 and b is -49.11868548425394\n",
      "Iteration 1874, the loss is 4.517705611880967, parameters k is 11.42582146672881 and b is -49.11852738148714\n",
      "Iteration 1875, the loss is 4.517702363973292, parameters k is 11.425734964752525 and b is -49.118369278720344\n",
      "Iteration 1876, the loss is 4.517699116065611, parameters k is 11.425648462776241 and b is -49.11821117595355\n",
      "Iteration 1877, the loss is 4.517696745512612, parameters k is 11.425561960799957 and b is -49.11805307318675\n",
      "Iteration 1878, the loss is 4.517694189342809, parameters k is 11.42572909518731 and b is -49.11785544472825\n",
      "Iteration 1879, the loss is 4.517690941435132, parameters k is 11.425642593211025 and b is -49.11769734196145\n",
      "Iteration 1880, the loss is 4.517687693527454, parameters k is 11.425556091234741 and b is -49.117539239194656\n",
      "Iteration 1881, the loss is 4.517685009976126, parameters k is 11.425469589258457 and b is -49.11738113642786\n",
      "Iteration 1882, the loss is 4.517682766804651, parameters k is 11.425636723645809 and b is -49.11718350796936\n",
      "Iteration 1883, the loss is 4.517679518896973, parameters k is 11.425550221669525 and b is -49.11702540520256\n",
      "Iteration 1884, the loss is 4.5176762709892975, parameters k is 11.42546371969324 and b is -49.116867302435764\n",
      "Iteration 1885, the loss is 4.5176732744396375, parameters k is 11.425377217716957 and b is -49.11670919966897\n",
      "Iteration 1886, the loss is 4.517671344266492, parameters k is 11.425544352104309 and b is -49.11651157121047\n",
      "Iteration 1887, the loss is 4.517668096358811, parameters k is 11.425457850128025 and b is -49.11635346844367\n",
      "Iteration 1888, the loss is 4.517664848451135, parameters k is 11.42537134815174 and b is -49.11619536567687\n",
      "Iteration 1889, the loss is 4.51766160054346, parameters k is 11.425284846175456 and b is -49.116037262910076\n",
      "Iteration 1890, the loss is 4.517659967643591, parameters k is 11.425198344199172 and b is -49.11587916014328\n",
      "Iteration 1891, the loss is 4.517658207013172, parameters k is 11.425614964752532 and b is -49.11564200599308\n",
      "Iteration 1892, the loss is 4.517654959105499, parameters k is 11.425528462776247 and b is -49.115483903226284\n",
      "Iteration 1893, the loss is 4.5176517111978205, parameters k is 11.425441960799963 and b is -49.11532580045949\n",
      "Iteration 1894, the loss is 4.517648463290147, parameters k is 11.42535545882368 and b is -49.11516769769269\n",
      "Iteration 1895, the loss is 4.5176452153824656, parameters k is 11.425268956847395 and b is -49.115009594925894\n",
      "Iteration 1896, the loss is 4.517641967474792, parameters k is 11.425182454871111 and b is -49.1148514921591\n",
      "Iteration 1897, the loss is 4.517638719567113, parameters k is 11.425095952894827 and b is -49.1146933893923\n",
      "Iteration 1898, the loss is 4.517636458379127, parameters k is 11.425009450918543 and b is -49.114535286625504\n",
      "Iteration 1899, the loss is 4.517633792844307, parameters k is 11.425176585305895 and b is -49.114337658167\n",
      "Iteration 1900, the loss is 4.517630544936632, parameters k is 11.42509008332961 and b is -49.114179555400206\n",
      "Iteration 1901, the loss is 4.517627297028952, parameters k is 11.425003581353327 and b is -49.11402145263341\n",
      "Iteration 1902, the loss is 4.517624722842636, parameters k is 11.424917079377042 and b is -49.11386334986661\n",
      "Iteration 1903, the loss is 4.517622370306148, parameters k is 11.425084213764395 and b is -49.11366572140811\n",
      "Iteration 1904, the loss is 4.517619122398472, parameters k is 11.42499771178811 and b is -49.113507618641314\n",
      "Iteration 1905, the loss is 4.517615874490798, parameters k is 11.424911209811826 and b is -49.11334951587452\n",
      "Iteration 1906, the loss is 4.51761298730615, parameters k is 11.424824707835542 and b is -49.11319141310772\n",
      "Iteration 1907, the loss is 4.517610947767986, parameters k is 11.424991842222894 and b is -49.11299378464922\n",
      "Iteration 1908, the loss is 4.517607699860313, parameters k is 11.42490534024661 and b is -49.11283568188242\n",
      "Iteration 1909, the loss is 4.517604451952634, parameters k is 11.424818838270326 and b is -49.112677579115626\n",
      "Iteration 1910, the loss is 4.517601251769661, parameters k is 11.424732336294042 and b is -49.11251947634883\n",
      "Iteration 1911, the loss is 4.517599525229835, parameters k is 11.424899470681394 and b is -49.11232184789033\n",
      "Iteration 1912, the loss is 4.517596277322156, parameters k is 11.42481296870511 and b is -49.11216374512353\n",
      "Iteration 1913, the loss is 4.517593029414478, parameters k is 11.424726466728826 and b is -49.112005642356735\n",
      "Iteration 1914, the loss is 4.517589781506798, parameters k is 11.424639964752542 and b is -49.11184753958994\n",
      "Iteration 1915, the loss is 4.51758783741805, parameters k is 11.424553462776258 and b is -49.11168943682314\n",
      "Iteration 1916, the loss is 4.5175848547839985, parameters k is 11.42472059716361 and b is -49.11149180836464\n",
      "Iteration 1917, the loss is 4.517581606876318, parameters k is 11.424634095187326 and b is -49.11133370559784\n",
      "Iteration 1918, the loss is 4.517578358968641, parameters k is 11.424547593211042 and b is -49.11117560283105\n",
      "Iteration 1919, the loss is 4.517576101881564, parameters k is 11.424461091234758 and b is -49.11101750006425\n",
      "Iteration 1920, the loss is 4.517573432245841, parameters k is 11.42462822562211 and b is -49.11081987160575\n",
      "Iteration 1921, the loss is 4.517570184338158, parameters k is 11.424541723645826 and b is -49.11066176883895\n",
      "Iteration 1922, the loss is 4.5175669364304865, parameters k is 11.424455221669541 and b is -49.110503666072155\n",
      "Iteration 1923, the loss is 4.517564366345072, parameters k is 11.424368719693257 and b is -49.11034556330536\n",
      "Iteration 1924, the loss is 4.517562009707683, parameters k is 11.42453585408061 and b is -49.11014793484686\n",
      "Iteration 1925, the loss is 4.5175587617999975, parameters k is 11.424449352104325 and b is -49.10998983208006\n",
      "Iteration 1926, the loss is 4.517555513892323, parameters k is 11.424362850128041 and b is -49.109831729313264\n",
      "Iteration 1927, the loss is 4.517552630808587, parameters k is 11.424276348151757 and b is -49.10967362654647\n",
      "Iteration 1928, the loss is 4.517550587169525, parameters k is 11.42444348253911 and b is -49.109475998087966\n",
      "Iteration 1929, the loss is 4.517547339261845, parameters k is 11.424356980562825 and b is -49.10931789532117\n",
      "Iteration 1930, the loss is 4.517544091354163, parameters k is 11.424270478586541 and b is -49.10915979255437\n",
      "Iteration 1931, the loss is 4.5175408952721, parameters k is 11.424183976610257 and b is -49.109001689787576\n",
      "Iteration 1932, the loss is 4.517539164631365, parameters k is 11.424351110997609 and b is -49.108804061329074\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1933, the loss is 4.517535916723685, parameters k is 11.424264609021325 and b is -49.10864595856228\n",
      "Iteration 1934, the loss is 4.517532668816011, parameters k is 11.42417810704504 and b is -49.10848785579548\n",
      "Iteration 1935, the loss is 4.51752942090833, parameters k is 11.424091605068757 and b is -49.108329753028684\n",
      "Iteration 1936, the loss is 4.517527480920489, parameters k is 11.424005103092473 and b is -49.10817165026189\n",
      "Iteration 1937, the loss is 4.517524494185527, parameters k is 11.424172237479825 and b is -49.107974021803386\n",
      "Iteration 1938, the loss is 4.517521246277854, parameters k is 11.42408573550354 and b is -49.10781591903659\n",
      "Iteration 1939, the loss is 4.51751799837017, parameters k is 11.423999233527256 and b is -49.10765781626979\n",
      "Iteration 1940, the loss is 4.517515745384005, parameters k is 11.423912731550972 and b is -49.107499713502996\n",
      "Iteration 1941, the loss is 4.517513071647367, parameters k is 11.424079865938324 and b is -49.107302085044495\n",
      "Iteration 1942, the loss is 4.517509823739693, parameters k is 11.42399336396204 and b is -49.1071439822777\n",
      "Iteration 1943, the loss is 4.517506575832013, parameters k is 11.423906861985756 and b is -49.1069858795109\n",
      "Iteration 1944, the loss is 4.5175040098475145, parameters k is 11.423820360009472 and b is -49.106827776744105\n",
      "Iteration 1945, the loss is 4.517501649109211, parameters k is 11.423987494396824 and b is -49.1066301482856\n",
      "Iteration 1946, the loss is 4.517498401201538, parameters k is 11.42390099242054 and b is -49.10647204551881\n",
      "Iteration 1947, the loss is 4.517495153293854, parameters k is 11.423814490444256 and b is -49.10631394275201\n",
      "Iteration 1948, the loss is 4.517492274311025, parameters k is 11.423727988467972 and b is -49.10615583998521\n",
      "Iteration 1949, the loss is 4.517490226571046, parameters k is 11.423895122855324 and b is -49.10595821152671\n",
      "Iteration 1950, the loss is 4.517486978663374, parameters k is 11.42380862087904 and b is -49.105800108759915\n",
      "Iteration 1951, the loss is 4.5174837307556945, parameters k is 11.423722118902756 and b is -49.10564200599312\n",
      "Iteration 1952, the loss is 4.517480538774537, parameters k is 11.423635616926472 and b is -49.10548390322632\n",
      "Iteration 1953, the loss is 4.5174788040328915, parameters k is 11.423802751313824 and b is -49.10528627476782\n",
      "Iteration 1954, the loss is 4.517475556125218, parameters k is 11.42371624933754 and b is -49.105128172001024\n",
      "Iteration 1955, the loss is 4.517472308217541, parameters k is 11.423629747361256 and b is -49.10497006923423\n",
      "Iteration 1956, the loss is 4.517469060309859, parameters k is 11.423543245384971 and b is -49.10481196646743\n",
      "Iteration 1957, the loss is 4.517467124422924, parameters k is 11.423456743408687 and b is -49.104653863700634\n",
      "Iteration 1958, the loss is 4.517464133587061, parameters k is 11.42362387779604 and b is -49.10445623524213\n",
      "Iteration 1959, the loss is 4.5174608856793785, parameters k is 11.423537375819755 and b is -49.104298132475336\n",
      "Iteration 1960, the loss is 4.517457637771703, parameters k is 11.423450873843471 and b is -49.10414002970854\n",
      "Iteration 1961, the loss is 4.517455388886437, parameters k is 11.423364371867187 and b is -49.10398192694174\n",
      "Iteration 1962, the loss is 4.517452711048896, parameters k is 11.42353150625454 and b is -49.10378429848324\n",
      "Iteration 1963, the loss is 4.517449463141221, parameters k is 11.423445004278255 and b is -49.103626195716444\n",
      "Iteration 1964, the loss is 4.5174462152335435, parameters k is 11.423358502301971 and b is -49.10346809294965\n",
      "Iteration 1965, the loss is 4.517443653349949, parameters k is 11.423272000325687 and b is -49.10330999018285\n",
      "Iteration 1966, the loss is 4.517441288510742, parameters k is 11.423439134713039 and b is -49.10311236172435\n",
      "Iteration 1967, the loss is 4.517438040603058, parameters k is 11.423352632736755 and b is -49.10295425895755\n",
      "Iteration 1968, the loss is 4.517434792695389, parameters k is 11.42326613076047 and b is -49.102796156190756\n",
      "Iteration 1969, the loss is 4.517431917813455, parameters k is 11.423179628784187 and b is -49.10263805342396\n",
      "Iteration 1970, the loss is 4.517429865972582, parameters k is 11.423346763171539 and b is -49.10244042496546\n",
      "Iteration 1971, the loss is 4.517426618064904, parameters k is 11.423260261195255 and b is -49.10228232219866\n",
      "Iteration 1972, the loss is 4.517423370157228, parameters k is 11.42317375921897 and b is -49.102124219431865\n",
      "Iteration 1973, the loss is 4.517420182276971, parameters k is 11.423087257242686 and b is -49.10196611666507\n",
      "Iteration 1974, the loss is 4.51741844343442, parameters k is 11.423254391630039 and b is -49.10176848820657\n",
      "Iteration 1975, the loss is 4.517415195526744, parameters k is 11.423167889653755 and b is -49.10161038543977\n",
      "Iteration 1976, the loss is 4.517411947619069, parameters k is 11.42308138767747 and b is -49.10145228267297\n",
      "Iteration 1977, the loss is 4.5174086997113925, parameters k is 11.422994885701186 and b is -49.101294179906176\n",
      "Iteration 1978, the loss is 4.517406767925361, parameters k is 11.422908383724902 and b is -49.10113607713938\n",
      "Iteration 1979, the loss is 4.517403772988584, parameters k is 11.423075518112254 and b is -49.10093844868088\n",
      "Iteration 1980, the loss is 4.5174005250809115, parameters k is 11.42298901613597 and b is -49.10078034591408\n",
      "Iteration 1981, the loss is 4.517397277173228, parameters k is 11.422902514159686 and b is -49.100622243147285\n",
      "Iteration 1982, the loss is 4.517395032388878, parameters k is 11.422816012183402 and b is -49.10046414038049\n",
      "Iteration 1983, the loss is 4.517392350450425, parameters k is 11.422983146570754 and b is -49.10026651192199\n",
      "Iteration 1984, the loss is 4.517389102542753, parameters k is 11.42289664459447 and b is -49.10010840915519\n",
      "Iteration 1985, the loss is 4.517385854635071, parameters k is 11.422810142618186 and b is -49.09995030638839\n",
      "Iteration 1986, the loss is 4.517383296852386, parameters k is 11.422723640641902 and b is -49.0997922036216\n",
      "Iteration 1987, the loss is 4.517380927912271, parameters k is 11.422890775029254 and b is -49.099594575163096\n",
      "Iteration 1988, the loss is 4.517377680004591, parameters k is 11.42280427305297 and b is -49.0994364723963\n",
      "Iteration 1989, the loss is 4.517374432096915, parameters k is 11.422717771076686 and b is -49.0992783696295\n",
      "Iteration 1990, the loss is 4.5173715613158985, parameters k is 11.422631269100401 and b is -49.099120266862705\n",
      "Iteration 1991, the loss is 4.517369505374107, parameters k is 11.422798403487754 and b is -49.098922638404204\n",
      "Iteration 1992, the loss is 4.517366257466435, parameters k is 11.42271190151147 and b is -49.09876453563741\n",
      "Iteration 1993, the loss is 4.517363009558752, parameters k is 11.422625399535185 and b is -49.09860643287061\n",
      "Iteration 1994, the loss is 4.517359825779415, parameters k is 11.422538897558901 and b is -49.098448330103814\n",
      "Iteration 1995, the loss is 4.517358082835952, parameters k is 11.422706031946253 and b is -49.09825070164531\n",
      "Iteration 1996, the loss is 4.517354834928273, parameters k is 11.42261952996997 and b is -49.098092598878516\n",
      "Iteration 1997, the loss is 4.517351587020597, parameters k is 11.422533027993685 and b is -49.09793449611172\n",
      "Iteration 1998, the loss is 4.517348339112918, parameters k is 11.422446526017401 and b is -49.09777639334492\n",
      "Iteration 1999, the loss is 4.517346411427796, parameters k is 11.422360024041117 and b is -49.097618290578126\n"
     ]
    }
   ],
   "source": [
    "#initialized parameters\n",
    "\n",
    "k = random.random() * 200 - 100  # -100 100\n",
    "b = random.random() * 200 - 100  # -100 100\n",
    "\n",
    "learning_rate = 1e-2\n",
    "\n",
    "iteration_num = 2000 \n",
    "losses = []\n",
    "for i in range(iteration_num):\n",
    "    \n",
    "    price_use_current_parameters = [price(r, k, b) for r in X_rm]  # \\hat{y}\n",
    "    \n",
    "    current_loss = loss(y, price_use_current_parameters)\n",
    "    losses.append(current_loss)\n",
    "    print(\"Iteration {}, the loss is {}, parameters k is {} and b is {}\".format(i,current_loss,k,b))\n",
    "    \n",
    "    k_gradient = partial_derivative_k(X_rm, y, price_use_current_parameters)\n",
    "    b_gradient = partial_derivative_b(y, price_use_current_parameters)\n",
    "    \n",
    "    k = k + (-1 * k_gradient) * learning_rate\n",
    "    b = b + (-1 * b_gradient) * learning_rate\n",
    "best_k = k\n",
    "best_b = b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1e8ab44feb8>]"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(list(range(iteration_num)),losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x1e8ab4b2048>"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "price_use_best_parameters = [price(r, best_k, best_b) for r in X_rm]\n",
    "\n",
    "plt.scatter(X_rm,y)\n",
    "plt.scatter(X_rm,price_use_current_parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<评阅点>\n",
    "+ 是否将Loss改成了“绝对值”(3')\n",
    "+ 是否完成了偏导的重新定义(5')\n",
    "+ 新的模型Loss是否能够收敛 (11’)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
